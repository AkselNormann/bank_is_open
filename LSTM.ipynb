{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-2008.pkl\n",
      "2008-2009.pkl\n",
      "(1267, 3, 240)\n",
      "(1266, 3, 240)\n",
      "2009-2010.pkl\n",
      "(2533, 3, 240)\n",
      "(1263, 3, 240)\n",
      "2010-2011.pkl\n",
      "(3796, 3, 240)\n",
      "(1263, 3, 240)\n",
      "2012-2013.pkl\n",
      "(5059, 3, 240)\n",
      "(1265, 3, 240)\n",
      "2013-2014.pkl\n",
      "(6324, 3, 240)\n",
      "(1272, 3, 240)\n",
      "2014-2015.pkl\n",
      "(7596, 3, 240)\n",
      "(1263, 3, 240)\n",
      "2015-2016.pkl\n",
      "(8859, 3, 240)\n",
      "(1269, 3, 240)\n",
      "2016-2017.pkl\n",
      "2017-2018.pkl\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data_short/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "\n",
    "X_train = np.zeros(5)\n",
    "\n",
    "for file in files[:-2]:\n",
    "    if \".pkl\" not in file: continue\n",
    "    \n",
    "    with open(game_data_path + file, 'rb') as f:\n",
    "        print(file)\n",
    "        if X_train.shape[0] == 5:\n",
    "            X_train, y_train = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "            print(X_train.shape)\n",
    "            print(X_add.shape)\n",
    "            X_train = np.concatenate((X_train, X_add), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_add), axis = 0)\n",
    "\n",
    "with open(game_data_path + files[-2], 'rb') as f:\n",
    "        print(files[-2])\n",
    "        X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "with open(game_data_path + files[-1], 'rb') as f:\n",
    "        print(files[-1])\n",
    "        X_test, y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X, y):\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, target_size, num_layers, batch_size, time_steps):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Initialize LSTM unit\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2out = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size , hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, self.time_steps, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.time_steps, self.hidden_dim))\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden = self.lstm(input_seq, self.hidden)\n",
    "        \n",
    "        pred = self.hidden2out(lstm_out)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "model = LSTMModel(input_dim = 240,\n",
    "                     hidden_dim = 50,\n",
    "                     target_size = 1,\n",
    "                     num_layers = 2,\n",
    "                     batch_size = 10, \n",
    "                     time_steps = 3)\n",
    "                     \n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Losses after 49 iterations:\n",
      "Train: 4647.05224609375\n",
      "----------\n",
      "Losses after 99 iterations:\n",
      "Train: 4645.2138671875\n",
      "----------\n",
      "Losses after 149 iterations:\n",
      "Train: 4619.85546875\n",
      "----------\n",
      "Losses after 199 iterations:\n",
      "Train: 4597.67041015625\n",
      "----------\n",
      "Losses after 249 iterations:\n",
      "Train: 4581.478515625\n",
      "----------\n",
      "Losses after 299 iterations:\n",
      "Train: 4612.57568359375\n",
      "----------\n",
      "Losses after 349 iterations:\n",
      "Train: 4629.36767578125\n",
      "----------\n",
      "Losses after 399 iterations:\n",
      "Train: 4643.70458984375\n",
      "----------\n",
      "Losses after 449 iterations:\n",
      "Train: 4632.33349609375\n",
      "----------\n",
      "Losses after 499 iterations:\n",
      "Train: 4637.7353515625\n",
      "----------\n",
      "Losses after 549 iterations:\n",
      "Train: 4594.4013671875\n",
      "----------\n",
      "Losses after 599 iterations:\n",
      "Train: 4419.1787109375\n",
      "----------\n",
      "Losses after 649 iterations:\n",
      "Train: 4283.677734375\n",
      "----------\n",
      "Losses after 699 iterations:\n",
      "Train: 4405.4775390625\n",
      "----------\n",
      "Losses after 749 iterations:\n",
      "Train: 4449.09912109375\n",
      "----------\n",
      "Losses after 799 iterations:\n",
      "Train: 4474.68701171875\n",
      "----------\n",
      "Losses after 849 iterations:\n",
      "Train: 3873.900146484375\n",
      "----------\n",
      "Losses after 899 iterations:\n",
      "Train: 3718.61083984375\n",
      "----------\n",
      "Losses after 949 iterations:\n",
      "Train: 3726.930908203125\n",
      "----------\n",
      "Losses after 999 iterations:\n",
      "Train: 3692.3125\n",
      "----------\n",
      "Losses after 1049 iterations:\n",
      "Train: 3690.8017578125\n",
      "----------\n",
      "Losses after 1099 iterations:\n",
      "Train: 3674.763671875\n",
      "----------\n",
      "Losses after 1149 iterations:\n",
      "Train: 3671.66650390625\n",
      "----------\n",
      "Losses after 1199 iterations:\n",
      "Train: 3670.85986328125\n",
      "----------\n",
      "Losses after 1249 iterations:\n",
      "Train: 3668.052490234375\n",
      "----------\n",
      "Losses after 1299 iterations:\n",
      "Train: 3666.334228515625\n",
      "----------\n",
      "Losses after 1349 iterations:\n",
      "Train: 3662.3076171875\n",
      "----------\n",
      "Losses after 1399 iterations:\n",
      "Train: 3650.26318359375\n",
      "----------\n",
      "Losses after 1449 iterations:\n",
      "Train: 3644.8623046875\n",
      "----------\n",
      "Losses after 1499 iterations:\n",
      "Train: 4486.16015625\n",
      "----------\n",
      "Losses after 1549 iterations:\n",
      "Train: 4555.33740234375\n",
      "----------\n",
      "Losses after 1599 iterations:\n",
      "Train: 4534.4326171875\n",
      "----------\n",
      "Losses after 1649 iterations:\n",
      "Train: 4433.79833984375\n",
      "----------\n",
      "Losses after 1699 iterations:\n",
      "Train: 4500.53759765625\n",
      "----------\n",
      "Losses after 1749 iterations:\n",
      "Train: 4513.0341796875\n",
      "----------\n",
      "Losses after 1799 iterations:\n",
      "Train: 4501.533203125\n",
      "----------\n",
      "Losses after 1849 iterations:\n",
      "Train: 4465.0390625\n",
      "----------\n",
      "Losses after 1899 iterations:\n",
      "Train: 4511.94580078125\n",
      "----------\n",
      "Losses after 1949 iterations:\n",
      "Train: 4479.48828125\n",
      "----------\n",
      "Losses after 1999 iterations:\n",
      "Train: 4301.8544921875\n",
      "----------\n",
      "Losses after 2049 iterations:\n",
      "Train: 4417.2490234375\n",
      "----------\n",
      "Losses after 2099 iterations:\n",
      "Train: 4581.61669921875\n",
      "----------\n",
      "Losses after 2149 iterations:\n",
      "Train: 4618.33837890625\n",
      "----------\n",
      "Losses after 2199 iterations:\n",
      "Train: 4616.04052734375\n",
      "----------\n",
      "Losses after 2249 iterations:\n",
      "Train: 4551.5263671875\n",
      "----------\n",
      "Losses after 2299 iterations:\n",
      "Train: 4594.0087890625\n",
      "----------\n",
      "Losses after 2349 iterations:\n",
      "Train: 4515.75\n",
      "----------\n",
      "Losses after 2399 iterations:\n",
      "Train: 4138.537109375\n",
      "----------\n",
      "Losses after 2449 iterations:\n",
      "Train: 3855.343994140625\n",
      "----------\n",
      "Losses after 2499 iterations:\n",
      "Train: 4432.98193359375\n",
      "----------\n",
      "Losses after 2549 iterations:\n",
      "Train: 4412.810546875\n",
      "----------\n",
      "Losses after 2599 iterations:\n",
      "Train: 4337.015625\n",
      "----------\n",
      "Losses after 2649 iterations:\n",
      "Train: 4296.19775390625\n",
      "----------\n",
      "Losses after 2699 iterations:\n",
      "Train: 4063.599853515625\n",
      "----------\n",
      "Losses after 2749 iterations:\n",
      "Train: 4460.32373046875\n",
      "----------\n",
      "Losses after 2799 iterations:\n",
      "Train: 4431.74072265625\n",
      "----------\n",
      "Losses after 2849 iterations:\n",
      "Train: 4437.13232421875\n",
      "----------\n",
      "Losses after 2899 iterations:\n",
      "Train: 4406.52099609375\n",
      "----------\n",
      "Losses after 2949 iterations:\n",
      "Train: 4430.33984375\n",
      "----------\n",
      "Losses after 2999 iterations:\n",
      "Train: 4502.01025390625\n",
      "----------\n",
      "Losses after 3049 iterations:\n",
      "Train: 4452.25830078125\n",
      "----------\n",
      "Losses after 3099 iterations:\n",
      "Train: 4432.6083984375\n",
      "----------\n",
      "Losses after 3149 iterations:\n",
      "Train: 4419.31787109375\n",
      "----------\n",
      "Losses after 3199 iterations:\n",
      "Train: 4090.467529296875\n",
      "----------\n",
      "Losses after 3249 iterations:\n",
      "Train: 3904.9833984375\n",
      "----------\n",
      "Losses after 3299 iterations:\n",
      "Train: 3797.828857421875\n",
      "----------\n",
      "Losses after 3349 iterations:\n",
      "Train: 3733.385986328125\n",
      "----------\n",
      "Losses after 3399 iterations:\n",
      "Train: 3693.646728515625\n",
      "----------\n",
      "Losses after 3449 iterations:\n",
      "Train: 3668.960693359375\n",
      "----------\n",
      "Losses after 3499 iterations:\n",
      "Train: 3651.18408203125\n",
      "----------\n",
      "Losses after 3549 iterations:\n",
      "Train: 3638.68310546875\n",
      "----------\n",
      "Losses after 3599 iterations:\n",
      "Train: 3631.381591796875\n",
      "----------\n",
      "Losses after 3649 iterations:\n",
      "Train: 3627.383544921875\n",
      "----------\n",
      "Losses after 3699 iterations:\n",
      "Train: 3629.428955078125\n",
      "----------\n",
      "Losses after 3749 iterations:\n",
      "Train: 3628.947021484375\n",
      "----------\n",
      "Losses after 3799 iterations:\n",
      "Train: 3627.35107421875\n",
      "----------\n",
      "Losses after 3849 iterations:\n",
      "Train: 3625.850830078125\n",
      "----------\n",
      "Losses after 3899 iterations:\n",
      "Train: 3626.004638671875\n",
      "----------\n",
      "Losses after 3949 iterations:\n",
      "Train: 3627.0322265625\n",
      "----------\n",
      "Losses after 3999 iterations:\n",
      "Train: 3628.28515625\n",
      "----------\n",
      "Losses after 4049 iterations:\n",
      "Train: 3628.21923828125\n",
      "----------\n",
      "Losses after 4099 iterations:\n",
      "Train: 3628.934814453125\n",
      "----------\n",
      "Losses after 4149 iterations:\n",
      "Train: 3630.092041015625\n",
      "----------\n",
      "Losses after 4199 iterations:\n",
      "Train: 3630.885498046875\n",
      "----------\n",
      "Losses after 4249 iterations:\n",
      "Train: 3631.75537109375\n",
      "----------\n",
      "Losses after 4299 iterations:\n",
      "Train: 3633.00341796875\n",
      "----------\n",
      "Losses after 4349 iterations:\n",
      "Train: 3635.2646484375\n",
      "----------\n",
      "Losses after 4399 iterations:\n",
      "Train: 4652.51708984375\n",
      "----------\n",
      "Losses after 4449 iterations:\n",
      "Train: 4651.673828125\n",
      "----------\n",
      "Losses after 4499 iterations:\n",
      "Train: 3642.416259765625\n",
      "----------\n",
      "Losses after 4549 iterations:\n",
      "Train: 4638.67529296875\n",
      "----------\n",
      "Losses after 4599 iterations:\n",
      "Train: 4643.8759765625\n",
      "----------\n",
      "Losses after 4649 iterations:\n",
      "Train: 4644.38916015625\n",
      "----------\n",
      "Losses after 4699 iterations:\n",
      "Train: 4564.3095703125\n",
      "----------\n",
      "Losses after 4749 iterations:\n",
      "Train: 4537.103515625\n",
      "----------\n",
      "Losses after 4799 iterations:\n",
      "Train: 4403.3564453125\n",
      "----------\n",
      "Losses after 4849 iterations:\n",
      "Train: 4477.79833984375\n",
      "----------\n",
      "Losses after 4899 iterations:\n",
      "Train: 4517.9384765625\n",
      "----------\n",
      "Losses after 4949 iterations:\n",
      "Train: 4541.82568359375\n",
      "----------\n",
      "Losses after 4999 iterations:\n",
      "Train: 4551.62109375\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(5000):   # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(X_train), model.batch_size):\n",
    "        if i + model.batch_size >= len(X_train) : continue\n",
    "        if i > 100: break\n",
    "        \n",
    "        #Pytorch accumulates gradients. We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM, detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network.\n",
    "        batch_input = X_train[i : i + model.batch_size] #.reshape((X.shape[1], model.batch_size, X.shape[2]))\n",
    "        batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "                                                    \n",
    "        targets = Variable(torch.from_numpy(y[i : i + model.batch_size])).type(torch.FloatTensor)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        scores = model(batch)\n",
    "        scores = scores[:, -1].reshape((model.batch_size)) # we only care about the last output\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        \n",
    "        train_loss += loss.detach().numpy()\n",
    "        \n",
    "    ## validation loss\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(\"----------\")\n",
    "        print(\"Losses after {} iterations:\".format(epoch))\n",
    "        print(\"Train: {}\".format(loss.detach().numpy()))\n",
    "#        with torch.no_grad():\n",
    "#             batch_input = val_X\n",
    "#             batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "#             targets = Variable(torch.from_numpy(val_y)).type(torch.FloatTensor)\n",
    "#             scores = model(batch)\n",
    "#             scores = scores[:, -1].reshape((len(val_y))) # we only care about the last output\n",
    "#             val_loss = loss_function(scores, targets)\n",
    "#             print(\"Val: {}\".format(val_loss))\n",
    "#             val_losses.append(val_loss)\n",
    "#             losses.append(train_loss/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    batch_input = val_X\n",
    "    batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "    targets = Variable(torch.from_numpy(val_y)).type(torch.FloatTensor)\n",
    "    scores = model(batch)\n",
    "    scores = scores[:, -1].reshape((len(val_y))) # we only care about the last output\n",
    "    val_loss = loss_function(scores, targets)\n",
    "    print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
