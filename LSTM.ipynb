{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-2008.pkl\n",
      "2008-2009.pkl\n",
      "(1267, 3, 506)\n",
      "(1266, 3, 506)\n",
      "2009-2010.pkl\n",
      "(2533, 3, 506)\n",
      "(1263, 3, 506)\n",
      "2010-2011.pkl\n",
      "(3796, 3, 506)\n",
      "(1263, 3, 504)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-64361944d280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_add\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "\n",
    "X_train = np.zeros(5)\n",
    "\n",
    "for file in files[:-2]:\n",
    "    if \".pkl\" not in file: continue\n",
    "    \n",
    "    with open(game_data_path + file, 'rb') as f:\n",
    "        print(file)\n",
    "        if X_train.shape[0] == 5:\n",
    "            X_train, y_train = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "            print(X_train.shape)\n",
    "            print(X_add.shape)\n",
    "            X_train = np.concatenate((X_train, X_add), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_add), axis = 0)\n",
    "\n",
    "with open(game_data_path + files[-2], 'rb') as f:\n",
    "        print(files[-2])\n",
    "        X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "with open(game_data_path + files[-1], 'rb') as f:\n",
    "        print(files[-1])\n",
    "        X_test, y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X, y):\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, target_size, num_layers, batch_size, time_steps, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Initialize LSTM unit\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2out = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size , hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, self.time_steps, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.time_steps, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden = self.lstm(input_seq, self.hidden)\n",
    "        \n",
    "        drop_out = self.drop(lstm_out)\n",
    "        \n",
    "        pred = self.hidden2out(drop_out)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "model = LSTMModel(input_dim = 506,\n",
    "                     hidden_dim = 100,\n",
    "                     target_size = 1,\n",
    "                     num_layers = 2,\n",
    "                     batch_size = 10, \n",
    "                     time_steps = 3,\n",
    "                     dropout = 0.25\n",
    "                 )\n",
    "                     \n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Losses after 50 iterations:\n",
      "Train: 36685.01171875\n",
      "----------\n",
      "Losses after 100 iterations:\n",
      "Train: 36408.7734375\n",
      "----------\n",
      "Losses after 150 iterations:\n",
      "Train: 36011.77734375\n",
      "----------\n",
      "Losses after 200 iterations:\n",
      "Train: 35363.8203125\n",
      "----------\n",
      "Losses after 250 iterations:\n",
      "Train: 34367.4296875\n",
      "----------\n",
      "Losses after 300 iterations:\n",
      "Train: 32798.515625\n",
      "----------\n",
      "Losses after 350 iterations:\n",
      "Train: 30401.861328125\n",
      "----------\n",
      "Losses after 400 iterations:\n",
      "Train: 27507.20703125\n",
      "----------\n",
      "Losses after 450 iterations:\n",
      "Train: 24176.681640625\n",
      "----------\n",
      "Losses after 500 iterations:\n",
      "Train: 21110.4765625\n",
      "----------\n",
      "Losses after 550 iterations:\n",
      "Train: 18733.837890625\n",
      "----------\n",
      "Losses after 600 iterations:\n",
      "Train: 16493.751953125\n",
      "----------\n",
      "Losses after 650 iterations:\n",
      "Train: 14206.6201171875\n",
      "----------\n",
      "Losses after 700 iterations:\n",
      "Train: 12912.609375\n",
      "----------\n",
      "Losses after 750 iterations:\n",
      "Train: 11520.3720703125\n",
      "----------\n",
      "Losses after 800 iterations:\n",
      "Train: 9821.4072265625\n",
      "----------\n",
      "Losses after 850 iterations:\n",
      "Train: 8703.556640625\n",
      "----------\n",
      "Losses after 900 iterations:\n",
      "Train: 7470.58349609375\n",
      "----------\n",
      "Losses after 950 iterations:\n",
      "Train: 7079.7568359375\n",
      "----------\n",
      "Losses after 1000 iterations:\n",
      "Train: 5900.93994140625\n",
      "----------\n",
      "Losses after 1050 iterations:\n",
      "Train: 5476.916015625\n",
      "----------\n",
      "Losses after 1100 iterations:\n",
      "Train: 5544.9111328125\n",
      "----------\n",
      "Losses after 1150 iterations:\n",
      "Train: 3672.26220703125\n",
      "----------\n",
      "Losses after 1200 iterations:\n",
      "Train: 3876.849853515625\n",
      "----------\n",
      "Losses after 1250 iterations:\n",
      "Train: 4065.474609375\n",
      "----------\n",
      "Losses after 1300 iterations:\n",
      "Train: 2418.542724609375\n",
      "----------\n",
      "Losses after 1350 iterations:\n",
      "Train: 3080.68603515625\n",
      "----------\n",
      "Losses after 1400 iterations:\n",
      "Train: 3256.389404296875\n",
      "----------\n",
      "Losses after 1450 iterations:\n",
      "Train: 2367.875\n",
      "----------\n",
      "Losses after 1500 iterations:\n",
      "Train: 2081.845703125\n",
      "----------\n",
      "Losses after 1550 iterations:\n",
      "Train: 2515.148193359375\n",
      "----------\n",
      "Losses after 1600 iterations:\n",
      "Train: 2447.834716796875\n",
      "----------\n",
      "Losses after 1650 iterations:\n",
      "Train: 2190.407958984375\n",
      "----------\n",
      "Losses after 1700 iterations:\n",
      "Train: 1538.9061279296875\n",
      "----------\n",
      "Losses after 1750 iterations:\n",
      "Train: 1489.53271484375\n",
      "----------\n",
      "Losses after 1800 iterations:\n",
      "Train: 1351.1761474609375\n",
      "----------\n",
      "Losses after 1850 iterations:\n",
      "Train: 1514.643798828125\n",
      "----------\n",
      "Losses after 1900 iterations:\n",
      "Train: 1438.9951171875\n",
      "----------\n",
      "Losses after 1950 iterations:\n",
      "Train: 1381.963134765625\n",
      "----------\n",
      "Losses after 2000 iterations:\n",
      "Train: 1418.204833984375\n",
      "----------\n",
      "Losses after 2050 iterations:\n",
      "Train: 934.7229614257812\n",
      "----------\n",
      "Losses after 2100 iterations:\n",
      "Train: 1406.85009765625\n",
      "----------\n",
      "Losses after 2150 iterations:\n",
      "Train: 1386.780517578125\n",
      "----------\n",
      "Losses after 2200 iterations:\n",
      "Train: 1151.0179443359375\n",
      "----------\n",
      "Losses after 2250 iterations:\n",
      "Train: 1370.2796630859375\n",
      "----------\n",
      "Losses after 2300 iterations:\n",
      "Train: 1030.22412109375\n",
      "----------\n",
      "Losses after 2350 iterations:\n",
      "Train: 1084.9296875\n",
      "----------\n",
      "Losses after 2400 iterations:\n",
      "Train: 1306.1036376953125\n",
      "----------\n",
      "Losses after 2450 iterations:\n",
      "Train: 1062.6925048828125\n",
      "----------\n",
      "Losses after 2500 iterations:\n",
      "Train: 820.1736450195312\n",
      "----------\n",
      "Losses after 2550 iterations:\n",
      "Train: 1422.7158203125\n",
      "----------\n",
      "Losses after 2600 iterations:\n",
      "Train: 975.7271728515625\n",
      "----------\n",
      "Losses after 2650 iterations:\n",
      "Train: 1468.4278564453125\n",
      "----------\n",
      "Losses after 2700 iterations:\n",
      "Train: 1300.9991455078125\n",
      "----------\n",
      "Losses after 2750 iterations:\n",
      "Train: 836.7471923828125\n",
      "----------\n",
      "Losses after 2800 iterations:\n",
      "Train: 1081.3798828125\n",
      "----------\n",
      "Losses after 2850 iterations:\n",
      "Train: 1012.8868408203125\n",
      "----------\n",
      "Losses after 2900 iterations:\n",
      "Train: 948.5977783203125\n",
      "----------\n",
      "Losses after 2950 iterations:\n",
      "Train: 1245.0882568359375\n",
      "----------\n",
      "Losses after 3000 iterations:\n",
      "Train: 957.5830688476562\n",
      "----------\n",
      "Losses after 3050 iterations:\n",
      "Train: 861.2700805664062\n",
      "----------\n",
      "Losses after 3100 iterations:\n",
      "Train: 753.6804809570312\n",
      "----------\n",
      "Losses after 3150 iterations:\n",
      "Train: 1134.130126953125\n",
      "----------\n",
      "Losses after 3200 iterations:\n",
      "Train: 696.3800048828125\n",
      "----------\n",
      "Losses after 3250 iterations:\n",
      "Train: 1382.8253173828125\n",
      "----------\n",
      "Losses after 3300 iterations:\n",
      "Train: 851.9300537109375\n",
      "----------\n",
      "Losses after 3350 iterations:\n",
      "Train: 915.9879760742188\n",
      "----------\n",
      "Losses after 3400 iterations:\n",
      "Train: 814.6397094726562\n",
      "----------\n",
      "Losses after 3450 iterations:\n",
      "Train: 1050.434326171875\n",
      "----------\n",
      "Losses after 3500 iterations:\n",
      "Train: 810.0842895507812\n",
      "----------\n",
      "Losses after 3550 iterations:\n",
      "Train: 1012.0006713867188\n",
      "----------\n",
      "Losses after 3600 iterations:\n",
      "Train: 811.20947265625\n",
      "----------\n",
      "Losses after 3650 iterations:\n",
      "Train: 911.635986328125\n",
      "----------\n",
      "Losses after 3700 iterations:\n",
      "Train: 1355.837646484375\n",
      "----------\n",
      "Losses after 3750 iterations:\n",
      "Train: 562.6075439453125\n",
      "----------\n",
      "Losses after 3800 iterations:\n",
      "Train: 996.0325317382812\n",
      "----------\n",
      "Losses after 3850 iterations:\n",
      "Train: 991.444091796875\n",
      "----------\n",
      "Losses after 3900 iterations:\n",
      "Train: 1232.43408203125\n",
      "----------\n",
      "Losses after 3950 iterations:\n",
      "Train: 811.4513549804688\n",
      "----------\n",
      "Losses after 4000 iterations:\n",
      "Train: 1044.1856689453125\n",
      "----------\n",
      "Losses after 4050 iterations:\n",
      "Train: 953.182861328125\n",
      "----------\n",
      "Losses after 4100 iterations:\n",
      "Train: 822.8162841796875\n",
      "----------\n",
      "Losses after 4150 iterations:\n",
      "Train: 782.8846435546875\n",
      "----------\n",
      "Losses after 4200 iterations:\n",
      "Train: 895.0343017578125\n",
      "----------\n",
      "Losses after 4250 iterations:\n",
      "Train: 1083.1036376953125\n",
      "----------\n",
      "Losses after 4300 iterations:\n",
      "Train: 804.5508422851562\n",
      "----------\n",
      "Losses after 4350 iterations:\n",
      "Train: 1318.87890625\n",
      "----------\n",
      "Losses after 4400 iterations:\n",
      "Train: 671.310302734375\n",
      "----------\n",
      "Losses after 4450 iterations:\n",
      "Train: 949.0357055664062\n",
      "----------\n",
      "Losses after 4500 iterations:\n",
      "Train: 819.7722778320312\n",
      "----------\n",
      "Losses after 4550 iterations:\n",
      "Train: 878.117919921875\n",
      "----------\n",
      "Losses after 4600 iterations:\n",
      "Train: 1027.9825439453125\n",
      "----------\n",
      "Losses after 4650 iterations:\n",
      "Train: 1028.0091552734375\n",
      "----------\n",
      "Losses after 4700 iterations:\n",
      "Train: 873.7280883789062\n",
      "----------\n",
      "Losses after 4750 iterations:\n",
      "Train: 1015.793212890625\n",
      "----------\n",
      "Losses after 4800 iterations:\n",
      "Train: 775.9569091796875\n",
      "----------\n",
      "Losses after 4850 iterations:\n",
      "Train: 728.2817993164062\n",
      "----------\n",
      "Losses after 4900 iterations:\n",
      "Train: 531.1065063476562\n",
      "----------\n",
      "Losses after 4950 iterations:\n",
      "Train: 798.0797119140625\n",
      "----------\n",
      "Losses after 5000 iterations:\n",
      "Train: 750.3516235351562\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(5000):   # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(X_train), model.batch_size):\n",
    "        if i + model.batch_size >= len(X_train) : continue\n",
    "        if i > 10: break\n",
    "        \n",
    "        #Pytorch accumulates gradients. We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM, detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network.\n",
    "        batch_input = X_train[i : i + model.batch_size] #.reshape((X.shape[1], model.batch_size, X.shape[2]))\n",
    "        batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "                                                    \n",
    "        targets = Variable(torch.from_numpy(y[i : i + model.batch_size])).type(torch.FloatTensor)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        scores = model(batch)\n",
    "        scores = scores[:, -1].reshape((model.batch_size)) # we only care about the last output\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #optimizer.zero_grad()   # clear gradients for next train\n",
    "        \n",
    "        train_loss += loss.detach().numpy()\n",
    "        \n",
    "    ## validation loss\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(\"----------\")\n",
    "        print(\"Losses after {} iterations:\".format(epoch + 1))\n",
    "        print(\"Train: {}\".format(loss.detach().numpy()))\n",
    "#         with torch.no_grad():\n",
    "#             batch_input = val_X\n",
    "#             batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "#             targets = Variable(torch.from_numpy(val_y)).type(torch.FloatTensor)\n",
    "#             scores = model(batch)\n",
    "#             scores = scores[:, -1].reshape((len(val_y))) # we only care about the last output\n",
    "#             val_loss = loss_function(scores, targets)\n",
    "#             print(\"Val: {}\".format(val_loss))\n",
    "#             val_losses.append(val_loss)\n",
    "#             losses.append(train_loss/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    batch_input = val_X\n",
    "    batch = Variable(torch.from_numpy(batch_input)).type(torch.FloatTensor)\n",
    "    targets = Variable(torch.from_numpy(val_y)).type(torch.FloatTensor)\n",
    "    scores = model(batch)\n",
    "    scores = scores[:, -1].reshape((len(val_y))) # we only care about the last output\n",
    "    val_loss = loss_function(scores, targets)\n",
    "    print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
