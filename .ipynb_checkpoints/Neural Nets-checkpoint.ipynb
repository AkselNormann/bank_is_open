{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-2014.pkl\n",
      "2014-2015.pkl\n",
      "(1272, 3, 508)\n",
      "(1263, 3, 508)\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "start = 5\n",
    "with open(game_data_path + files[start], 'rb') as f:\n",
    "    print(files[start])\n",
    "    X, y = pickle.load(f, encoding='latin1')\n",
    "for i in range(1):\n",
    "    with open(game_data_path + files[start + 1 + i], 'rb') as f:\n",
    "        print(files[start + 1 + i])\n",
    "        X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "        print(X.shape)\n",
    "        print(X_add.shape)\n",
    "        X = np.concatenate((X, X_add), axis = 0)\n",
    "        y = np.concatenate((y, y_add), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2475, 1524])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2475, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValSplit(X, y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "\n",
    "    p = np.random.permutation(len(X))\n",
    "    X = X[p]\n",
    "    y = y[p]\n",
    "\n",
    "    val = 0.2\n",
    "    val = round(len(X) * val)\n",
    "    X_val = X[:val]\n",
    "    y_val = y[:val]\n",
    "    X = X[val:]\n",
    "    y = y[val:]\n",
    "    \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = trainValSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-2016.pkl\n"
     ]
    }
   ],
   "source": [
    "# BySeason trainValSplit\n",
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "with open(game_data_path + files[7], 'rb') as f:\n",
    "    print(files[7])\n",
    "    X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "X = X[y > 0]\n",
    "y = y[y > 0]\n",
    "X_val = X_val[y_val > 0]\n",
    "y_val = y_val[y_val > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 185.],\n",
      "        [ 255.],\n",
      "        [ 183.],\n",
      "        [ 200.],\n",
      "        [ 192.],\n",
      "        [ 227.],\n",
      "        [ 190.],\n",
      "        [ 202.],\n",
      "        [ 199.],\n",
      "        [ 217.]])\n"
     ]
    }
   ],
   "source": [
    "# Split train/test:\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (drop1): Dropout(p=0.5)\n",
      "  (drop2): Dropout(p=0.4)\n",
      "  (drop3): Dropout(p=0.25)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=36825.2031\n",
      "Val Loss=7107.9331\n",
      "-----\n",
      "100\n",
      "Loss=1170.4954\n",
      "Val Loss=10611.4912\n",
      "-----\n",
      "200\n",
      "Loss=807.7614\n",
      "Val Loss=10225.2627\n",
      "-----\n",
      "300\n",
      "Loss=677.3436\n",
      "Val Loss=10075.8594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5758bd32ac39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# input x and predict based on x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# must be (1. nn output, 2. target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5758bd32ac39>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# perform dropout on input vector embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# x = self.drop1(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# x = self.drop2(F.relu(self.lin1(x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        \n",
    "        # layer 3 fully connected 1 unit (output)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "        # self.lin4 = nn.Linear(50, n_output)\n",
    "        \n",
    "        # dropouts\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        # x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = self.drop2(F.relu(self.lin1(x)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        # x = self.drop3(F.relu(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.000001, weight_decay = 1e-2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-2013.pkl\n",
      "2013-2014.pkl\n",
      "(1265, 3, 508)\n",
      "(1272, 3, 508)\n",
      "2014-2015.pkl\n",
      "(2537, 3, 508)\n",
      "(1263, 3, 508)\n",
      "2015-2016.pkl\n",
      "(3800, 3, 508)\n",
      "(1269, 3, 508)\n",
      "2016-2017.pkl\n",
      "2017-2018.pkl\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "\n",
    "files = [\"2012-2013.pkl\", \"2013-2014.pkl\", \"2014-2015.pkl\", \"2015-2016.pkl\",\"2016-2017.pkl\", \"2017-2018.pkl\"]\n",
    "\n",
    "X_train = np.zeros(5)\n",
    "\n",
    "for file in files[:-2]:\n",
    "    if \".pkl\" not in file: continue\n",
    "    \n",
    "    with open(game_data_path + file, 'rb') as f:\n",
    "        print(file)\n",
    "        if X_train.shape[0] == 5:\n",
    "            X_train, y_train = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "            print(X_train.shape)\n",
    "            print(X_add.shape)\n",
    "            X_train = np.concatenate((X_train, X_add), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_add), axis = 0)\n",
    "\n",
    "with open(game_data_path + files[-2], 'rb') as f:\n",
    "        print(files[-2])\n",
    "        X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "with open(game_data_path + files[-1], 'rb') as f:\n",
    "        print(files[-1])\n",
    "        X_test, y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X, y):\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test.reshape((X_test.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=33785.3164\n",
      "Val Loss=8918.6826\n",
      "-----\n",
      "100\n",
      "Loss=1563.3461\n",
      "Val Loss=2937.0347\n",
      "-----\n",
      "200\n",
      "Loss=985.1339\n",
      "Val Loss=1594.7726\n",
      "-----\n",
      "300\n",
      "Loss=765.8445\n",
      "Val Loss=1144.1522\n",
      "-----\n",
      "400\n",
      "Loss=654.3577\n",
      "Val Loss=932.2725\n",
      "-----\n",
      "500\n",
      "Loss=595.5217\n",
      "Val Loss=825.0071\n",
      "-----\n",
      "600\n",
      "Loss=556.5037\n",
      "Val Loss=755.7455\n",
      "-----\n",
      "700\n",
      "Loss=525.3077\n",
      "Val Loss=701.0831\n",
      "-----\n",
      "800\n",
      "Loss=502.7728\n",
      "Val Loss=662.5638\n",
      "-----\n",
      "900\n",
      "Loss=484.7969\n",
      "Val Loss=631.7883\n",
      "-----\n",
      "1000\n",
      "Loss=471.7117\n",
      "Val Loss=609.6647\n",
      "-----\n",
      "1100\n",
      "Loss=458.5612\n",
      "Val Loss=587.8519\n",
      "-----\n",
      "1200\n",
      "Loss=447.8412\n",
      "Val Loss=569.7866\n",
      "-----\n",
      "1300\n",
      "Loss=442.5939\n",
      "Val Loss=560.9316\n",
      "-----\n",
      "1400\n",
      "Loss=439.7016\n",
      "Val Loss=555.8060\n",
      "-----\n",
      "1500\n",
      "Loss=431.0389\n",
      "Val Loss=541.8175\n",
      "-----\n",
      "1600\n",
      "Loss=424.8046\n",
      "Val Loss=531.2537\n",
      "-----\n",
      "1700\n",
      "Loss=423.2522\n",
      "Val Loss=529.3687\n",
      "-----\n",
      "1800\n",
      "Loss=411.8873\n",
      "Val Loss=509.8192\n",
      "-----\n",
      "1900\n",
      "Loss=403.7342\n",
      "Val Loss=495.9784\n",
      "-----\n",
      "2000\n",
      "Loss=397.7825\n",
      "Val Loss=486.1068\n",
      "-----\n",
      "2100\n",
      "Loss=397.4094\n",
      "Val Loss=485.5478\n",
      "-----\n",
      "2200\n",
      "Loss=395.6142\n",
      "Val Loss=482.7234\n",
      "-----\n",
      "2300\n",
      "Loss=387.7166\n",
      "Val Loss=469.3416\n",
      "-----\n",
      "2400\n",
      "Loss=384.9765\n",
      "Val Loss=464.8377\n",
      "-----\n",
      "2500\n",
      "Loss=381.7712\n",
      "Val Loss=459.3698\n",
      "-----\n",
      "2600\n",
      "Loss=377.9476\n",
      "Val Loss=452.8523\n",
      "-----\n",
      "2700\n",
      "Loss=375.2342\n",
      "Val Loss=448.1406\n",
      "-----\n",
      "2800\n",
      "Loss=373.5734\n",
      "Val Loss=445.4286\n",
      "-----\n",
      "2900\n",
      "Loss=371.6293\n",
      "Val Loss=442.0831\n",
      "-----\n",
      "3000\n",
      "Loss=368.0094\n",
      "Val Loss=435.5896\n",
      "-----\n",
      "3100\n",
      "Loss=367.1343\n",
      "Val Loss=434.2079\n",
      "-----\n",
      "3200\n",
      "Loss=366.5140\n",
      "Val Loss=433.2523\n",
      "-----\n",
      "3300\n",
      "Loss=366.0182\n",
      "Val Loss=432.4929\n",
      "-----\n",
      "3400\n",
      "Loss=366.4672\n",
      "Val Loss=433.6159\n",
      "-----\n",
      "3500\n",
      "Loss=366.2339\n",
      "Val Loss=433.3291\n",
      "-----\n",
      "3600\n",
      "Loss=364.7100\n",
      "Val Loss=430.6276\n",
      "-----\n",
      "3700\n",
      "Loss=363.2518\n",
      "Val Loss=428.0550\n",
      "-----\n",
      "3800\n",
      "Loss=363.7597\n",
      "Val Loss=429.2303\n",
      "-----\n",
      "3900\n",
      "Loss=364.8251\n",
      "Val Loss=431.4604\n",
      "-----\n",
      "4000\n",
      "Loss=363.2684\n",
      "Val Loss=428.6893\n",
      "-----\n",
      "4100\n",
      "Loss=361.4476\n",
      "Val Loss=425.4147\n",
      "-----\n",
      "4200\n",
      "Loss=361.5667\n",
      "Val Loss=425.8189\n",
      "-----\n",
      "4300\n",
      "Loss=361.0224\n",
      "Val Loss=425.0169\n",
      "-----\n",
      "4400\n",
      "Loss=360.6242\n",
      "Val Loss=424.3706\n",
      "-----\n",
      "4500\n",
      "Loss=359.7036\n",
      "Val Loss=422.7572\n",
      "-----\n",
      "4600\n",
      "Loss=358.5341\n",
      "Val Loss=420.6340\n",
      "-----\n",
      "4700\n",
      "Loss=357.6155\n",
      "Val Loss=419.0080\n",
      "-----\n",
      "4800\n",
      "Loss=355.7886\n",
      "Val Loss=415.5068\n",
      "-----\n",
      "4900\n",
      "Loss=355.2842\n",
      "Val Loss=414.7288\n",
      "-----\n",
      "5000\n",
      "Loss=355.5103\n",
      "Val Loss=415.4456\n",
      "-----\n",
      "5100\n",
      "Loss=355.3832\n",
      "Val Loss=415.4041\n",
      "-----\n",
      "5200\n",
      "Loss=355.4253\n",
      "Val Loss=415.7044\n",
      "-----\n",
      "5300\n",
      "Loss=355.3907\n",
      "Val Loss=415.8365\n",
      "-----\n",
      "5400\n",
      "Loss=355.0202\n",
      "Val Loss=415.2882\n",
      "-----\n",
      "5500\n",
      "Loss=355.2706\n",
      "Val Loss=415.9778\n",
      "-----\n",
      "5600\n",
      "Loss=355.4169\n",
      "Val Loss=416.4919\n",
      "-----\n",
      "5700\n",
      "Loss=355.0399\n",
      "Val Loss=415.9427\n",
      "-----\n",
      "5800\n",
      "Loss=356.0314\n",
      "Val Loss=418.2155\n",
      "-----\n",
      "5900\n",
      "Loss=356.6409\n",
      "Val Loss=419.5242\n",
      "-----\n",
      "6000\n",
      "Loss=356.3663\n",
      "Val Loss=419.1824\n",
      "-----\n",
      "6100\n",
      "Loss=356.0518\n",
      "Val Loss=418.7368\n",
      "-----\n",
      "6200\n",
      "Loss=355.7024\n",
      "Val Loss=418.2462\n",
      "-----\n",
      "6300\n",
      "Loss=355.5407\n",
      "Val Loss=418.1396\n",
      "-----\n",
      "6400\n",
      "Loss=355.2729\n",
      "Val Loss=417.7886\n",
      "-----\n",
      "6500\n",
      "Loss=355.3227\n",
      "Val Loss=418.1324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-70c5842ab558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, 20)\n",
    "        self.lin3 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=42581.4375\n",
      "Val Loss=10719.4131\n",
      "-----\n",
      "100\n",
      "Loss=1306.5361\n",
      "Val Loss=2321.2966\n",
      "-----\n",
      "200\n",
      "Loss=850.5865\n",
      "Val Loss=1298.9583\n",
      "-----\n",
      "300\n",
      "Loss=689.8105\n",
      "Val Loss=990.5787\n",
      "-----\n",
      "400\n",
      "Loss=608.4941\n",
      "Val Loss=843.3696\n",
      "-----\n",
      "500\n",
      "Loss=557.8242\n",
      "Val Loss=754.6586\n",
      "-----\n",
      "600\n",
      "Loss=524.5273\n",
      "Val Loss=698.3322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9897ec0aae13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=37606.9297\n",
      "Val Loss=11558.6992\n",
      "-----\n",
      "100\n",
      "Loss=1368.6725\n",
      "Val Loss=2553.0046\n",
      "-----\n",
      "200\n",
      "Loss=955.1909\n",
      "Val Loss=1541.4231\n",
      "-----\n",
      "300\n",
      "Loss=795.8029\n",
      "Val Loss=1208.0969\n",
      "-----\n",
      "400\n",
      "Loss=709.5984\n",
      "Val Loss=1038.4926\n",
      "-----\n",
      "500\n",
      "Loss=646.3178\n",
      "Val Loss=915.2849\n",
      "-----\n",
      "600\n",
      "Loss=610.1110\n",
      "Val Loss=848.4869\n",
      "-----\n",
      "700\n",
      "Loss=574.7874\n",
      "Val Loss=783.7828\n",
      "-----\n",
      "800\n",
      "Loss=548.4568\n",
      "Val Loss=736.1489\n",
      "-----\n",
      "900\n",
      "Loss=527.2394\n",
      "Val Loss=699.5761\n",
      "-----\n",
      "1000\n",
      "Loss=510.4058\n",
      "Val Loss=671.8802\n",
      "-----\n",
      "1100\n",
      "Loss=492.3026\n",
      "Val Loss=641.2138\n",
      "-----\n",
      "1200\n",
      "Loss=477.6333\n",
      "Val Loss=615.8599\n",
      "-----\n",
      "1300\n",
      "Loss=468.2047\n",
      "Val Loss=600.1415\n",
      "-----\n",
      "1400\n",
      "Loss=455.9815\n",
      "Val Loss=580.1837\n",
      "-----\n",
      "1500\n",
      "Loss=448.0787\n",
      "Val Loss=567.5602\n",
      "-----\n",
      "1600\n",
      "Loss=443.2267\n",
      "Val Loss=560.1873\n",
      "-----\n",
      "1700\n",
      "Loss=435.6959\n",
      "Val Loss=547.8133\n",
      "-----\n",
      "1800\n",
      "Loss=430.2346\n",
      "Val Loss=539.0519\n",
      "-----\n",
      "1900\n",
      "Loss=426.4042\n",
      "Val Loss=532.8874\n",
      "-----\n",
      "2000\n",
      "Loss=427.5907\n",
      "Val Loss=535.1319\n",
      "-----\n",
      "2100\n",
      "Loss=422.4556\n",
      "Val Loss=526.8211\n",
      "-----\n",
      "2200\n",
      "Loss=417.0105\n",
      "Val Loss=518.0982\n",
      "-----\n",
      "2300\n",
      "Loss=411.3961\n",
      "Val Loss=508.8803\n",
      "-----\n",
      "2400\n",
      "Loss=409.5310\n",
      "Val Loss=506.0508\n",
      "-----\n",
      "2500\n",
      "Loss=403.2860\n",
      "Val Loss=495.9080\n",
      "-----\n",
      "2600\n",
      "Loss=399.6589\n",
      "Val Loss=490.1894\n",
      "-----\n",
      "2700\n",
      "Loss=399.5225\n",
      "Val Loss=490.3832\n",
      "-----\n",
      "2800\n",
      "Loss=393.0695\n",
      "Val Loss=479.7225\n",
      "-----\n",
      "2900\n",
      "Loss=388.8851\n",
      "Val Loss=472.8209\n",
      "-----\n",
      "3000\n",
      "Loss=389.8295\n",
      "Val Loss=474.7453\n",
      "-----\n",
      "3100\n",
      "Loss=385.7337\n",
      "Val Loss=468.1717\n",
      "-----\n",
      "3200\n",
      "Loss=381.8192\n",
      "Val Loss=461.5987\n",
      "-----\n",
      "3300\n",
      "Loss=382.3410\n",
      "Val Loss=462.9368\n",
      "-----\n",
      "3400\n",
      "Loss=379.6094\n",
      "Val Loss=458.3533\n",
      "-----\n",
      "3500\n",
      "Loss=375.9757\n",
      "Val Loss=452.1883\n",
      "-----\n",
      "3600\n",
      "Loss=372.9839\n",
      "Val Loss=447.1633\n",
      "-----\n",
      "3700\n",
      "Loss=373.3954\n",
      "Val Loss=448.1543\n",
      "-----\n",
      "3800\n",
      "Loss=376.3696\n",
      "Val Loss=453.6820\n",
      "-----\n",
      "3900\n",
      "Loss=373.6752\n",
      "Val Loss=449.1070\n",
      "-----\n",
      "4000\n",
      "Loss=373.0878\n",
      "Val Loss=448.2971\n",
      "-----\n",
      "4100\n",
      "Loss=382.9492\n",
      "Val Loss=465.8482\n",
      "-----\n",
      "4200\n",
      "Loss=386.0651\n",
      "Val Loss=471.4705\n",
      "-----\n",
      "4300\n",
      "Loss=380.1551\n",
      "Val Loss=461.4237\n",
      "-----\n",
      "4400\n",
      "Loss=381.3483\n",
      "Val Loss=463.7497\n",
      "-----\n",
      "4500\n",
      "Loss=379.3018\n",
      "Val Loss=460.4229\n",
      "-----\n",
      "4600\n",
      "Loss=374.3010\n",
      "Val Loss=451.9472\n",
      "-----\n",
      "4700\n",
      "Loss=370.4363\n",
      "Val Loss=445.3591\n",
      "-----\n",
      "4800\n",
      "Loss=369.5335\n",
      "Val Loss=443.8275\n",
      "-----\n",
      "4900\n",
      "Loss=369.8821\n",
      "Val Loss=444.6233\n",
      "-----\n",
      "5000\n",
      "Loss=371.1474\n",
      "Val Loss=447.0870\n",
      "-----\n",
      "5100\n",
      "Loss=371.4919\n",
      "Val Loss=447.8318\n",
      "-----\n",
      "5200\n",
      "Loss=369.5501\n",
      "Val Loss=444.5900\n",
      "-----\n",
      "5300\n",
      "Loss=367.2194\n",
      "Val Loss=440.5443\n",
      "-----\n",
      "5400\n",
      "Loss=365.4702\n",
      "Val Loss=437.5421\n",
      "-----\n",
      "5500\n",
      "Loss=365.0505\n",
      "Val Loss=437.0020\n",
      "-----\n",
      "5600\n",
      "Loss=364.9515\n",
      "Val Loss=436.9209\n",
      "-----\n",
      "5700\n",
      "Loss=364.5691\n",
      "Val Loss=436.4106\n",
      "-----\n",
      "5800\n",
      "Loss=364.1082\n",
      "Val Loss=435.6190\n",
      "-----\n",
      "5900\n",
      "Loss=362.6435\n",
      "Val Loss=433.0969\n",
      "-----\n",
      "6000\n",
      "Loss=361.4625\n",
      "Val Loss=431.0419\n",
      "-----\n",
      "6100\n",
      "Loss=361.0588\n",
      "Val Loss=430.4490\n",
      "-----\n",
      "6200\n",
      "Loss=361.2514\n",
      "Val Loss=430.9656\n",
      "-----\n",
      "6300\n",
      "Loss=361.5981\n",
      "Val Loss=431.7760\n",
      "-----\n",
      "6400\n",
      "Loss=361.5125\n",
      "Val Loss=431.7606\n",
      "-----\n",
      "6500\n",
      "Loss=361.0179\n",
      "Val Loss=430.9814\n",
      "-----\n",
      "6600\n",
      "Loss=360.3048\n",
      "Val Loss=429.7789\n",
      "-----\n",
      "6700\n",
      "Loss=359.7066\n",
      "Val Loss=428.8293\n",
      "-----\n",
      "6800\n",
      "Loss=359.5605\n",
      "Val Loss=428.7033\n",
      "-----\n",
      "6900\n",
      "Loss=359.5238\n",
      "Val Loss=428.7367\n",
      "-----\n",
      "7000\n",
      "Loss=359.4157\n",
      "Val Loss=428.6599\n",
      "-----\n",
      "7100\n",
      "Loss=359.5300\n",
      "Val Loss=429.0182\n",
      "-----\n",
      "7200\n",
      "Loss=359.7588\n",
      "Val Loss=429.6122\n",
      "-----\n",
      "7300\n",
      "Loss=359.9913\n",
      "Val Loss=430.1755\n",
      "-----\n",
      "7400\n",
      "Loss=360.0109\n",
      "Val Loss=430.3483\n",
      "-----\n",
      "7500\n",
      "Loss=359.7708\n",
      "Val Loss=430.0252\n",
      "-----\n",
      "7600\n",
      "Loss=359.8484\n",
      "Val Loss=430.3162\n",
      "-----\n",
      "7700\n",
      "Loss=359.9969\n",
      "Val Loss=430.7301\n",
      "-----\n",
      "7800\n",
      "Loss=359.9460\n",
      "Val Loss=430.7636\n",
      "-----\n",
      "7900\n",
      "Loss=360.0738\n",
      "Val Loss=431.1246\n",
      "-----\n",
      "8000\n",
      "Loss=360.2249\n",
      "Val Loss=431.5795\n",
      "-----\n",
      "8100\n",
      "Loss=360.4987\n",
      "Val Loss=432.2188\n",
      "-----\n",
      "8200\n",
      "Loss=360.4989\n",
      "Val Loss=432.3384\n",
      "-----\n",
      "8300\n",
      "Loss=360.2130\n",
      "Val Loss=431.9329\n",
      "-----\n",
      "8400\n",
      "Loss=360.0841\n",
      "Val Loss=431.8096\n",
      "-----\n",
      "8500\n",
      "Loss=360.0184\n",
      "Val Loss=431.8214\n",
      "-----\n",
      "8600\n",
      "Loss=360.1673\n",
      "Val Loss=432.2284\n",
      "-----\n",
      "8700\n",
      "Loss=360.2498\n",
      "Val Loss=432.5141\n",
      "-----\n",
      "8800\n",
      "Loss=360.2021\n",
      "Val Loss=432.5626\n",
      "-----\n",
      "8900\n",
      "Loss=360.1523\n",
      "Val Loss=432.5879\n",
      "-----\n",
      "9000\n",
      "Loss=360.1832\n",
      "Val Loss=432.7579\n",
      "-----\n",
      "9100\n",
      "Loss=360.2911\n",
      "Val Loss=433.0720\n",
      "-----\n",
      "9200\n",
      "Loss=360.2441\n",
      "Val Loss=433.1069\n",
      "-----\n",
      "9300\n",
      "Loss=360.1350\n",
      "Val Loss=433.0006\n",
      "-----\n",
      "9400\n",
      "Loss=360.1742\n",
      "Val Loss=433.1856\n",
      "-----\n",
      "9500\n",
      "Loss=360.2268\n",
      "Val Loss=433.3943\n",
      "-----\n",
      "9600\n",
      "Loss=360.3090\n",
      "Val Loss=433.6687\n",
      "-----\n",
      "9700\n",
      "Loss=360.5567\n",
      "Val Loss=434.2415\n",
      "-----\n",
      "9800\n",
      "Loss=360.8636\n",
      "Val Loss=434.9050\n",
      "-----\n",
      "9900\n",
      "Loss=360.9444\n",
      "Val Loss=435.1669\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, 20)\n",
    "        self.lin3 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 10)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score,min_score = y.max(),y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=6283.0562\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "10\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "20\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "30\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "40\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "50\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "60\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "70\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "80\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "90\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "100\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "110\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "120\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "130\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "140\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f0f5211d0a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = F.relu(self.lin2(x))\n",
    "        x = F.sigmoid(self.lin2(x)) * (max_score-min_score+1) + min_score-0.5\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin4): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=37447.8125\n",
      "Val Loss=26164.3418\n",
      "-----\n",
      "10\n",
      "Loss=4057.9683\n",
      "Val Loss=4649.6416\n",
      "-----\n",
      "20\n",
      "Loss=2523.9739\n",
      "Val Loss=3129.1521\n",
      "-----\n",
      "30\n",
      "Loss=1748.1691\n",
      "Val Loss=2246.6807\n",
      "-----\n",
      "40\n",
      "Loss=1251.9819\n",
      "Val Loss=1484.7157\n",
      "-----\n",
      "50\n",
      "Loss=3541.5464\n",
      "Val Loss=1832.9534\n",
      "-----\n",
      "60\n",
      "Loss=2437.2241\n",
      "Val Loss=1412.3756\n",
      "-----\n",
      "70\n",
      "Loss=1915.5098\n",
      "Val Loss=1156.5074\n",
      "-----\n",
      "80\n",
      "Loss=1648.2031\n",
      "Val Loss=1023.6251\n",
      "-----\n",
      "90\n",
      "Loss=1560.4374\n",
      "Val Loss=958.4852\n",
      "-----\n",
      "100\n",
      "Loss=1453.0210\n",
      "Val Loss=896.4127\n",
      "-----\n",
      "110\n",
      "Loss=1346.1403\n",
      "Val Loss=842.2306\n",
      "-----\n",
      "120\n",
      "Loss=1250.8499\n",
      "Val Loss=794.8298\n",
      "-----\n",
      "130\n",
      "Loss=1162.4333\n",
      "Val Loss=753.2693\n",
      "-----\n",
      "140\n",
      "Loss=1089.8679\n",
      "Val Loss=718.8808\n",
      "-----\n",
      "150\n",
      "Loss=1024.3414\n",
      "Val Loss=687.1143\n",
      "-----\n",
      "160\n",
      "Loss=964.9587\n",
      "Val Loss=659.1415\n",
      "-----\n",
      "170\n",
      "Loss=914.0073\n",
      "Val Loss=634.7504\n",
      "-----\n",
      "180\n",
      "Loss=871.6451\n",
      "Val Loss=613.9793\n",
      "-----\n",
      "190\n",
      "Loss=833.2755\n",
      "Val Loss=595.1749\n",
      "-----\n",
      "200\n",
      "Loss=800.9696\n",
      "Val Loss=578.9271\n",
      "-----\n",
      "210\n",
      "Loss=770.3821\n",
      "Val Loss=563.3556\n",
      "-----\n",
      "220\n",
      "Loss=743.0634\n",
      "Val Loss=549.5952\n",
      "-----\n",
      "230\n",
      "Loss=719.1339\n",
      "Val Loss=537.1641\n",
      "-----\n",
      "240\n",
      "Loss=697.5311\n",
      "Val Loss=526.3766\n",
      "-----\n",
      "250\n",
      "Loss=679.2608\n",
      "Val Loss=516.9741\n",
      "-----\n",
      "260\n",
      "Loss=661.7529\n",
      "Val Loss=508.1165\n",
      "-----\n",
      "270\n",
      "Loss=647.0465\n",
      "Val Loss=500.6179\n",
      "-----\n",
      "280\n",
      "Loss=632.6282\n",
      "Val Loss=493.0283\n",
      "-----\n",
      "290\n",
      "Loss=620.3017\n",
      "Val Loss=486.7352\n",
      "-----\n",
      "300\n",
      "Loss=608.0845\n",
      "Val Loss=480.4954\n",
      "-----\n",
      "310\n",
      "Loss=597.1663\n",
      "Val Loss=474.9512\n",
      "-----\n",
      "320\n",
      "Loss=585.6809\n",
      "Val Loss=469.1995\n",
      "-----\n",
      "330\n",
      "Loss=574.5640\n",
      "Val Loss=463.6263\n",
      "-----\n",
      "340\n",
      "Loss=565.7253\n",
      "Val Loss=459.1885\n",
      "-----\n",
      "350\n",
      "Loss=557.1280\n",
      "Val Loss=454.9077\n",
      "-----\n",
      "360\n",
      "Loss=549.8862\n",
      "Val Loss=451.4419\n",
      "-----\n",
      "370\n",
      "Loss=543.6667\n",
      "Val Loss=448.2429\n",
      "-----\n",
      "380\n",
      "Loss=536.8134\n",
      "Val Loss=445.0639\n",
      "-----\n",
      "390\n",
      "Loss=531.9646\n",
      "Val Loss=442.6245\n",
      "-----\n",
      "400\n",
      "Loss=525.8113\n",
      "Val Loss=439.3785\n",
      "-----\n",
      "410\n",
      "Loss=519.4305\n",
      "Val Loss=436.0942\n",
      "-----\n",
      "420\n",
      "Loss=513.7729\n",
      "Val Loss=433.2394\n",
      "-----\n",
      "430\n",
      "Loss=508.8152\n",
      "Val Loss=430.8198\n",
      "-----\n",
      "440\n",
      "Loss=504.6475\n",
      "Val Loss=428.6999\n",
      "-----\n",
      "450\n",
      "Loss=500.8715\n",
      "Val Loss=426.7246\n",
      "-----\n",
      "460\n",
      "Loss=496.7569\n",
      "Val Loss=424.7225\n",
      "-----\n",
      "470\n",
      "Loss=493.0015\n",
      "Val Loss=422.8799\n",
      "-----\n",
      "480\n",
      "Loss=490.3830\n",
      "Val Loss=421.6147\n",
      "-----\n",
      "490\n",
      "Loss=487.1273\n",
      "Val Loss=419.9056\n",
      "-----\n",
      "500\n",
      "Loss=483.8248\n",
      "Val Loss=418.2505\n",
      "-----\n",
      "510\n",
      "Loss=480.6190\n",
      "Val Loss=416.5864\n",
      "-----\n",
      "520\n",
      "Loss=477.0618\n",
      "Val Loss=414.8917\n",
      "-----\n",
      "530\n",
      "Loss=473.6910\n",
      "Val Loss=413.2916\n",
      "-----\n",
      "540\n",
      "Loss=470.6732\n",
      "Val Loss=411.9474\n",
      "-----\n",
      "550\n",
      "Loss=468.2523\n",
      "Val Loss=410.7889\n",
      "-----\n",
      "560\n",
      "Loss=465.6012\n",
      "Val Loss=409.5282\n",
      "-----\n",
      "570\n",
      "Loss=462.9658\n",
      "Val Loss=408.2622\n",
      "-----\n",
      "580\n",
      "Loss=460.5450\n",
      "Val Loss=407.0560\n",
      "-----\n",
      "590\n",
      "Loss=457.9605\n",
      "Val Loss=405.7615\n",
      "-----\n",
      "600\n",
      "Loss=455.2212\n",
      "Val Loss=404.5021\n",
      "-----\n",
      "610\n",
      "Loss=453.1022\n",
      "Val Loss=403.4650\n",
      "-----\n",
      "620\n",
      "Loss=450.9941\n",
      "Val Loss=402.3986\n",
      "-----\n",
      "630\n",
      "Loss=448.1793\n",
      "Val Loss=400.9672\n",
      "-----\n",
      "640\n",
      "Loss=445.8024\n",
      "Val Loss=399.9203\n",
      "-----\n",
      "650\n",
      "Loss=444.6358\n",
      "Val Loss=399.4961\n",
      "-----\n",
      "660\n",
      "Loss=444.5019\n",
      "Val Loss=399.5383\n",
      "-----\n",
      "670\n",
      "Loss=444.4148\n",
      "Val Loss=399.4860\n",
      "-----\n",
      "680\n",
      "Loss=443.3302\n",
      "Val Loss=398.8576\n",
      "-----\n",
      "690\n",
      "Loss=440.7293\n",
      "Val Loss=397.5834\n",
      "-----\n",
      "700\n",
      "Loss=437.9739\n",
      "Val Loss=396.2948\n",
      "-----\n",
      "710\n",
      "Loss=435.5699\n",
      "Val Loss=395.1948\n",
      "-----\n",
      "720\n",
      "Loss=433.5442\n",
      "Val Loss=394.2599\n",
      "-----\n",
      "730\n",
      "Loss=431.7228\n",
      "Val Loss=393.4276\n",
      "-----\n",
      "740\n",
      "Loss=430.3162\n",
      "Val Loss=392.8405\n",
      "-----\n",
      "750\n",
      "Loss=428.9170\n",
      "Val Loss=392.1959\n",
      "-----\n",
      "760\n",
      "Loss=427.3840\n",
      "Val Loss=391.5919\n",
      "-----\n",
      "770\n",
      "Loss=426.3945\n",
      "Val Loss=391.1922\n",
      "-----\n",
      "780\n",
      "Loss=425.3922\n",
      "Val Loss=390.7541\n",
      "-----\n",
      "790\n",
      "Loss=424.0637\n",
      "Val Loss=390.2160\n",
      "-----\n",
      "800\n",
      "Loss=422.7231\n",
      "Val Loss=389.6653\n",
      "-----\n",
      "810\n",
      "Loss=421.4984\n",
      "Val Loss=389.1758\n",
      "-----\n",
      "820\n",
      "Loss=420.4643\n",
      "Val Loss=388.7617\n",
      "-----\n",
      "830\n",
      "Loss=419.2599\n",
      "Val Loss=388.2431\n",
      "-----\n",
      "840\n",
      "Loss=417.8727\n",
      "Val Loss=387.6311\n",
      "-----\n",
      "850\n",
      "Loss=416.5820\n",
      "Val Loss=387.0863\n",
      "-----\n",
      "860\n",
      "Loss=415.2983\n",
      "Val Loss=386.5403\n",
      "-----\n",
      "870\n",
      "Loss=414.4043\n",
      "Val Loss=386.1792\n",
      "-----\n",
      "880\n",
      "Loss=413.7115\n",
      "Val Loss=385.9022\n",
      "-----\n",
      "890\n",
      "Loss=413.0226\n",
      "Val Loss=385.6905\n",
      "-----\n",
      "900\n",
      "Loss=412.7365\n",
      "Val Loss=385.6394\n",
      "-----\n",
      "910\n",
      "Loss=412.5659\n",
      "Val Loss=385.5404\n",
      "-----\n",
      "920\n",
      "Loss=411.6447\n",
      "Val Loss=385.1082\n",
      "-----\n",
      "930\n",
      "Loss=410.3079\n",
      "Val Loss=384.4761\n",
      "-----\n",
      "940\n",
      "Loss=408.4960\n",
      "Val Loss=383.6647\n",
      "-----\n",
      "950\n",
      "Loss=406.6981\n",
      "Val Loss=382.8976\n",
      "-----\n",
      "960\n",
      "Loss=405.1230\n",
      "Val Loss=382.2211\n",
      "-----\n",
      "970\n",
      "Loss=403.7541\n",
      "Val Loss=381.6414\n",
      "-----\n",
      "980\n",
      "Loss=402.5822\n",
      "Val Loss=381.1619\n",
      "-----\n",
      "990\n",
      "Loss=401.5605\n",
      "Val Loss=380.7801\n",
      "-----\n",
      "1000\n",
      "Loss=401.0236\n",
      "Val Loss=380.6009\n",
      "-----\n",
      "1010\n",
      "Loss=400.4601\n",
      "Val Loss=380.3611\n",
      "-----\n",
      "1020\n",
      "Loss=399.7281\n",
      "Val Loss=380.0847\n",
      "-----\n",
      "1030\n",
      "Loss=399.2162\n",
      "Val Loss=379.9317\n",
      "-----\n",
      "1040\n",
      "Loss=399.2146\n",
      "Val Loss=379.9785\n",
      "-----\n",
      "1050\n",
      "Loss=399.2337\n",
      "Val Loss=379.9617\n",
      "-----\n",
      "1060\n",
      "Loss=398.7922\n",
      "Val Loss=379.7782\n",
      "-----\n",
      "1070\n",
      "Loss=398.1843\n",
      "Val Loss=379.5608\n",
      "-----\n",
      "1080\n",
      "Loss=397.3899\n",
      "Val Loss=379.2650\n",
      "-----\n",
      "1090\n",
      "Loss=396.8074\n",
      "Val Loss=379.0331\n",
      "-----\n",
      "1100\n",
      "Loss=396.2210\n",
      "Val Loss=378.7638\n",
      "-----\n",
      "1110\n",
      "Loss=395.3122\n",
      "Val Loss=378.3910\n",
      "-----\n",
      "1120\n",
      "Loss=394.2828\n",
      "Val Loss=377.9433\n",
      "-----\n",
      "1130\n",
      "Loss=393.1253\n",
      "Val Loss=377.4826\n",
      "-----\n",
      "1140\n",
      "Loss=391.9166\n",
      "Val Loss=377.0046\n",
      "-----\n",
      "1150\n",
      "Loss=390.9108\n",
      "Val Loss=376.6219\n",
      "-----\n",
      "1160\n",
      "Loss=389.9227\n",
      "Val Loss=376.2446\n",
      "-----\n",
      "1170\n",
      "Loss=388.9682\n",
      "Val Loss=375.8870\n",
      "-----\n",
      "1180\n",
      "Loss=388.1418\n",
      "Val Loss=375.6028\n",
      "-----\n",
      "1190\n",
      "Loss=387.6283\n",
      "Val Loss=375.4472\n",
      "-----\n",
      "1200\n",
      "Loss=387.2011\n",
      "Val Loss=375.2990\n",
      "-----\n",
      "1210\n",
      "Loss=386.5762\n",
      "Val Loss=375.0668\n",
      "-----\n",
      "1220\n",
      "Loss=385.8430\n",
      "Val Loss=374.8357\n",
      "-----\n",
      "1230\n",
      "Loss=385.5374\n",
      "Val Loss=374.7483\n",
      "-----\n",
      "1240\n",
      "Loss=385.2321\n",
      "Val Loss=374.6632\n",
      "-----\n",
      "1250\n",
      "Loss=384.9656\n",
      "Val Loss=374.5789\n",
      "-----\n",
      "1260\n",
      "Loss=384.5611\n",
      "Val Loss=374.4333\n",
      "-----\n",
      "1270\n",
      "Loss=383.8993\n",
      "Val Loss=374.1904\n",
      "-----\n",
      "1280\n",
      "Loss=383.1811\n",
      "Val Loss=373.9467\n",
      "-----\n",
      "1290\n",
      "Loss=382.5097\n",
      "Val Loss=373.7121\n",
      "-----\n",
      "1300\n",
      "Loss=381.9807\n",
      "Val Loss=373.5364\n",
      "-----\n",
      "1310\n",
      "Loss=381.4985\n",
      "Val Loss=373.3704\n",
      "-----\n",
      "1320\n",
      "Loss=380.8022\n",
      "Val Loss=373.1155\n",
      "-----\n",
      "1330\n",
      "Loss=379.8558\n",
      "Val Loss=372.7812\n",
      "-----\n",
      "1340\n",
      "Loss=378.8657\n",
      "Val Loss=372.4407\n",
      "-----\n",
      "1350\n",
      "Loss=377.7942\n",
      "Val Loss=372.0746\n",
      "-----\n",
      "1360\n",
      "Loss=376.8129\n",
      "Val Loss=371.7646\n",
      "-----\n",
      "1370\n",
      "Loss=376.0313\n",
      "Val Loss=371.5208\n",
      "-----\n",
      "1380\n",
      "Loss=375.3330\n",
      "Val Loss=371.3254\n",
      "-----\n",
      "1390\n",
      "Loss=374.9203\n",
      "Val Loss=371.2143\n",
      "-----\n",
      "1400\n",
      "Loss=374.6479\n",
      "Val Loss=371.1297\n",
      "-----\n",
      "1410\n",
      "Loss=374.3262\n",
      "Val Loss=371.0228\n",
      "-----\n",
      "1420\n",
      "Loss=374.1032\n",
      "Val Loss=370.9609\n",
      "-----\n",
      "1430\n",
      "Loss=373.8662\n",
      "Val Loss=370.8909\n",
      "-----\n",
      "1440\n",
      "Loss=373.6433\n",
      "Val Loss=370.8289\n",
      "-----\n",
      "1450\n",
      "Loss=373.3937\n",
      "Val Loss=370.7491\n",
      "-----\n",
      "1460\n",
      "Loss=373.0664\n",
      "Val Loss=370.6493\n",
      "-----\n",
      "1470\n",
      "Loss=372.7152\n",
      "Val Loss=370.5389\n",
      "-----\n",
      "1480\n",
      "Loss=372.2968\n",
      "Val Loss=370.4103\n",
      "-----\n",
      "1490\n",
      "Loss=371.7578\n",
      "Val Loss=370.2449\n",
      "-----\n",
      "1500\n",
      "Loss=371.1781\n",
      "Val Loss=370.0845\n",
      "-----\n",
      "1510\n",
      "Loss=370.6652\n",
      "Val Loss=369.9409\n",
      "-----\n",
      "1520\n",
      "Loss=370.2209\n",
      "Val Loss=369.8204\n",
      "-----\n",
      "1530\n",
      "Loss=369.7892\n",
      "Val Loss=369.7082\n",
      "-----\n",
      "1540\n",
      "Loss=369.3012\n",
      "Val Loss=369.5846\n",
      "-----\n",
      "1550\n",
      "Loss=368.7622\n",
      "Val Loss=369.4446\n",
      "-----\n",
      "1560\n",
      "Loss=368.2319\n",
      "Val Loss=369.3124\n",
      "-----\n",
      "1570\n",
      "Loss=367.7845\n",
      "Val Loss=369.2030\n",
      "-----\n",
      "1580\n",
      "Loss=367.4170\n",
      "Val Loss=369.1230\n",
      "-----\n",
      "1590\n",
      "Loss=367.0705\n",
      "Val Loss=369.0411\n",
      "-----\n",
      "1600\n",
      "Loss=366.7440\n",
      "Val Loss=368.9689\n",
      "-----\n",
      "1610\n",
      "Loss=366.4772\n",
      "Val Loss=368.9113\n",
      "-----\n",
      "1620\n",
      "Loss=366.2748\n",
      "Val Loss=368.8671\n",
      "-----\n",
      "1630\n",
      "Loss=366.1041\n",
      "Val Loss=368.8423\n",
      "-----\n",
      "1640\n",
      "Loss=366.0643\n",
      "Val Loss=368.8443\n",
      "-----\n",
      "1650\n",
      "Loss=366.0607\n",
      "Val Loss=368.8476\n",
      "-----\n",
      "1660\n",
      "Loss=365.9634\n",
      "Val Loss=368.8330\n",
      "-----\n",
      "1670\n",
      "Loss=365.9475\n",
      "Val Loss=368.8394\n",
      "-----\n",
      "1680\n",
      "Loss=365.9719\n",
      "Val Loss=368.8528\n",
      "-----\n",
      "1690\n",
      "Loss=365.8977\n",
      "Val Loss=368.8361\n",
      "-----\n",
      "1700\n",
      "Loss=365.7121\n",
      "Val Loss=368.7969\n",
      "-----\n",
      "1710\n",
      "Loss=365.4343\n",
      "Val Loss=368.7372\n",
      "-----\n",
      "1720\n",
      "Loss=365.0949\n",
      "Val Loss=368.6642\n",
      "-----\n",
      "1730\n",
      "Loss=364.7193\n",
      "Val Loss=368.5871\n",
      "-----\n",
      "1740\n",
      "Loss=364.4589\n",
      "Val Loss=368.5324\n",
      "-----\n",
      "1750\n",
      "Loss=364.2392\n",
      "Val Loss=368.4899\n",
      "-----\n",
      "1760\n",
      "Loss=364.0299\n",
      "Val Loss=368.4462\n",
      "-----\n",
      "1770\n",
      "Loss=363.7927\n",
      "Val Loss=368.3952\n",
      "-----\n",
      "1780\n",
      "Loss=363.5234\n",
      "Val Loss=368.3413\n",
      "-----\n",
      "1790\n",
      "Loss=363.2583\n",
      "Val Loss=368.2894\n",
      "-----\n",
      "1800\n",
      "Loss=362.9942\n",
      "Val Loss=368.2436\n",
      "-----\n",
      "1810\n",
      "Loss=362.7522\n",
      "Val Loss=368.2039\n",
      "-----\n",
      "1820\n",
      "Loss=362.5439\n",
      "Val Loss=368.1670\n",
      "-----\n",
      "1830\n",
      "Loss=362.3534\n",
      "Val Loss=368.1330\n",
      "-----\n",
      "1840\n",
      "Loss=362.1846\n",
      "Val Loss=368.1077\n",
      "-----\n",
      "1850\n",
      "Loss=362.0863\n",
      "Val Loss=368.1018\n",
      "-----\n",
      "1860\n",
      "Loss=361.9856\n",
      "Val Loss=368.0947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "1870\n",
      "Loss=361.8349\n",
      "Val Loss=368.0793\n",
      "-----\n",
      "1880\n",
      "Loss=361.6346\n",
      "Val Loss=368.0490\n",
      "-----\n",
      "1890\n",
      "Loss=361.4469\n",
      "Val Loss=368.0215\n",
      "-----\n",
      "1900\n",
      "Loss=361.3196\n",
      "Val Loss=368.0057\n",
      "-----\n",
      "1910\n",
      "Loss=361.1628\n",
      "Val Loss=367.9884\n",
      "-----\n",
      "1920\n",
      "Loss=361.0459\n",
      "Val Loss=367.9825\n",
      "-----\n",
      "1930\n",
      "Loss=360.9426\n",
      "Val Loss=367.9785\n",
      "-----\n",
      "1940\n",
      "Loss=360.8513\n",
      "Val Loss=367.9758\n",
      "-----\n",
      "1950\n",
      "Loss=360.7485\n",
      "Val Loss=367.9688\n",
      "-----\n",
      "1960\n",
      "Loss=360.6475\n",
      "Val Loss=367.9595\n",
      "-----\n",
      "1970\n",
      "Loss=360.5254\n",
      "Val Loss=367.9439\n",
      "-----\n",
      "1980\n",
      "Loss=360.4198\n",
      "Val Loss=367.9345\n",
      "-----\n",
      "1990\n",
      "Loss=360.3029\n",
      "Val Loss=367.9248\n",
      "-----\n",
      "2000\n",
      "Loss=360.1508\n",
      "Val Loss=367.9129\n",
      "-----\n",
      "2010\n",
      "Loss=359.9661\n",
      "Val Loss=367.8911\n",
      "-----\n",
      "2020\n",
      "Loss=359.7601\n",
      "Val Loss=367.8645\n",
      "-----\n",
      "2030\n",
      "Loss=359.5638\n",
      "Val Loss=367.8371\n",
      "-----\n",
      "2040\n",
      "Loss=359.3549\n",
      "Val Loss=367.8121\n",
      "-----\n",
      "2050\n",
      "Loss=359.1550\n",
      "Val Loss=367.7912\n",
      "-----\n",
      "2060\n",
      "Loss=358.9401\n",
      "Val Loss=367.7663\n",
      "-----\n",
      "2070\n",
      "Loss=358.6499\n",
      "Val Loss=367.7298\n",
      "-----\n",
      "2080\n",
      "Loss=358.3561\n",
      "Val Loss=367.6936\n",
      "-----\n",
      "2090\n",
      "Loss=358.0793\n",
      "Val Loss=367.6615\n",
      "-----\n",
      "2100\n",
      "Loss=357.8161\n",
      "Val Loss=367.6371\n",
      "-----\n",
      "2110\n",
      "Loss=357.5466\n",
      "Val Loss=367.6102\n",
      "-----\n",
      "2120\n",
      "Loss=357.2715\n",
      "Val Loss=367.5850\n",
      "-----\n",
      "2130\n",
      "Loss=357.0299\n",
      "Val Loss=367.5651\n",
      "-----\n",
      "2140\n",
      "Loss=356.8190\n",
      "Val Loss=367.5450\n",
      "-----\n",
      "2150\n",
      "Loss=356.5843\n",
      "Val Loss=367.5249\n",
      "-----\n",
      "2160\n",
      "Loss=356.3773\n",
      "Val Loss=367.5067\n",
      "-----\n",
      "2170\n",
      "Loss=356.1715\n",
      "Val Loss=367.4919\n",
      "-----\n",
      "2180\n",
      "Loss=355.9797\n",
      "Val Loss=367.4781\n",
      "-----\n",
      "2190\n",
      "Loss=355.8016\n",
      "Val Loss=367.4659\n",
      "-----\n",
      "2200\n",
      "Loss=355.6535\n",
      "Val Loss=367.4564\n",
      "-----\n",
      "2210\n",
      "Loss=355.5077\n",
      "Val Loss=367.4471\n",
      "-----\n",
      "2220\n",
      "Loss=355.3434\n",
      "Val Loss=367.4358\n",
      "-----\n",
      "2230\n",
      "Loss=355.1629\n",
      "Val Loss=367.4235\n",
      "-----\n",
      "2240\n",
      "Loss=354.9787\n",
      "Val Loss=367.4127\n",
      "-----\n",
      "2250\n",
      "Loss=354.7897\n",
      "Val Loss=367.4034\n",
      "-----\n",
      "2260\n",
      "Loss=354.6195\n",
      "Val Loss=367.3952\n",
      "-----\n",
      "2270\n",
      "Loss=354.4559\n",
      "Val Loss=367.3865\n",
      "-----\n",
      "2280\n",
      "Loss=354.3298\n",
      "Val Loss=367.3800\n",
      "-----\n",
      "2290\n",
      "Loss=354.2300\n",
      "Val Loss=367.3737\n",
      "-----\n",
      "2300\n",
      "Loss=354.1460\n",
      "Val Loss=367.3687\n",
      "-----\n",
      "2310\n",
      "Loss=354.0662\n",
      "Val Loss=367.3642\n",
      "-----\n",
      "2320\n",
      "Loss=353.9645\n",
      "Val Loss=367.3547\n",
      "-----\n",
      "2330\n",
      "Loss=353.8406\n",
      "Val Loss=367.3441\n",
      "-----\n",
      "2340\n",
      "Loss=353.7368\n",
      "Val Loss=367.3342\n",
      "-----\n",
      "2350\n",
      "Loss=353.6664\n",
      "Val Loss=367.3256\n",
      "-----\n",
      "2360\n",
      "Loss=353.5968\n",
      "Val Loss=367.3182\n",
      "-----\n",
      "2370\n",
      "Loss=353.5064\n",
      "Val Loss=367.3104\n",
      "-----\n",
      "2380\n",
      "Loss=353.4092\n",
      "Val Loss=367.3034\n",
      "-----\n",
      "2390\n",
      "Loss=353.3246\n",
      "Val Loss=367.2959\n",
      "-----\n",
      "2400\n",
      "Loss=353.2518\n",
      "Val Loss=367.2895\n",
      "-----\n",
      "2410\n",
      "Loss=353.1764\n",
      "Val Loss=367.2830\n",
      "-----\n",
      "2420\n",
      "Loss=353.0995\n",
      "Val Loss=367.2766\n",
      "-----\n",
      "2430\n",
      "Loss=353.0218\n",
      "Val Loss=367.2692\n",
      "-----\n",
      "2440\n",
      "Loss=352.9619\n",
      "Val Loss=367.2603\n",
      "-----\n",
      "2450\n",
      "Loss=352.9099\n",
      "Val Loss=367.2484\n",
      "-----\n",
      "2460\n",
      "Loss=352.8390\n",
      "Val Loss=367.2377\n",
      "-----\n",
      "2470\n",
      "Loss=352.7932\n",
      "Val Loss=367.2279\n",
      "-----\n",
      "2480\n",
      "Loss=352.7589\n",
      "Val Loss=367.2165\n",
      "-----\n",
      "2490\n",
      "Loss=352.7352\n",
      "Val Loss=367.2063\n",
      "-----\n",
      "2500\n",
      "Loss=352.7143\n",
      "Val Loss=367.1963\n",
      "-----\n",
      "2510\n",
      "Loss=352.6996\n",
      "Val Loss=367.1865\n",
      "-----\n",
      "2520\n",
      "Loss=352.6845\n",
      "Val Loss=367.1783\n",
      "-----\n",
      "2530\n",
      "Loss=352.6575\n",
      "Val Loss=367.1721\n",
      "-----\n",
      "2540\n",
      "Loss=352.6442\n",
      "Val Loss=367.1651\n",
      "-----\n",
      "2550\n",
      "Loss=352.5926\n",
      "Val Loss=367.1580\n",
      "-----\n",
      "2560\n",
      "Loss=352.5668\n",
      "Val Loss=367.1517\n",
      "-----\n",
      "2570\n",
      "Loss=352.5442\n",
      "Val Loss=367.1451\n",
      "-----\n",
      "2580\n",
      "Loss=352.5214\n",
      "Val Loss=367.1393\n",
      "-----\n",
      "2590\n",
      "Loss=352.5142\n",
      "Val Loss=367.1318\n",
      "-----\n",
      "2600\n",
      "Loss=352.5012\n",
      "Val Loss=367.1259\n",
      "-----\n",
      "2610\n",
      "Loss=352.4969\n",
      "Val Loss=367.1209\n",
      "-----\n",
      "2620\n",
      "Loss=352.4626\n",
      "Val Loss=367.1158\n",
      "-----\n",
      "2630\n",
      "Loss=352.4178\n",
      "Val Loss=367.1106\n",
      "-----\n",
      "2640\n",
      "Loss=352.3608\n",
      "Val Loss=367.1057\n",
      "-----\n",
      "2650\n",
      "Loss=352.3043\n",
      "Val Loss=367.1015\n",
      "-----\n",
      "2660\n",
      "Loss=352.2378\n",
      "Val Loss=367.0980\n",
      "-----\n",
      "2670\n",
      "Loss=352.1679\n",
      "Val Loss=367.0955\n",
      "-----\n",
      "2680\n",
      "Loss=352.0865\n",
      "Val Loss=367.0936\n",
      "-----\n",
      "2690\n",
      "Loss=352.0167\n",
      "Val Loss=367.0900\n",
      "-----\n",
      "2700\n",
      "Loss=351.9536\n",
      "Val Loss=367.0853\n",
      "-----\n",
      "2710\n",
      "Loss=351.8807\n",
      "Val Loss=367.0820\n",
      "-----\n",
      "2720\n",
      "Loss=351.7991\n",
      "Val Loss=367.0790\n",
      "-----\n",
      "2730\n",
      "Loss=351.7361\n",
      "Val Loss=367.0765\n",
      "-----\n",
      "2740\n",
      "Loss=351.6793\n",
      "Val Loss=367.0745\n",
      "-----\n",
      "2750\n",
      "Loss=351.6452\n",
      "Val Loss=367.0715\n",
      "-----\n",
      "2760\n",
      "Loss=351.6367\n",
      "Val Loss=367.0687\n",
      "-----\n",
      "2770\n",
      "Loss=351.6461\n",
      "Val Loss=367.0677\n",
      "-----\n",
      "2780\n",
      "Loss=351.6318\n",
      "Val Loss=367.0659\n",
      "-----\n",
      "2790\n",
      "Loss=351.6195\n",
      "Val Loss=367.0639\n",
      "-----\n",
      "2800\n",
      "Loss=351.6154\n",
      "Val Loss=367.0610\n",
      "-----\n",
      "2810\n",
      "Loss=351.6113\n",
      "Val Loss=367.0591\n",
      "-----\n",
      "2820\n",
      "Loss=351.6168\n",
      "Val Loss=367.0567\n",
      "-----\n",
      "2830\n",
      "Loss=351.6310\n",
      "Val Loss=367.0547\n",
      "-----\n",
      "2840\n",
      "Loss=351.6228\n",
      "Val Loss=367.0527\n",
      "-----\n",
      "2850\n",
      "Loss=351.6075\n",
      "Val Loss=367.0511\n",
      "-----\n",
      "2860\n",
      "Loss=351.5770\n",
      "Val Loss=367.0491\n",
      "-----\n",
      "2870\n",
      "Loss=351.5422\n",
      "Val Loss=367.0492\n",
      "-----\n",
      "2880\n",
      "Loss=351.4997\n",
      "Val Loss=367.0480\n",
      "-----\n",
      "2890\n",
      "Loss=351.4675\n",
      "Val Loss=367.0459\n",
      "-----\n",
      "2900\n",
      "Loss=351.4586\n",
      "Val Loss=367.0427\n",
      "-----\n",
      "2910\n",
      "Loss=351.4431\n",
      "Val Loss=367.0407\n",
      "-----\n",
      "2920\n",
      "Loss=351.4384\n",
      "Val Loss=367.0389\n",
      "-----\n",
      "2930\n",
      "Loss=351.4284\n",
      "Val Loss=367.0368\n",
      "-----\n",
      "2940\n",
      "Loss=351.4043\n",
      "Val Loss=367.0356\n",
      "-----\n",
      "2950\n",
      "Loss=351.3807\n",
      "Val Loss=367.0338\n",
      "-----\n",
      "2960\n",
      "Loss=351.3669\n",
      "Val Loss=367.0314\n",
      "-----\n",
      "2970\n",
      "Loss=351.3435\n",
      "Val Loss=367.0279\n",
      "-----\n",
      "2980\n",
      "Loss=351.3028\n",
      "Val Loss=367.0249\n",
      "-----\n",
      "2990\n",
      "Loss=351.2589\n",
      "Val Loss=367.0238\n",
      "-----\n",
      "3000\n",
      "Loss=351.2289\n",
      "Val Loss=367.0218\n",
      "-----\n",
      "3010\n",
      "Loss=351.2043\n",
      "Val Loss=367.0201\n",
      "-----\n",
      "3020\n",
      "Loss=351.1709\n",
      "Val Loss=367.0183\n",
      "-----\n",
      "3030\n",
      "Loss=351.1392\n",
      "Val Loss=367.0173\n",
      "-----\n",
      "3040\n",
      "Loss=351.1008\n",
      "Val Loss=367.0161\n",
      "-----\n",
      "3050\n",
      "Loss=351.0715\n",
      "Val Loss=367.0147\n",
      "-----\n",
      "3060\n",
      "Loss=351.0777\n",
      "Val Loss=367.0117\n",
      "-----\n",
      "3070\n",
      "Loss=351.0752\n",
      "Val Loss=367.0098\n",
      "-----\n",
      "3080\n",
      "Loss=351.0692\n",
      "Val Loss=367.0075\n",
      "-----\n",
      "3090\n",
      "Loss=351.0594\n",
      "Val Loss=367.0047\n",
      "-----\n",
      "3100\n",
      "Loss=351.0664\n",
      "Val Loss=367.0026\n",
      "-----\n",
      "3110\n",
      "Loss=351.0691\n",
      "Val Loss=367.0009\n",
      "-----\n",
      "3120\n",
      "Loss=351.0328\n",
      "Val Loss=366.9993\n",
      "-----\n",
      "3130\n",
      "Loss=350.9835\n",
      "Val Loss=366.9982\n",
      "-----\n",
      "3140\n",
      "Loss=350.9308\n",
      "Val Loss=366.9977\n",
      "-----\n",
      "3150\n",
      "Loss=350.8715\n",
      "Val Loss=366.9971\n",
      "-----\n",
      "3160\n",
      "Loss=350.8288\n",
      "Val Loss=366.9951\n",
      "-----\n",
      "3170\n",
      "Loss=350.8039\n",
      "Val Loss=366.9935\n",
      "-----\n",
      "3180\n",
      "Loss=350.7870\n",
      "Val Loss=366.9933\n",
      "-----\n",
      "3190\n",
      "Loss=350.7668\n",
      "Val Loss=366.9951\n",
      "-----\n",
      "3200\n",
      "Loss=350.7386\n",
      "Val Loss=366.9963\n",
      "-----\n",
      "3210\n",
      "Loss=350.7110\n",
      "Val Loss=366.9969\n",
      "-----\n",
      "3220\n",
      "Loss=350.6918\n",
      "Val Loss=366.9951\n",
      "-----\n",
      "3230\n",
      "Loss=350.6544\n",
      "Val Loss=366.9931\n",
      "-----\n",
      "3240\n",
      "Loss=350.6312\n",
      "Val Loss=366.9917\n",
      "-----\n",
      "3250\n",
      "Loss=350.6235\n",
      "Val Loss=366.9908\n",
      "-----\n",
      "3260\n",
      "Loss=350.5969\n",
      "Val Loss=366.9910\n",
      "-----\n",
      "3270\n",
      "Loss=350.5750\n",
      "Val Loss=366.9900\n",
      "-----\n",
      "3280\n",
      "Loss=350.5435\n",
      "Val Loss=366.9879\n",
      "-----\n",
      "3290\n",
      "Loss=350.5127\n",
      "Val Loss=366.9863\n",
      "-----\n",
      "3300\n",
      "Loss=350.4998\n",
      "Val Loss=366.9847\n",
      "-----\n",
      "3310\n",
      "Loss=350.4953\n",
      "Val Loss=366.9828\n",
      "-----\n",
      "3320\n",
      "Loss=350.4904\n",
      "Val Loss=366.9802\n",
      "-----\n",
      "3330\n",
      "Loss=350.5096\n",
      "Val Loss=366.9768\n",
      "-----\n",
      "3340\n",
      "Loss=350.5132\n",
      "Val Loss=366.9720\n",
      "-----\n",
      "3350\n",
      "Loss=350.5092\n",
      "Val Loss=366.9681\n",
      "-----\n",
      "3360\n",
      "Loss=350.5067\n",
      "Val Loss=366.9645\n",
      "-----\n",
      "3370\n",
      "Loss=350.5129\n",
      "Val Loss=366.9605\n",
      "-----\n",
      "3380\n",
      "Loss=350.5239\n",
      "Val Loss=366.9573\n",
      "-----\n",
      "3390\n",
      "Loss=350.5352\n",
      "Val Loss=366.9555\n",
      "-----\n",
      "3400\n",
      "Loss=350.5601\n",
      "Val Loss=366.9543\n",
      "-----\n",
      "3410\n",
      "Loss=350.5754\n",
      "Val Loss=366.9521\n",
      "-----\n",
      "3420\n",
      "Loss=350.5952\n",
      "Val Loss=366.9496\n",
      "-----\n",
      "3430\n",
      "Loss=350.6234\n",
      "Val Loss=366.9474\n",
      "-----\n",
      "3440\n",
      "Loss=350.6544\n",
      "Val Loss=366.9471\n",
      "-----\n",
      "3450\n",
      "Loss=350.6723\n",
      "Val Loss=366.9459\n",
      "-----\n",
      "3460\n",
      "Loss=350.6836\n",
      "Val Loss=366.9450\n",
      "-----\n",
      "3470\n",
      "Loss=350.6923\n",
      "Val Loss=366.9445\n",
      "-----\n",
      "3480\n",
      "Loss=350.7016\n",
      "Val Loss=366.9442\n",
      "-----\n",
      "3490\n",
      "Loss=350.6870\n",
      "Val Loss=366.9450\n",
      "-----\n",
      "3500\n",
      "Loss=350.6930\n",
      "Val Loss=366.9455\n",
      "-----\n",
      "3510\n",
      "Loss=350.7085\n",
      "Val Loss=366.9463\n",
      "-----\n",
      "3520\n",
      "Loss=350.7164\n",
      "Val Loss=366.9478\n",
      "-----\n",
      "3530\n",
      "Loss=350.7137\n",
      "Val Loss=366.9500\n",
      "-----\n",
      "3540\n",
      "Loss=350.7057\n",
      "Val Loss=366.9516\n",
      "-----\n",
      "3550\n",
      "Loss=350.6931\n",
      "Val Loss=366.9531\n",
      "-----\n",
      "3560\n",
      "Loss=350.6857\n",
      "Val Loss=366.9534\n",
      "-----\n",
      "3570\n",
      "Loss=350.6956\n",
      "Val Loss=366.9529\n",
      "-----\n",
      "3580\n",
      "Loss=350.7025\n",
      "Val Loss=366.9532\n",
      "-----\n",
      "3590\n",
      "Loss=350.7130\n",
      "Val Loss=366.9526\n",
      "-----\n",
      "3600\n",
      "Loss=350.7298\n",
      "Val Loss=366.9519\n",
      "-----\n",
      "3610\n",
      "Loss=350.7596\n",
      "Val Loss=366.9511\n",
      "-----\n",
      "3620\n",
      "Loss=350.7827\n",
      "Val Loss=366.9503\n",
      "-----\n",
      "3630\n",
      "Loss=350.8318\n",
      "Val Loss=366.9502\n",
      "-----\n",
      "3640\n",
      "Loss=350.8963\n",
      "Val Loss=366.9512\n",
      "-----\n",
      "3650\n",
      "Loss=350.9587\n",
      "Val Loss=366.9533\n",
      "-----\n",
      "3660\n",
      "Loss=351.0265\n",
      "Val Loss=366.9550\n",
      "-----\n",
      "3670\n",
      "Loss=351.0853\n",
      "Val Loss=366.9575\n",
      "-----\n",
      "3680\n",
      "Loss=351.1384\n",
      "Val Loss=366.9616\n",
      "-----\n",
      "3690\n",
      "Loss=351.1748\n",
      "Val Loss=366.9620\n",
      "-----\n",
      "3700\n",
      "Loss=351.2068\n",
      "Val Loss=366.9628\n",
      "-----\n",
      "3710\n",
      "Loss=351.2263\n",
      "Val Loss=366.9633\n",
      "-----\n",
      "3720\n",
      "Loss=351.2248\n",
      "Val Loss=366.9619\n",
      "-----\n",
      "3730\n",
      "Loss=351.2178\n",
      "Val Loss=366.9612\n",
      "-----\n",
      "3740\n",
      "Loss=351.2102\n",
      "Val Loss=366.9608\n",
      "-----\n",
      "3750\n",
      "Loss=351.1932\n",
      "Val Loss=366.9594\n",
      "-----\n",
      "3760\n",
      "Loss=351.1702\n",
      "Val Loss=366.9598\n",
      "-----\n",
      "3770\n",
      "Loss=351.1508\n",
      "Val Loss=366.9601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "3780\n",
      "Loss=351.1407\n",
      "Val Loss=366.9616\n",
      "-----\n",
      "3790\n",
      "Loss=351.1372\n",
      "Val Loss=366.9626\n",
      "-----\n",
      "3800\n",
      "Loss=351.1367\n",
      "Val Loss=366.9631\n",
      "-----\n",
      "3810\n",
      "Loss=351.1435\n",
      "Val Loss=366.9644\n",
      "-----\n",
      "3820\n",
      "Loss=351.1481\n",
      "Val Loss=366.9652\n",
      "-----\n",
      "3830\n",
      "Loss=351.1569\n",
      "Val Loss=366.9666\n",
      "-----\n",
      "3840\n",
      "Loss=351.1796\n",
      "Val Loss=366.9685\n",
      "-----\n",
      "3850\n",
      "Loss=351.1960\n",
      "Val Loss=366.9700\n",
      "-----\n",
      "3860\n",
      "Loss=351.2157\n",
      "Val Loss=366.9695\n",
      "-----\n",
      "3870\n",
      "Loss=351.1949\n",
      "Val Loss=366.9675\n",
      "-----\n",
      "3880\n",
      "Loss=351.1584\n",
      "Val Loss=366.9645\n",
      "-----\n",
      "3890\n",
      "Loss=351.0965\n",
      "Val Loss=366.9604\n",
      "-----\n",
      "3900\n",
      "Loss=351.0178\n",
      "Val Loss=366.9553\n",
      "-----\n",
      "3910\n",
      "Loss=350.9276\n",
      "Val Loss=366.9493\n",
      "-----\n",
      "3920\n",
      "Loss=350.8267\n",
      "Val Loss=366.9427\n",
      "-----\n",
      "3930\n",
      "Loss=350.7206\n",
      "Val Loss=366.9373\n",
      "-----\n",
      "3940\n",
      "Loss=350.6189\n",
      "Val Loss=366.9316\n",
      "-----\n",
      "3950\n",
      "Loss=350.5289\n",
      "Val Loss=366.9292\n",
      "-----\n",
      "3960\n",
      "Loss=350.4233\n",
      "Val Loss=366.9273\n",
      "-----\n",
      "3970\n",
      "Loss=350.3360\n",
      "Val Loss=366.9282\n",
      "-----\n",
      "3980\n",
      "Loss=350.2718\n",
      "Val Loss=366.9289\n",
      "-----\n",
      "3990\n",
      "Loss=350.2083\n",
      "Val Loss=366.9306\n",
      "-----\n",
      "4000\n",
      "Loss=350.1631\n",
      "Val Loss=366.9312\n",
      "-----\n",
      "4010\n",
      "Loss=350.1266\n",
      "Val Loss=366.9309\n",
      "-----\n",
      "4020\n",
      "Loss=350.0717\n",
      "Val Loss=366.9319\n",
      "-----\n",
      "4030\n",
      "Loss=350.0307\n",
      "Val Loss=366.9322\n",
      "-----\n",
      "4040\n",
      "Loss=349.9917\n",
      "Val Loss=366.9327\n",
      "-----\n",
      "4050\n",
      "Loss=349.9482\n",
      "Val Loss=366.9319\n",
      "-----\n",
      "4060\n",
      "Loss=349.9249\n",
      "Val Loss=366.9316\n",
      "-----\n",
      "4070\n",
      "Loss=349.9082\n",
      "Val Loss=366.9323\n",
      "-----\n",
      "4080\n",
      "Loss=349.8931\n",
      "Val Loss=366.9334\n",
      "-----\n",
      "4090\n",
      "Loss=349.8651\n",
      "Val Loss=366.9337\n",
      "-----\n",
      "4100\n",
      "Loss=349.8334\n",
      "Val Loss=366.9343\n",
      "-----\n",
      "4110\n",
      "Loss=349.7954\n",
      "Val Loss=366.9347\n",
      "-----\n",
      "4120\n",
      "Loss=349.7561\n",
      "Val Loss=366.9344\n",
      "-----\n",
      "4130\n",
      "Loss=349.7076\n",
      "Val Loss=366.9343\n",
      "-----\n",
      "4140\n",
      "Loss=349.6355\n",
      "Val Loss=366.9328\n",
      "-----\n",
      "4150\n",
      "Loss=349.5556\n",
      "Val Loss=366.9330\n",
      "-----\n",
      "4160\n",
      "Loss=349.4777\n",
      "Val Loss=366.9342\n",
      "-----\n",
      "4170\n",
      "Loss=349.4067\n",
      "Val Loss=366.9352\n",
      "-----\n",
      "4180\n",
      "Loss=349.3330\n",
      "Val Loss=366.9370\n",
      "-----\n",
      "4190\n",
      "Loss=349.2641\n",
      "Val Loss=366.9391\n",
      "-----\n",
      "4200\n",
      "Loss=349.1861\n",
      "Val Loss=366.9404\n",
      "-----\n",
      "4210\n",
      "Loss=349.1136\n",
      "Val Loss=366.9419\n",
      "-----\n",
      "4220\n",
      "Loss=349.0298\n",
      "Val Loss=366.9433\n",
      "-----\n",
      "4230\n",
      "Loss=348.9622\n",
      "Val Loss=366.9429\n",
      "-----\n",
      "4240\n",
      "Loss=348.9164\n",
      "Val Loss=366.9424\n",
      "-----\n",
      "4250\n",
      "Loss=348.8717\n",
      "Val Loss=366.9426\n",
      "-----\n",
      "4260\n",
      "Loss=348.8168\n",
      "Val Loss=366.9416\n",
      "-----\n",
      "4270\n",
      "Loss=348.7316\n",
      "Val Loss=366.9385\n",
      "-----\n",
      "4280\n",
      "Loss=348.6588\n",
      "Val Loss=366.9370\n",
      "-----\n",
      "4290\n",
      "Loss=348.5937\n",
      "Val Loss=366.9353\n",
      "-----\n",
      "4300\n",
      "Loss=348.5330\n",
      "Val Loss=366.9360\n",
      "-----\n",
      "4310\n",
      "Loss=348.4835\n",
      "Val Loss=366.9368\n",
      "-----\n",
      "4320\n",
      "Loss=348.4489\n",
      "Val Loss=366.9384\n",
      "-----\n",
      "4330\n",
      "Loss=348.4120\n",
      "Val Loss=366.9395\n",
      "-----\n",
      "4340\n",
      "Loss=348.3703\n",
      "Val Loss=366.9413\n",
      "-----\n",
      "4350\n",
      "Loss=348.3268\n",
      "Val Loss=366.9435\n",
      "-----\n",
      "4360\n",
      "Loss=348.2849\n",
      "Val Loss=366.9465\n",
      "-----\n",
      "4370\n",
      "Loss=348.2543\n",
      "Val Loss=366.9501\n",
      "-----\n",
      "4380\n",
      "Loss=348.2285\n",
      "Val Loss=366.9518\n",
      "-----\n",
      "4390\n",
      "Loss=348.2076\n",
      "Val Loss=366.9521\n",
      "-----\n",
      "4400\n",
      "Loss=348.1863\n",
      "Val Loss=366.9520\n",
      "-----\n",
      "4410\n",
      "Loss=348.1743\n",
      "Val Loss=366.9530\n",
      "-----\n",
      "4420\n",
      "Loss=348.1404\n",
      "Val Loss=366.9554\n",
      "-----\n",
      "4430\n",
      "Loss=348.0889\n",
      "Val Loss=366.9585\n",
      "-----\n",
      "4440\n",
      "Loss=348.0501\n",
      "Val Loss=366.9617\n",
      "-----\n",
      "4450\n",
      "Loss=348.0191\n",
      "Val Loss=366.9635\n",
      "-----\n",
      "4460\n",
      "Loss=348.0005\n",
      "Val Loss=366.9650\n",
      "-----\n",
      "4470\n",
      "Loss=347.9691\n",
      "Val Loss=366.9658\n",
      "-----\n",
      "4480\n",
      "Loss=347.9547\n",
      "Val Loss=366.9660\n",
      "-----\n",
      "4490\n",
      "Loss=347.9258\n",
      "Val Loss=366.9641\n",
      "-----\n",
      "4500\n",
      "Loss=347.8709\n",
      "Val Loss=366.9635\n",
      "-----\n",
      "4510\n",
      "Loss=347.8078\n",
      "Val Loss=366.9645\n",
      "-----\n",
      "4520\n",
      "Loss=347.7434\n",
      "Val Loss=366.9679\n",
      "-----\n",
      "4530\n",
      "Loss=347.6751\n",
      "Val Loss=366.9696\n",
      "-----\n",
      "4540\n",
      "Loss=347.6006\n",
      "Val Loss=366.9700\n",
      "-----\n",
      "4550\n",
      "Loss=347.5220\n",
      "Val Loss=366.9746\n",
      "-----\n",
      "4560\n",
      "Loss=347.4492\n",
      "Val Loss=366.9807\n",
      "-----\n",
      "4570\n",
      "Loss=347.3685\n",
      "Val Loss=366.9872\n",
      "-----\n",
      "4580\n",
      "Loss=347.3077\n",
      "Val Loss=366.9926\n",
      "-----\n",
      "4590\n",
      "Loss=347.2603\n",
      "Val Loss=366.9964\n",
      "-----\n",
      "4600\n",
      "Loss=347.2249\n",
      "Val Loss=366.9987\n",
      "-----\n",
      "4610\n",
      "Loss=347.2114\n",
      "Val Loss=367.0003\n",
      "-----\n",
      "4620\n",
      "Loss=347.2131\n",
      "Val Loss=367.0002\n",
      "-----\n",
      "4630\n",
      "Loss=347.2216\n",
      "Val Loss=366.9973\n",
      "-----\n",
      "4640\n",
      "Loss=347.2285\n",
      "Val Loss=366.9914\n",
      "-----\n",
      "4650\n",
      "Loss=347.2216\n",
      "Val Loss=366.9847\n",
      "-----\n",
      "4660\n",
      "Loss=347.2001\n",
      "Val Loss=366.9776\n",
      "-----\n",
      "4670\n",
      "Loss=347.1591\n",
      "Val Loss=366.9772\n",
      "-----\n",
      "4680\n",
      "Loss=347.1205\n",
      "Val Loss=366.9796\n",
      "-----\n",
      "4690\n",
      "Loss=347.0916\n",
      "Val Loss=366.9821\n",
      "-----\n",
      "4700\n",
      "Loss=347.0779\n",
      "Val Loss=366.9830\n",
      "-----\n",
      "4710\n",
      "Loss=347.0816\n",
      "Val Loss=366.9821\n",
      "-----\n",
      "4720\n",
      "Loss=347.1152\n",
      "Val Loss=366.9791\n",
      "-----\n",
      "4730\n",
      "Loss=347.1741\n",
      "Val Loss=366.9743\n",
      "-----\n",
      "4740\n",
      "Loss=347.2344\n",
      "Val Loss=366.9682\n",
      "-----\n",
      "4750\n",
      "Loss=347.2918\n",
      "Val Loss=366.9624\n",
      "-----\n",
      "4760\n",
      "Loss=347.3407\n",
      "Val Loss=366.9589\n",
      "-----\n",
      "4770\n",
      "Loss=347.3975\n",
      "Val Loss=366.9569\n",
      "-----\n",
      "4780\n",
      "Loss=347.4481\n",
      "Val Loss=366.9546\n",
      "-----\n",
      "4790\n",
      "Loss=347.5044\n",
      "Val Loss=366.9546\n",
      "-----\n",
      "4800\n",
      "Loss=347.5830\n",
      "Val Loss=366.9530\n",
      "-----\n",
      "4810\n",
      "Loss=347.6609\n",
      "Val Loss=366.9526\n",
      "-----\n",
      "4820\n",
      "Loss=347.7381\n",
      "Val Loss=366.9526\n",
      "-----\n",
      "4830\n",
      "Loss=347.8166\n",
      "Val Loss=366.9527\n",
      "-----\n",
      "4840\n",
      "Loss=347.8910\n",
      "Val Loss=366.9522\n",
      "-----\n",
      "4850\n",
      "Loss=347.9526\n",
      "Val Loss=366.9529\n",
      "-----\n",
      "4860\n",
      "Loss=347.9910\n",
      "Val Loss=366.9539\n",
      "-----\n",
      "4870\n",
      "Loss=348.0089\n",
      "Val Loss=366.9554\n",
      "-----\n",
      "4880\n",
      "Loss=348.0269\n",
      "Val Loss=366.9570\n",
      "-----\n",
      "4890\n",
      "Loss=348.0460\n",
      "Val Loss=366.9587\n",
      "-----\n",
      "4900\n",
      "Loss=348.0659\n",
      "Val Loss=366.9599\n",
      "-----\n",
      "4910\n",
      "Loss=348.0698\n",
      "Val Loss=366.9619\n",
      "-----\n",
      "4920\n",
      "Loss=348.0656\n",
      "Val Loss=366.9639\n",
      "-----\n",
      "4930\n",
      "Loss=348.0585\n",
      "Val Loss=366.9657\n",
      "-----\n",
      "4940\n",
      "Loss=348.0423\n",
      "Val Loss=366.9671\n",
      "-----\n",
      "4950\n",
      "Loss=348.0187\n",
      "Val Loss=366.9684\n",
      "-----\n",
      "4960\n",
      "Loss=347.9917\n",
      "Val Loss=366.9684\n",
      "-----\n",
      "4970\n",
      "Loss=347.9608\n",
      "Val Loss=366.9693\n",
      "-----\n",
      "4980\n",
      "Loss=347.9178\n",
      "Val Loss=366.9705\n",
      "-----\n",
      "4990\n",
      "Loss=347.8716\n",
      "Val Loss=366.9716\n",
      "-----\n",
      "5000\n",
      "Loss=347.8367\n",
      "Val Loss=366.9733\n",
      "-----\n",
      "5010\n",
      "Loss=347.7870\n",
      "Val Loss=366.9761\n",
      "-----\n",
      "5020\n",
      "Loss=347.7270\n",
      "Val Loss=366.9787\n",
      "-----\n",
      "5030\n",
      "Loss=347.6534\n",
      "Val Loss=366.9835\n",
      "-----\n",
      "5040\n",
      "Loss=347.5743\n",
      "Val Loss=366.9878\n",
      "-----\n",
      "5050\n",
      "Loss=347.5141\n",
      "Val Loss=366.9911\n",
      "-----\n",
      "5060\n",
      "Loss=347.4741\n",
      "Val Loss=366.9937\n",
      "-----\n",
      "5070\n",
      "Loss=347.4405\n",
      "Val Loss=366.9970\n",
      "-----\n",
      "5080\n",
      "Loss=347.4190\n",
      "Val Loss=366.9992\n",
      "-----\n",
      "5090\n",
      "Loss=347.3974\n",
      "Val Loss=367.0006\n",
      "-----\n",
      "5100\n",
      "Loss=347.3780\n",
      "Val Loss=367.0025\n",
      "-----\n",
      "5110\n",
      "Loss=347.3745\n",
      "Val Loss=367.0034\n",
      "-----\n",
      "5120\n",
      "Loss=347.3761\n",
      "Val Loss=367.0035\n",
      "-----\n",
      "5130\n",
      "Loss=347.3799\n",
      "Val Loss=367.0042\n",
      "-----\n",
      "5140\n",
      "Loss=347.3841\n",
      "Val Loss=367.0045\n",
      "-----\n",
      "5150\n",
      "Loss=347.4042\n",
      "Val Loss=367.0055\n",
      "-----\n",
      "5160\n",
      "Loss=347.4339\n",
      "Val Loss=367.0071\n",
      "-----\n",
      "5170\n",
      "Loss=347.4444\n",
      "Val Loss=367.0085\n",
      "-----\n",
      "5180\n",
      "Loss=347.4477\n",
      "Val Loss=367.0108\n",
      "-----\n",
      "5190\n",
      "Loss=347.4403\n",
      "Val Loss=367.0135\n",
      "-----\n",
      "5200\n",
      "Loss=347.4387\n",
      "Val Loss=367.0161\n",
      "-----\n",
      "5210\n",
      "Loss=347.4504\n",
      "Val Loss=367.0183\n",
      "-----\n",
      "5220\n",
      "Loss=347.4590\n",
      "Val Loss=367.0206\n",
      "-----\n",
      "5230\n",
      "Loss=347.4632\n",
      "Val Loss=367.0231\n",
      "-----\n",
      "5240\n",
      "Loss=347.4693\n",
      "Val Loss=367.0244\n",
      "-----\n",
      "5250\n",
      "Loss=347.4321\n",
      "Val Loss=367.0267\n",
      "-----\n",
      "5260\n",
      "Loss=347.3860\n",
      "Val Loss=367.0292\n",
      "-----\n",
      "5270\n",
      "Loss=347.3398\n",
      "Val Loss=367.0312\n",
      "-----\n",
      "5280\n",
      "Loss=347.3052\n",
      "Val Loss=367.0328\n",
      "-----\n",
      "5290\n",
      "Loss=347.2663\n",
      "Val Loss=367.0361\n",
      "-----\n",
      "5300\n",
      "Loss=347.2253\n",
      "Val Loss=367.0358\n",
      "-----\n",
      "5310\n",
      "Loss=347.1664\n",
      "Val Loss=367.0362\n",
      "-----\n",
      "5320\n",
      "Loss=347.1042\n",
      "Val Loss=367.0354\n",
      "-----\n",
      "5330\n",
      "Loss=347.0606\n",
      "Val Loss=367.0341\n",
      "-----\n",
      "5340\n",
      "Loss=347.0365\n",
      "Val Loss=367.0321\n",
      "-----\n",
      "5350\n",
      "Loss=347.0171\n",
      "Val Loss=367.0306\n",
      "-----\n",
      "5360\n",
      "Loss=346.9963\n",
      "Val Loss=367.0309\n",
      "-----\n",
      "5370\n",
      "Loss=346.9845\n",
      "Val Loss=367.0302\n",
      "-----\n",
      "5380\n",
      "Loss=346.9752\n",
      "Val Loss=367.0306\n",
      "-----\n",
      "5390\n",
      "Loss=347.0003\n",
      "Val Loss=367.0312\n",
      "-----\n",
      "5400\n",
      "Loss=347.0291\n",
      "Val Loss=367.0311\n",
      "-----\n",
      "5410\n",
      "Loss=347.0744\n",
      "Val Loss=367.0313\n",
      "-----\n",
      "5420\n",
      "Loss=347.1358\n",
      "Val Loss=367.0301\n",
      "-----\n",
      "5430\n",
      "Loss=347.1944\n",
      "Val Loss=367.0301\n",
      "-----\n",
      "5440\n",
      "Loss=347.2538\n",
      "Val Loss=367.0307\n",
      "-----\n",
      "5450\n",
      "Loss=347.3128\n",
      "Val Loss=367.0301\n",
      "-----\n",
      "5460\n",
      "Loss=347.3631\n",
      "Val Loss=367.0294\n",
      "-----\n",
      "5470\n",
      "Loss=347.4099\n",
      "Val Loss=367.0296\n",
      "-----\n",
      "5480\n",
      "Loss=347.4623\n",
      "Val Loss=367.0275\n",
      "-----\n",
      "5490\n",
      "Loss=347.4967\n",
      "Val Loss=367.0253\n",
      "-----\n",
      "5500\n",
      "Loss=347.5303\n",
      "Val Loss=367.0229\n",
      "-----\n",
      "5510\n",
      "Loss=347.5640\n",
      "Val Loss=367.0209\n",
      "-----\n",
      "5520\n",
      "Loss=347.6083\n",
      "Val Loss=367.0197\n",
      "-----\n",
      "5530\n",
      "Loss=347.6693\n",
      "Val Loss=367.0211\n",
      "-----\n",
      "5540\n",
      "Loss=347.7443\n",
      "Val Loss=367.0236\n",
      "-----\n",
      "5550\n",
      "Loss=347.8232\n",
      "Val Loss=367.0263\n",
      "-----\n",
      "5560\n",
      "Loss=347.8945\n",
      "Val Loss=367.0272\n",
      "-----\n",
      "5570\n",
      "Loss=347.9397\n",
      "Val Loss=367.0246\n",
      "-----\n",
      "5580\n",
      "Loss=347.9836\n",
      "Val Loss=367.0190\n",
      "-----\n",
      "5590\n",
      "Loss=348.0136\n",
      "Val Loss=367.0156\n",
      "-----\n",
      "5600\n",
      "Loss=348.0491\n",
      "Val Loss=367.0155\n",
      "-----\n",
      "5610\n",
      "Loss=348.0835\n",
      "Val Loss=367.0163\n",
      "-----\n",
      "5620\n",
      "Loss=348.1067\n",
      "Val Loss=367.0178\n",
      "-----\n",
      "5630\n",
      "Loss=348.1251\n",
      "Val Loss=367.0184\n",
      "-----\n",
      "5640\n",
      "Loss=348.1394\n",
      "Val Loss=367.0188\n",
      "-----\n",
      "5650\n",
      "Loss=348.1458\n",
      "Val Loss=367.0188\n",
      "-----\n",
      "5660\n",
      "Loss=348.1685\n",
      "Val Loss=367.0211\n",
      "-----\n",
      "5670\n",
      "Loss=348.1849\n",
      "Val Loss=367.0231\n",
      "-----\n",
      "5680\n",
      "Loss=348.2215\n",
      "Val Loss=367.0251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "5690\n",
      "Loss=348.2589\n",
      "Val Loss=367.0282\n",
      "-----\n",
      "5700\n",
      "Loss=348.2944\n",
      "Val Loss=367.0307\n",
      "-----\n",
      "5710\n",
      "Loss=348.3342\n",
      "Val Loss=367.0330\n",
      "-----\n",
      "5720\n",
      "Loss=348.3683\n",
      "Val Loss=367.0344\n",
      "-----\n",
      "5730\n",
      "Loss=348.3923\n",
      "Val Loss=367.0335\n",
      "-----\n",
      "5740\n",
      "Loss=348.4009\n",
      "Val Loss=367.0334\n",
      "-----\n",
      "5750\n",
      "Loss=348.4045\n",
      "Val Loss=367.0350\n",
      "-----\n",
      "5760\n",
      "Loss=348.3972\n",
      "Val Loss=367.0344\n",
      "-----\n",
      "5770\n",
      "Loss=348.3726\n",
      "Val Loss=367.0330\n",
      "-----\n",
      "5780\n",
      "Loss=348.3239\n",
      "Val Loss=367.0285\n",
      "-----\n",
      "5790\n",
      "Loss=348.2537\n",
      "Val Loss=367.0244\n",
      "-----\n",
      "5800\n",
      "Loss=348.1790\n",
      "Val Loss=367.0220\n",
      "-----\n",
      "5810\n",
      "Loss=348.1092\n",
      "Val Loss=367.0216\n",
      "-----\n",
      "5820\n",
      "Loss=348.0515\n",
      "Val Loss=367.0208\n",
      "-----\n",
      "5830\n",
      "Loss=347.9853\n",
      "Val Loss=367.0205\n",
      "-----\n",
      "5840\n",
      "Loss=347.9016\n",
      "Val Loss=367.0182\n",
      "-----\n",
      "5850\n",
      "Loss=347.8198\n",
      "Val Loss=367.0168\n",
      "-----\n",
      "5860\n",
      "Loss=347.7242\n",
      "Val Loss=367.0147\n",
      "-----\n",
      "5870\n",
      "Loss=347.6218\n",
      "Val Loss=367.0126\n",
      "-----\n",
      "5880\n",
      "Loss=347.5251\n",
      "Val Loss=367.0109\n",
      "-----\n",
      "5890\n",
      "Loss=347.4322\n",
      "Val Loss=367.0096\n",
      "-----\n",
      "5900\n",
      "Loss=347.3222\n",
      "Val Loss=367.0095\n",
      "-----\n",
      "5910\n",
      "Loss=347.2242\n",
      "Val Loss=367.0109\n",
      "-----\n",
      "5920\n",
      "Loss=347.1522\n",
      "Val Loss=367.0117\n",
      "-----\n",
      "5930\n",
      "Loss=347.0913\n",
      "Val Loss=367.0130\n",
      "-----\n",
      "5940\n",
      "Loss=347.0419\n",
      "Val Loss=367.0152\n",
      "-----\n",
      "5950\n",
      "Loss=346.9977\n",
      "Val Loss=367.0170\n",
      "-----\n",
      "5960\n",
      "Loss=346.9770\n",
      "Val Loss=367.0190\n",
      "-----\n",
      "5970\n",
      "Loss=346.9731\n",
      "Val Loss=367.0213\n",
      "-----\n",
      "5980\n",
      "Loss=346.9908\n",
      "Val Loss=367.0240\n",
      "-----\n",
      "5990\n",
      "Loss=347.0175\n",
      "Val Loss=367.0260\n",
      "-----\n",
      "6000\n",
      "Loss=347.0508\n",
      "Val Loss=367.0299\n",
      "-----\n",
      "6010\n",
      "Loss=347.0980\n",
      "Val Loss=367.0335\n",
      "-----\n",
      "6020\n",
      "Loss=347.1577\n",
      "Val Loss=367.0383\n",
      "-----\n",
      "6030\n",
      "Loss=347.2510\n",
      "Val Loss=367.0440\n",
      "-----\n",
      "6040\n",
      "Loss=347.3446\n",
      "Val Loss=367.0495\n",
      "-----\n",
      "6050\n",
      "Loss=347.4426\n",
      "Val Loss=367.0554\n",
      "-----\n",
      "6060\n",
      "Loss=347.5368\n",
      "Val Loss=367.0608\n",
      "-----\n",
      "6070\n",
      "Loss=347.6408\n",
      "Val Loss=367.0650\n",
      "-----\n",
      "6080\n",
      "Loss=347.6673\n",
      "Val Loss=367.0652\n",
      "-----\n",
      "6090\n",
      "Loss=347.6419\n",
      "Val Loss=367.0631\n",
      "-----\n",
      "6100\n",
      "Loss=347.5835\n",
      "Val Loss=367.0616\n",
      "-----\n",
      "6110\n",
      "Loss=347.5110\n",
      "Val Loss=367.0591\n",
      "-----\n",
      "6120\n",
      "Loss=347.3938\n",
      "Val Loss=367.0555\n",
      "-----\n",
      "6130\n",
      "Loss=347.2786\n",
      "Val Loss=367.0514\n",
      "-----\n",
      "6140\n",
      "Loss=347.1761\n",
      "Val Loss=367.0482\n",
      "-----\n",
      "6150\n",
      "Loss=347.0769\n",
      "Val Loss=367.0457\n",
      "-----\n",
      "6160\n",
      "Loss=346.9911\n",
      "Val Loss=367.0433\n",
      "-----\n",
      "6170\n",
      "Loss=346.9167\n",
      "Val Loss=367.0420\n",
      "-----\n",
      "6180\n",
      "Loss=346.8513\n",
      "Val Loss=367.0410\n",
      "-----\n",
      "6190\n",
      "Loss=346.7916\n",
      "Val Loss=367.0386\n",
      "-----\n",
      "6200\n",
      "Loss=346.7472\n",
      "Val Loss=367.0370\n",
      "-----\n",
      "6210\n",
      "Loss=346.7102\n",
      "Val Loss=367.0367\n",
      "-----\n",
      "6220\n",
      "Loss=346.6870\n",
      "Val Loss=367.0355\n",
      "-----\n",
      "6230\n",
      "Loss=346.6692\n",
      "Val Loss=367.0349\n",
      "-----\n",
      "6240\n",
      "Loss=346.6595\n",
      "Val Loss=367.0345\n",
      "-----\n",
      "6250\n",
      "Loss=346.6661\n",
      "Val Loss=367.0341\n",
      "-----\n",
      "6260\n",
      "Loss=346.6842\n",
      "Val Loss=367.0351\n",
      "-----\n",
      "6270\n",
      "Loss=346.7204\n",
      "Val Loss=367.0351\n",
      "-----\n",
      "6280\n",
      "Loss=346.7783\n",
      "Val Loss=367.0368\n",
      "-----\n",
      "6290\n",
      "Loss=346.8343\n",
      "Val Loss=367.0386\n",
      "-----\n",
      "6300\n",
      "Loss=346.8765\n",
      "Val Loss=367.0411\n",
      "-----\n",
      "6310\n",
      "Loss=346.9136\n",
      "Val Loss=367.0441\n",
      "-----\n",
      "6320\n",
      "Loss=346.9987\n",
      "Val Loss=367.0478\n",
      "-----\n",
      "6330\n",
      "Loss=347.0883\n",
      "Val Loss=367.0515\n",
      "-----\n",
      "6340\n",
      "Loss=347.1900\n",
      "Val Loss=367.0556\n",
      "-----\n",
      "6350\n",
      "Loss=347.2998\n",
      "Val Loss=367.0606\n",
      "-----\n",
      "6360\n",
      "Loss=347.4104\n",
      "Val Loss=367.0666\n",
      "-----\n",
      "6370\n",
      "Loss=347.4932\n",
      "Val Loss=367.0712\n",
      "-----\n",
      "6380\n",
      "Loss=347.5674\n",
      "Val Loss=367.0770\n",
      "-----\n",
      "6390\n",
      "Loss=347.6249\n",
      "Val Loss=367.0830\n",
      "-----\n",
      "6400\n",
      "Loss=347.6821\n",
      "Val Loss=367.0895\n",
      "-----\n",
      "6410\n",
      "Loss=347.7144\n",
      "Val Loss=367.0937\n",
      "-----\n",
      "6420\n",
      "Loss=347.7099\n",
      "Val Loss=367.0957\n",
      "-----\n",
      "6430\n",
      "Loss=347.6728\n",
      "Val Loss=367.0961\n",
      "-----\n",
      "6440\n",
      "Loss=347.6200\n",
      "Val Loss=367.0970\n",
      "-----\n",
      "6450\n",
      "Loss=347.5753\n",
      "Val Loss=367.0974\n",
      "-----\n",
      "6460\n",
      "Loss=347.5171\n",
      "Val Loss=367.0947\n",
      "-----\n",
      "6470\n",
      "Loss=347.4467\n",
      "Val Loss=367.0924\n",
      "-----\n",
      "6480\n",
      "Loss=347.3793\n",
      "Val Loss=367.0905\n",
      "-----\n",
      "6490\n",
      "Loss=347.3137\n",
      "Val Loss=367.0879\n",
      "-----\n",
      "6500\n",
      "Loss=347.2652\n",
      "Val Loss=367.0864\n",
      "-----\n",
      "6510\n",
      "Loss=347.2177\n",
      "Val Loss=367.0848\n",
      "-----\n",
      "6520\n",
      "Loss=347.1840\n",
      "Val Loss=367.0844\n",
      "-----\n",
      "6530\n",
      "Loss=347.1451\n",
      "Val Loss=367.0841\n",
      "-----\n",
      "6540\n",
      "Loss=347.1141\n",
      "Val Loss=367.0837\n",
      "-----\n",
      "6550\n",
      "Loss=347.0959\n",
      "Val Loss=367.0844\n",
      "-----\n",
      "6560\n",
      "Loss=347.0810\n",
      "Val Loss=367.0844\n",
      "-----\n",
      "6570\n",
      "Loss=347.0638\n",
      "Val Loss=367.0844\n",
      "-----\n",
      "6580\n",
      "Loss=347.0363\n",
      "Val Loss=367.0847\n",
      "-----\n",
      "6590\n",
      "Loss=347.0053\n",
      "Val Loss=367.0842\n",
      "-----\n",
      "6600\n",
      "Loss=346.9637\n",
      "Val Loss=367.0834\n",
      "-----\n",
      "6610\n",
      "Loss=346.9197\n",
      "Val Loss=367.0819\n",
      "-----\n",
      "6620\n",
      "Loss=346.8912\n",
      "Val Loss=367.0806\n",
      "-----\n",
      "6630\n",
      "Loss=346.8621\n",
      "Val Loss=367.0798\n",
      "-----\n",
      "6640\n",
      "Loss=346.8463\n",
      "Val Loss=367.0813\n",
      "-----\n",
      "6650\n",
      "Loss=346.8339\n",
      "Val Loss=367.0827\n",
      "-----\n",
      "6660\n",
      "Loss=346.8343\n",
      "Val Loss=367.0828\n",
      "-----\n",
      "6670\n",
      "Loss=346.8392\n",
      "Val Loss=367.0834\n",
      "-----\n",
      "6680\n",
      "Loss=346.8632\n",
      "Val Loss=367.0831\n",
      "-----\n",
      "6690\n",
      "Loss=346.8859\n",
      "Val Loss=367.0849\n",
      "-----\n",
      "6700\n",
      "Loss=346.9367\n",
      "Val Loss=367.0854\n",
      "-----\n",
      "6710\n",
      "Loss=346.9886\n",
      "Val Loss=367.0869\n",
      "-----\n",
      "6720\n",
      "Loss=347.0490\n",
      "Val Loss=367.0882\n",
      "-----\n",
      "6730\n",
      "Loss=347.0837\n",
      "Val Loss=367.0892\n",
      "-----\n",
      "6740\n",
      "Loss=347.1260\n",
      "Val Loss=367.0916\n",
      "-----\n",
      "6750\n",
      "Loss=347.1891\n",
      "Val Loss=367.0938\n",
      "-----\n",
      "6760\n",
      "Loss=347.2397\n",
      "Val Loss=367.0968\n",
      "-----\n",
      "6770\n",
      "Loss=347.2851\n",
      "Val Loss=367.1000\n",
      "-----\n",
      "6780\n",
      "Loss=347.3421\n",
      "Val Loss=367.1028\n",
      "-----\n",
      "6790\n",
      "Loss=347.3978\n",
      "Val Loss=367.1066\n",
      "-----\n",
      "6800\n",
      "Loss=347.4518\n",
      "Val Loss=367.1096\n",
      "-----\n",
      "6810\n",
      "Loss=347.5087\n",
      "Val Loss=367.1145\n",
      "-----\n",
      "6820\n",
      "Loss=347.5753\n",
      "Val Loss=367.1185\n",
      "-----\n",
      "6830\n",
      "Loss=347.6298\n",
      "Val Loss=367.1240\n",
      "-----\n",
      "6840\n",
      "Loss=347.6690\n",
      "Val Loss=367.1264\n",
      "-----\n",
      "6850\n",
      "Loss=347.6842\n",
      "Val Loss=367.1272\n",
      "-----\n",
      "6860\n",
      "Loss=347.6802\n",
      "Val Loss=367.1284\n",
      "-----\n",
      "6870\n",
      "Loss=347.6626\n",
      "Val Loss=367.1275\n",
      "-----\n",
      "6880\n",
      "Loss=347.6288\n",
      "Val Loss=367.1246\n",
      "-----\n",
      "6890\n",
      "Loss=347.5849\n",
      "Val Loss=367.1206\n",
      "-----\n",
      "6900\n",
      "Loss=347.5313\n",
      "Val Loss=367.1166\n",
      "-----\n",
      "6910\n",
      "Loss=347.4771\n",
      "Val Loss=367.1123\n",
      "-----\n",
      "6920\n",
      "Loss=347.4188\n",
      "Val Loss=367.1073\n",
      "-----\n",
      "6930\n",
      "Loss=347.3532\n",
      "Val Loss=367.1024\n",
      "-----\n",
      "6940\n",
      "Loss=347.2885\n",
      "Val Loss=367.0981\n",
      "-----\n",
      "6950\n",
      "Loss=347.2349\n",
      "Val Loss=367.0940\n",
      "-----\n",
      "6960\n",
      "Loss=347.1683\n",
      "Val Loss=367.0898\n",
      "-----\n",
      "6970\n",
      "Loss=347.1149\n",
      "Val Loss=367.0858\n",
      "-----\n",
      "6980\n",
      "Loss=347.0862\n",
      "Val Loss=367.0828\n",
      "-----\n",
      "6990\n",
      "Loss=347.0456\n",
      "Val Loss=367.0807\n",
      "-----\n",
      "7000\n",
      "Loss=347.0029\n",
      "Val Loss=367.0788\n",
      "-----\n",
      "7010\n",
      "Loss=346.9661\n",
      "Val Loss=367.0766\n",
      "-----\n",
      "7020\n",
      "Loss=346.9257\n",
      "Val Loss=367.0745\n",
      "-----\n",
      "7030\n",
      "Loss=346.8989\n",
      "Val Loss=367.0722\n",
      "-----\n",
      "7040\n",
      "Loss=346.8730\n",
      "Val Loss=367.0708\n",
      "-----\n",
      "7050\n",
      "Loss=346.8791\n",
      "Val Loss=367.0710\n",
      "-----\n",
      "7060\n",
      "Loss=346.8897\n",
      "Val Loss=367.0707\n",
      "-----\n",
      "7070\n",
      "Loss=346.9110\n",
      "Val Loss=367.0728\n",
      "-----\n",
      "7080\n",
      "Loss=346.9434\n",
      "Val Loss=367.0746\n",
      "-----\n",
      "7090\n",
      "Loss=346.9696\n",
      "Val Loss=367.0762\n",
      "-----\n",
      "7100\n",
      "Loss=347.0010\n",
      "Val Loss=367.0791\n",
      "-----\n",
      "7110\n",
      "Loss=347.0307\n",
      "Val Loss=367.0819\n",
      "-----\n",
      "7120\n",
      "Loss=347.0777\n",
      "Val Loss=367.0846\n",
      "-----\n",
      "7130\n",
      "Loss=347.1258\n",
      "Val Loss=367.0883\n",
      "-----\n",
      "7140\n",
      "Loss=347.1827\n",
      "Val Loss=367.0912\n",
      "-----\n",
      "7150\n",
      "Loss=347.2467\n",
      "Val Loss=367.0943\n",
      "-----\n",
      "7160\n",
      "Loss=347.3183\n",
      "Val Loss=367.0997\n",
      "-----\n",
      "7170\n",
      "Loss=347.3762\n",
      "Val Loss=367.1049\n",
      "-----\n",
      "7180\n",
      "Loss=347.4240\n",
      "Val Loss=367.1082\n",
      "-----\n",
      "7190\n",
      "Loss=347.4880\n",
      "Val Loss=367.1121\n",
      "-----\n",
      "7200\n",
      "Loss=347.5347\n",
      "Val Loss=367.1156\n",
      "-----\n",
      "7210\n",
      "Loss=347.5812\n",
      "Val Loss=367.1188\n",
      "-----\n",
      "7220\n",
      "Loss=347.6028\n",
      "Val Loss=367.1205\n",
      "-----\n",
      "7230\n",
      "Loss=347.6094\n",
      "Val Loss=367.1213\n",
      "-----\n",
      "7240\n",
      "Loss=347.6225\n",
      "Val Loss=367.1232\n",
      "-----\n",
      "7250\n",
      "Loss=347.6334\n",
      "Val Loss=367.1254\n",
      "-----\n",
      "7260\n",
      "Loss=347.6223\n",
      "Val Loss=367.1261\n",
      "-----\n",
      "7270\n",
      "Loss=347.6076\n",
      "Val Loss=367.1268\n",
      "-----\n",
      "7280\n",
      "Loss=347.5720\n",
      "Val Loss=367.1253\n",
      "-----\n",
      "7290\n",
      "Loss=347.5233\n",
      "Val Loss=367.1235\n",
      "-----\n",
      "7300\n",
      "Loss=347.4867\n",
      "Val Loss=367.1221\n",
      "-----\n",
      "7310\n",
      "Loss=347.4393\n",
      "Val Loss=367.1205\n",
      "-----\n",
      "7320\n",
      "Loss=347.3942\n",
      "Val Loss=367.1198\n",
      "-----\n",
      "7330\n",
      "Loss=347.3495\n",
      "Val Loss=367.1169\n",
      "-----\n",
      "7340\n",
      "Loss=347.3076\n",
      "Val Loss=367.1139\n",
      "-----\n",
      "7350\n",
      "Loss=347.2747\n",
      "Val Loss=367.1113\n",
      "-----\n",
      "7360\n",
      "Loss=347.2372\n",
      "Val Loss=367.1081\n",
      "-----\n",
      "7370\n",
      "Loss=347.2099\n",
      "Val Loss=367.1053\n",
      "-----\n",
      "7380\n",
      "Loss=347.1779\n",
      "Val Loss=367.1037\n",
      "-----\n",
      "7390\n",
      "Loss=347.1562\n",
      "Val Loss=367.1018\n",
      "-----\n",
      "7400\n",
      "Loss=347.1306\n",
      "Val Loss=367.0995\n",
      "-----\n",
      "7410\n",
      "Loss=347.1164\n",
      "Val Loss=367.0988\n",
      "-----\n",
      "7420\n",
      "Loss=347.1096\n",
      "Val Loss=367.0981\n",
      "-----\n",
      "7430\n",
      "Loss=347.1269\n",
      "Val Loss=367.0997\n",
      "-----\n",
      "7440\n",
      "Loss=347.1471\n",
      "Val Loss=367.1021\n",
      "-----\n",
      "7450\n",
      "Loss=347.1772\n",
      "Val Loss=367.1048\n",
      "-----\n",
      "7460\n",
      "Loss=347.2056\n",
      "Val Loss=367.1051\n",
      "-----\n",
      "7470\n",
      "Loss=347.2189\n",
      "Val Loss=367.1053\n",
      "-----\n",
      "7480\n",
      "Loss=347.2373\n",
      "Val Loss=367.1077\n",
      "-----\n",
      "7490\n",
      "Loss=347.2533\n",
      "Val Loss=367.1086\n",
      "-----\n",
      "7500\n",
      "Loss=347.2667\n",
      "Val Loss=367.1097\n",
      "-----\n",
      "7510\n",
      "Loss=347.2729\n",
      "Val Loss=367.1107\n",
      "-----\n",
      "7520\n",
      "Loss=347.2411\n",
      "Val Loss=367.1085\n",
      "-----\n",
      "7530\n",
      "Loss=347.2190\n",
      "Val Loss=367.1059\n",
      "-----\n",
      "7540\n",
      "Loss=347.2058\n",
      "Val Loss=367.1066\n",
      "-----\n",
      "7550\n",
      "Loss=347.1999\n",
      "Val Loss=367.1075\n",
      "-----\n",
      "7560\n",
      "Loss=347.2142\n",
      "Val Loss=367.1085\n",
      "-----\n",
      "7570\n",
      "Loss=347.2253\n",
      "Val Loss=367.1114\n",
      "-----\n",
      "7580\n",
      "Loss=347.2432\n",
      "Val Loss=367.1161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0c8da2742ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        self.lin3 = nn.Linear(100, 20)\n",
    "        self.lin4 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = self.lin4(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (lin3): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (lin4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (lin5): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=41027.6523\n",
      "Val Loss=44475.3789\n",
      "-----\n",
      "10\n",
      "Loss=19628.6055\n",
      "Val Loss=14524.2520\n",
      "-----\n",
      "20\n",
      "Loss=5190.3882\n",
      "Val Loss=5981.8555\n",
      "-----\n",
      "30\n",
      "Loss=3884.6340\n",
      "Val Loss=4650.5967\n",
      "-----\n",
      "40\n",
      "Loss=2937.4805\n",
      "Val Loss=3663.1064\n",
      "-----\n",
      "50\n",
      "Loss=2283.9312\n",
      "Val Loss=2945.1121\n",
      "-----\n",
      "60\n",
      "Loss=1808.6974\n",
      "Val Loss=2380.6660\n",
      "-----\n",
      "70\n",
      "Loss=1423.9235\n",
      "Val Loss=1894.7966\n",
      "-----\n",
      "80\n",
      "Loss=1104.2139\n",
      "Val Loss=1475.8422\n",
      "-----\n",
      "90\n",
      "Loss=845.6357\n",
      "Val Loss=1127.8082\n",
      "-----\n",
      "100\n",
      "Loss=668.0111\n",
      "Val Loss=853.1544\n",
      "-----\n",
      "110\n",
      "Loss=2664.2532\n",
      "Val Loss=2561.8818\n",
      "-----\n",
      "120\n",
      "Loss=1752.3610\n",
      "Val Loss=1026.6965\n",
      "-----\n",
      "130\n",
      "Loss=1409.2616\n",
      "Val Loss=909.9985\n",
      "-----\n",
      "140\n",
      "Loss=1422.6881\n",
      "Val Loss=898.7137\n",
      "-----\n",
      "150\n",
      "Loss=1295.6401\n",
      "Val Loss=836.0547\n",
      "-----\n",
      "160\n",
      "Loss=1165.0991\n",
      "Val Loss=776.3022\n",
      "-----\n",
      "170\n",
      "Loss=1068.0223\n",
      "Val Loss=731.0525\n",
      "-----\n",
      "180\n",
      "Loss=994.8151\n",
      "Val Loss=696.2775\n",
      "-----\n",
      "190\n",
      "Loss=935.0551\n",
      "Val Loss=666.6011\n",
      "-----\n",
      "200\n",
      "Loss=881.2934\n",
      "Val Loss=639.4598\n",
      "-----\n",
      "210\n",
      "Loss=833.2753\n",
      "Val Loss=614.9340\n",
      "-----\n",
      "220\n",
      "Loss=789.4785\n",
      "Val Loss=591.8828\n",
      "-----\n",
      "230\n",
      "Loss=750.1949\n",
      "Val Loss=571.3456\n",
      "-----\n",
      "240\n",
      "Loss=716.8950\n",
      "Val Loss=553.7895\n",
      "-----\n",
      "250\n",
      "Loss=687.9847\n",
      "Val Loss=538.2284\n",
      "-----\n",
      "260\n",
      "Loss=661.8625\n",
      "Val Loss=524.0061\n",
      "-----\n",
      "270\n",
      "Loss=638.5239\n",
      "Val Loss=511.3352\n",
      "-----\n",
      "280\n",
      "Loss=618.5578\n",
      "Val Loss=500.3744\n",
      "-----\n",
      "290\n",
      "Loss=601.6212\n",
      "Val Loss=490.9507\n",
      "-----\n",
      "300\n",
      "Loss=586.3743\n",
      "Val Loss=482.3935\n",
      "-----\n",
      "310\n",
      "Loss=572.1767\n",
      "Val Loss=474.3342\n",
      "-----\n",
      "320\n",
      "Loss=558.7354\n",
      "Val Loss=466.9123\n",
      "-----\n",
      "330\n",
      "Loss=546.8245\n",
      "Val Loss=460.3641\n",
      "-----\n",
      "340\n",
      "Loss=536.3555\n",
      "Val Loss=454.5526\n",
      "-----\n",
      "350\n",
      "Loss=527.0900\n",
      "Val Loss=449.3639\n",
      "-----\n",
      "360\n",
      "Loss=518.2013\n",
      "Val Loss=444.2856\n",
      "-----\n",
      "370\n",
      "Loss=509.0738\n",
      "Val Loss=439.0767\n",
      "-----\n",
      "380\n",
      "Loss=500.2628\n",
      "Val Loss=434.2029\n",
      "-----\n",
      "390\n",
      "Loss=492.2884\n",
      "Val Loss=429.8888\n",
      "-----\n",
      "400\n",
      "Loss=485.2322\n",
      "Val Loss=426.1072\n",
      "-----\n",
      "410\n",
      "Loss=479.3778\n",
      "Val Loss=423.0375\n",
      "-----\n",
      "420\n",
      "Loss=474.7814\n",
      "Val Loss=420.6042\n",
      "-----\n",
      "430\n",
      "Loss=470.3506\n",
      "Val Loss=418.1507\n",
      "-----\n",
      "440\n",
      "Loss=465.5855\n",
      "Val Loss=415.5858\n",
      "-----\n",
      "450\n",
      "Loss=461.0707\n",
      "Val Loss=413.2323\n",
      "-----\n",
      "460\n",
      "Loss=457.2335\n",
      "Val Loss=411.1859\n",
      "-----\n",
      "470\n",
      "Loss=453.9427\n",
      "Val Loss=409.4848\n",
      "-----\n",
      "480\n",
      "Loss=450.7362\n",
      "Val Loss=407.7756\n",
      "-----\n",
      "490\n",
      "Loss=447.1195\n",
      "Val Loss=405.8636\n",
      "-----\n",
      "500\n",
      "Loss=443.3921\n",
      "Val Loss=403.9519\n",
      "-----\n",
      "510\n",
      "Loss=439.8271\n",
      "Val Loss=402.0949\n",
      "-----\n",
      "520\n",
      "Loss=436.3694\n",
      "Val Loss=400.2950\n",
      "-----\n",
      "530\n",
      "Loss=433.0855\n",
      "Val Loss=398.6417\n",
      "-----\n",
      "540\n",
      "Loss=430.2396\n",
      "Val Loss=397.2307\n",
      "-----\n",
      "550\n",
      "Loss=427.8740\n",
      "Val Loss=396.0756\n",
      "-----\n",
      "560\n",
      "Loss=425.6602\n",
      "Val Loss=394.9814\n",
      "-----\n",
      "570\n",
      "Loss=423.6073\n",
      "Val Loss=393.9629\n",
      "-----\n",
      "580\n",
      "Loss=421.5263\n",
      "Val Loss=392.9225\n",
      "-----\n",
      "590\n",
      "Loss=419.3449\n",
      "Val Loss=391.8362\n",
      "-----\n",
      "600\n",
      "Loss=417.1271\n",
      "Val Loss=390.7320\n",
      "-----\n",
      "610\n",
      "Loss=414.8470\n",
      "Val Loss=389.6778\n",
      "-----\n",
      "620\n",
      "Loss=412.9722\n",
      "Val Loss=388.8605\n",
      "-----\n",
      "630\n",
      "Loss=411.6259\n",
      "Val Loss=388.2813\n",
      "-----\n",
      "640\n",
      "Loss=410.6024\n",
      "Val Loss=387.8144\n",
      "-----\n",
      "650\n",
      "Loss=409.5107\n",
      "Val Loss=387.2898\n",
      "-----\n",
      "660\n",
      "Loss=408.4341\n",
      "Val Loss=386.7682\n",
      "-----\n",
      "670\n",
      "Loss=407.1808\n",
      "Val Loss=386.1346\n",
      "-----\n",
      "680\n",
      "Loss=405.5112\n",
      "Val Loss=385.3505\n",
      "-----\n",
      "690\n",
      "Loss=403.7843\n",
      "Val Loss=384.5679\n",
      "-----\n",
      "700\n",
      "Loss=401.8995\n",
      "Val Loss=383.7270\n",
      "-----\n",
      "710\n",
      "Loss=400.0931\n",
      "Val Loss=382.9499\n",
      "-----\n",
      "720\n",
      "Loss=398.5410\n",
      "Val Loss=382.2754\n",
      "-----\n",
      "730\n",
      "Loss=397.1604\n",
      "Val Loss=381.6872\n",
      "-----\n",
      "740\n",
      "Loss=396.0072\n",
      "Val Loss=381.2086\n",
      "-----\n",
      "750\n",
      "Loss=394.9499\n",
      "Val Loss=380.7758\n",
      "-----\n",
      "760\n",
      "Loss=393.8837\n",
      "Val Loss=380.3298\n",
      "-----\n",
      "770\n",
      "Loss=392.9988\n",
      "Val Loss=379.9931\n",
      "-----\n",
      "780\n",
      "Loss=392.3021\n",
      "Val Loss=379.7072\n",
      "-----\n",
      "790\n",
      "Loss=391.6337\n",
      "Val Loss=379.4331\n",
      "-----\n",
      "800\n",
      "Loss=391.0512\n",
      "Val Loss=379.2065\n",
      "-----\n",
      "810\n",
      "Loss=390.6094\n",
      "Val Loss=379.0331\n",
      "-----\n",
      "820\n",
      "Loss=390.1341\n",
      "Val Loss=378.8271\n",
      "-----\n",
      "830\n",
      "Loss=389.7141\n",
      "Val Loss=378.6496\n",
      "-----\n",
      "840\n",
      "Loss=389.3597\n",
      "Val Loss=378.5121\n",
      "-----\n",
      "850\n",
      "Loss=389.0148\n",
      "Val Loss=378.3809\n",
      "-----\n",
      "860\n",
      "Loss=388.7044\n",
      "Val Loss=378.2524\n",
      "-----\n",
      "870\n",
      "Loss=388.1448\n",
      "Val Loss=378.0399\n",
      "-----\n",
      "880\n",
      "Loss=387.5909\n",
      "Val Loss=377.8372\n",
      "-----\n",
      "890\n",
      "Loss=386.9473\n",
      "Val Loss=377.5761\n",
      "-----\n",
      "900\n",
      "Loss=386.0861\n",
      "Val Loss=377.2311\n",
      "-----\n",
      "910\n",
      "Loss=385.1528\n",
      "Val Loss=376.8804\n",
      "-----\n",
      "920\n",
      "Loss=384.3118\n",
      "Val Loss=376.5571\n",
      "-----\n",
      "930\n",
      "Loss=383.4470\n",
      "Val Loss=376.2294\n",
      "-----\n",
      "940\n",
      "Loss=382.4967\n",
      "Val Loss=375.8785\n",
      "-----\n",
      "950\n",
      "Loss=381.5636\n",
      "Val Loss=375.5423\n",
      "-----\n",
      "960\n",
      "Loss=380.6923\n",
      "Val Loss=375.2416\n",
      "-----\n",
      "970\n",
      "Loss=380.0219\n",
      "Val Loss=375.0190\n",
      "-----\n",
      "980\n",
      "Loss=379.5339\n",
      "Val Loss=374.8584\n",
      "-----\n",
      "990\n",
      "Loss=379.1027\n",
      "Val Loss=374.7234\n",
      "-----\n",
      "1000\n",
      "Loss=378.7722\n",
      "Val Loss=374.6161\n",
      "-----\n",
      "1010\n",
      "Loss=378.4194\n",
      "Val Loss=374.5001\n",
      "-----\n",
      "1020\n",
      "Loss=377.9910\n",
      "Val Loss=374.3719\n",
      "-----\n",
      "1030\n",
      "Loss=377.5899\n",
      "Val Loss=374.2489\n",
      "-----\n",
      "1040\n",
      "Loss=377.2805\n",
      "Val Loss=374.1543\n",
      "-----\n",
      "1050\n",
      "Loss=377.0218\n",
      "Val Loss=374.0720\n",
      "-----\n",
      "1060\n",
      "Loss=376.6953\n",
      "Val Loss=373.9669\n",
      "-----\n",
      "1070\n",
      "Loss=376.2697\n",
      "Val Loss=373.8282\n",
      "-----\n",
      "1080\n",
      "Loss=375.7050\n",
      "Val Loss=373.6445\n",
      "-----\n",
      "1090\n",
      "Loss=375.0555\n",
      "Val Loss=373.4389\n",
      "-----\n",
      "1100\n",
      "Loss=374.4380\n",
      "Val Loss=373.2507\n",
      "-----\n",
      "1110\n",
      "Loss=373.8974\n",
      "Val Loss=373.1005\n",
      "-----\n",
      "1120\n",
      "Loss=373.5026\n",
      "Val Loss=372.9972\n",
      "-----\n",
      "1130\n",
      "Loss=373.3052\n",
      "Val Loss=372.9559\n",
      "-----\n",
      "1140\n",
      "Loss=373.2034\n",
      "Val Loss=372.9389\n",
      "-----\n",
      "1150\n",
      "Loss=373.1723\n",
      "Val Loss=372.9482\n",
      "-----\n",
      "1160\n",
      "Loss=373.2139\n",
      "Val Loss=372.9756\n",
      "-----\n",
      "1170\n",
      "Loss=373.3182\n",
      "Val Loss=373.0246\n",
      "-----\n",
      "1180\n",
      "Loss=373.3821\n",
      "Val Loss=373.0540\n",
      "-----\n",
      "1190\n",
      "Loss=373.3787\n",
      "Val Loss=373.0588\n",
      "-----\n",
      "1200\n",
      "Loss=373.2795\n",
      "Val Loss=373.0331\n",
      "-----\n",
      "1210\n",
      "Loss=373.0934\n",
      "Val Loss=372.9766\n",
      "-----\n",
      "1220\n",
      "Loss=372.8297\n",
      "Val Loss=372.8972\n",
      "-----\n",
      "1230\n",
      "Loss=372.4591\n",
      "Val Loss=372.7862\n",
      "-----\n",
      "1240\n",
      "Loss=372.0675\n",
      "Val Loss=372.6772\n",
      "-----\n",
      "1250\n",
      "Loss=371.6400\n",
      "Val Loss=372.5562\n",
      "-----\n",
      "1260\n",
      "Loss=371.1778\n",
      "Val Loss=372.4292\n",
      "-----\n",
      "1270\n",
      "Loss=370.6488\n",
      "Val Loss=372.2791\n",
      "-----\n",
      "1280\n",
      "Loss=370.1014\n",
      "Val Loss=372.1280\n",
      "-----\n",
      "1290\n",
      "Loss=369.4736\n",
      "Val Loss=371.9530\n",
      "-----\n",
      "1300\n",
      "Loss=368.8829\n",
      "Val Loss=371.8000\n",
      "-----\n",
      "1310\n",
      "Loss=368.3287\n",
      "Val Loss=371.6645\n",
      "-----\n",
      "1320\n",
      "Loss=367.8703\n",
      "Val Loss=371.5573\n",
      "-----\n",
      "1330\n",
      "Loss=367.5144\n",
      "Val Loss=371.4761\n",
      "-----\n",
      "1340\n",
      "Loss=367.2080\n",
      "Val Loss=371.4060\n",
      "-----\n",
      "1350\n",
      "Loss=366.9601\n",
      "Val Loss=371.3581\n",
      "-----\n",
      "1360\n",
      "Loss=366.8322\n",
      "Val Loss=371.3354\n",
      "-----\n",
      "1370\n",
      "Loss=366.7543\n",
      "Val Loss=371.3168\n",
      "-----\n",
      "1380\n",
      "Loss=366.6423\n",
      "Val Loss=371.2927\n",
      "-----\n",
      "1390\n",
      "Loss=366.5254\n",
      "Val Loss=371.2636\n",
      "-----\n",
      "1400\n",
      "Loss=366.3693\n",
      "Val Loss=371.2285\n",
      "-----\n",
      "1410\n",
      "Loss=366.1578\n",
      "Val Loss=371.1813\n",
      "-----\n",
      "1420\n",
      "Loss=365.8910\n",
      "Val Loss=371.1200\n",
      "-----\n",
      "1430\n",
      "Loss=365.5678\n",
      "Val Loss=371.0498\n",
      "-----\n",
      "1440\n",
      "Loss=365.2571\n",
      "Val Loss=370.9867\n",
      "-----\n",
      "1450\n",
      "Loss=364.9903\n",
      "Val Loss=370.9331\n",
      "-----\n",
      "1460\n",
      "Loss=364.7146\n",
      "Val Loss=370.8779\n",
      "-----\n",
      "1470\n",
      "Loss=364.4487\n",
      "Val Loss=370.8279\n",
      "-----\n",
      "1480\n",
      "Loss=364.2323\n",
      "Val Loss=370.7925\n",
      "-----\n",
      "1490\n",
      "Loss=364.0885\n",
      "Val Loss=370.7658\n",
      "-----\n",
      "1500\n",
      "Loss=363.8935\n",
      "Val Loss=370.7243\n",
      "-----\n",
      "1510\n",
      "Loss=363.5966\n",
      "Val Loss=370.6669\n",
      "-----\n",
      "1520\n",
      "Loss=363.3068\n",
      "Val Loss=370.6105\n",
      "-----\n",
      "1530\n",
      "Loss=363.0395\n",
      "Val Loss=370.5592\n",
      "-----\n",
      "1540\n",
      "Loss=362.7838\n",
      "Val Loss=370.5112\n",
      "-----\n",
      "1550\n",
      "Loss=362.5477\n",
      "Val Loss=370.4659\n",
      "-----\n",
      "1560\n",
      "Loss=362.3467\n",
      "Val Loss=370.4283\n",
      "-----\n",
      "1570\n",
      "Loss=362.1102\n",
      "Val Loss=370.3868\n",
      "-----\n",
      "1580\n",
      "Loss=361.8510\n",
      "Val Loss=370.3448\n",
      "-----\n",
      "1590\n",
      "Loss=361.5745\n",
      "Val Loss=370.3002\n",
      "-----\n",
      "1600\n",
      "Loss=361.2672\n",
      "Val Loss=370.2517\n",
      "-----\n",
      "1610\n",
      "Loss=360.9253\n",
      "Val Loss=370.1985\n",
      "-----\n",
      "1620\n",
      "Loss=360.5858\n",
      "Val Loss=370.1493\n",
      "-----\n",
      "1630\n",
      "Loss=360.2661\n",
      "Val Loss=370.1048\n",
      "-----\n",
      "1640\n",
      "Loss=359.9453\n",
      "Val Loss=370.0619\n",
      "-----\n",
      "1650\n",
      "Loss=359.6086\n",
      "Val Loss=370.0179\n",
      "-----\n",
      "1660\n",
      "Loss=359.2537\n",
      "Val Loss=369.9745\n",
      "-----\n",
      "1670\n",
      "Loss=358.9088\n",
      "Val Loss=369.9346\n",
      "-----\n",
      "1680\n",
      "Loss=358.5381\n",
      "Val Loss=369.8937\n",
      "-----\n",
      "1690\n",
      "Loss=358.1247\n",
      "Val Loss=369.8526\n",
      "-----\n",
      "1700\n",
      "Loss=357.7084\n",
      "Val Loss=369.8173\n",
      "-----\n",
      "1710\n",
      "Loss=357.3122\n",
      "Val Loss=369.7886\n",
      "-----\n",
      "1720\n",
      "Loss=356.9247\n",
      "Val Loss=369.7648\n",
      "-----\n",
      "1730\n",
      "Loss=356.5823\n",
      "Val Loss=369.7484\n",
      "-----\n",
      "1740\n",
      "Loss=356.2874\n",
      "Val Loss=369.7357\n",
      "-----\n",
      "1750\n",
      "Loss=356.0069\n",
      "Val Loss=369.7288\n",
      "-----\n",
      "1760\n",
      "Loss=355.7816\n",
      "Val Loss=369.7259\n",
      "-----\n",
      "1770\n",
      "Loss=355.5822\n",
      "Val Loss=369.7233\n",
      "-----\n",
      "1780\n",
      "Loss=355.4173\n",
      "Val Loss=369.7234\n",
      "-----\n",
      "1790\n",
      "Loss=355.3406\n",
      "Val Loss=369.7240\n",
      "-----\n",
      "1800\n",
      "Loss=355.2633\n",
      "Val Loss=369.7255\n",
      "-----\n",
      "1810\n",
      "Loss=355.2086\n",
      "Val Loss=369.7289\n",
      "-----\n",
      "1820\n",
      "Loss=355.1728\n",
      "Val Loss=369.7310\n",
      "-----\n",
      "1830\n",
      "Loss=355.1822\n",
      "Val Loss=369.7332\n",
      "-----\n",
      "1840\n",
      "Loss=355.1989\n",
      "Val Loss=369.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "1850\n",
      "Loss=355.2239\n",
      "Val Loss=369.7361\n",
      "-----\n",
      "1860\n",
      "Loss=355.2445\n",
      "Val Loss=369.7366\n",
      "-----\n",
      "1870\n",
      "Loss=355.2372\n",
      "Val Loss=369.7373\n",
      "-----\n",
      "1880\n",
      "Loss=355.2397\n",
      "Val Loss=369.7376\n",
      "-----\n",
      "1890\n",
      "Loss=355.2574\n",
      "Val Loss=369.7370\n",
      "-----\n",
      "1900\n",
      "Loss=355.2547\n",
      "Val Loss=369.7366\n",
      "-----\n",
      "1910\n",
      "Loss=355.2372\n",
      "Val Loss=369.7352\n",
      "-----\n",
      "1920\n",
      "Loss=355.2008\n",
      "Val Loss=369.7334\n",
      "-----\n",
      "1930\n",
      "Loss=355.1436\n",
      "Val Loss=369.7316\n",
      "-----\n",
      "1940\n",
      "Loss=355.0762\n",
      "Val Loss=369.7285\n",
      "-----\n",
      "1950\n",
      "Loss=354.9994\n",
      "Val Loss=369.7243\n",
      "-----\n",
      "1960\n",
      "Loss=354.9267\n",
      "Val Loss=369.7209\n",
      "-----\n",
      "1970\n",
      "Loss=354.8502\n",
      "Val Loss=369.7162\n",
      "-----\n",
      "1980\n",
      "Loss=354.7498\n",
      "Val Loss=369.7127\n",
      "-----\n",
      "1990\n",
      "Loss=354.6483\n",
      "Val Loss=369.7100\n",
      "-----\n",
      "2000\n",
      "Loss=354.5378\n",
      "Val Loss=369.7078\n",
      "-----\n",
      "2010\n",
      "Loss=354.4214\n",
      "Val Loss=369.7071\n",
      "-----\n",
      "2020\n",
      "Loss=354.2861\n",
      "Val Loss=369.7054\n",
      "-----\n",
      "2030\n",
      "Loss=354.1297\n",
      "Val Loss=369.7048\n",
      "-----\n",
      "2040\n",
      "Loss=353.9742\n",
      "Val Loss=369.7064\n",
      "-----\n",
      "2050\n",
      "Loss=353.8115\n",
      "Val Loss=369.7094\n",
      "-----\n",
      "2060\n",
      "Loss=353.6440\n",
      "Val Loss=369.7136\n",
      "-----\n",
      "2070\n",
      "Loss=353.4841\n",
      "Val Loss=369.7190\n",
      "-----\n",
      "2080\n",
      "Loss=353.3255\n",
      "Val Loss=369.7248\n",
      "-----\n",
      "2090\n",
      "Loss=353.1472\n",
      "Val Loss=369.7316\n",
      "-----\n",
      "2100\n",
      "Loss=352.9515\n",
      "Val Loss=369.7414\n",
      "-----\n",
      "2110\n",
      "Loss=352.7497\n",
      "Val Loss=369.7513\n",
      "-----\n",
      "2120\n",
      "Loss=352.5502\n",
      "Val Loss=369.7636\n",
      "-----\n",
      "2130\n",
      "Loss=352.3574\n",
      "Val Loss=369.7767\n",
      "-----\n",
      "2140\n",
      "Loss=352.1668\n",
      "Val Loss=369.7909\n",
      "-----\n",
      "2150\n",
      "Loss=351.9791\n",
      "Val Loss=369.8053\n",
      "-----\n",
      "2160\n",
      "Loss=351.8053\n",
      "Val Loss=369.8212\n",
      "-----\n",
      "2170\n",
      "Loss=351.6364\n",
      "Val Loss=369.8368\n",
      "-----\n",
      "2180\n",
      "Loss=351.4880\n",
      "Val Loss=369.8510\n",
      "-----\n",
      "2190\n",
      "Loss=351.3626\n",
      "Val Loss=369.8637\n",
      "-----\n",
      "2200\n",
      "Loss=351.2527\n",
      "Val Loss=369.8757\n",
      "-----\n",
      "2210\n",
      "Loss=351.1515\n",
      "Val Loss=369.8864\n",
      "-----\n",
      "2220\n",
      "Loss=351.0623\n",
      "Val Loss=369.8948\n",
      "-----\n",
      "2230\n",
      "Loss=350.9798\n",
      "Val Loss=369.9031\n",
      "-----\n",
      "2240\n",
      "Loss=350.8924\n",
      "Val Loss=369.9121\n",
      "-----\n",
      "2250\n",
      "Loss=350.8288\n",
      "Val Loss=369.9178\n",
      "-----\n",
      "2260\n",
      "Loss=350.7675\n",
      "Val Loss=369.9233\n",
      "-----\n",
      "2270\n",
      "Loss=350.6992\n",
      "Val Loss=369.9289\n",
      "-----\n",
      "2280\n",
      "Loss=350.6548\n",
      "Val Loss=369.9310\n",
      "-----\n",
      "2290\n",
      "Loss=350.6333\n",
      "Val Loss=369.9298\n",
      "-----\n",
      "2300\n",
      "Loss=350.6281\n",
      "Val Loss=369.9260\n",
      "-----\n",
      "2310\n",
      "Loss=350.6544\n",
      "Val Loss=369.9178\n",
      "-----\n",
      "2320\n",
      "Loss=350.7238\n",
      "Val Loss=369.9016\n",
      "-----\n",
      "2330\n",
      "Loss=350.8074\n",
      "Val Loss=369.8849\n",
      "-----\n",
      "2340\n",
      "Loss=350.9171\n",
      "Val Loss=369.8635\n",
      "-----\n",
      "2350\n",
      "Loss=351.0452\n",
      "Val Loss=369.8406\n",
      "-----\n",
      "2360\n",
      "Loss=351.1676\n",
      "Val Loss=369.8215\n",
      "-----\n",
      "2370\n",
      "Loss=351.2864\n",
      "Val Loss=369.8040\n",
      "-----\n",
      "2380\n",
      "Loss=351.4134\n",
      "Val Loss=369.7852\n",
      "-----\n",
      "2390\n",
      "Loss=351.5594\n",
      "Val Loss=369.7655\n",
      "-----\n",
      "2400\n",
      "Loss=351.7133\n",
      "Val Loss=369.7462\n",
      "-----\n",
      "2410\n",
      "Loss=351.8643\n",
      "Val Loss=369.7282\n",
      "-----\n",
      "2420\n",
      "Loss=351.9819\n",
      "Val Loss=369.7143\n",
      "-----\n",
      "2430\n",
      "Loss=352.1021\n",
      "Val Loss=369.7021\n",
      "-----\n",
      "2440\n",
      "Loss=352.2165\n",
      "Val Loss=369.6919\n",
      "-----\n",
      "2450\n",
      "Loss=352.3587\n",
      "Val Loss=369.6822\n",
      "-----\n",
      "2460\n",
      "Loss=352.4847\n",
      "Val Loss=369.6761\n",
      "-----\n",
      "2470\n",
      "Loss=352.5846\n",
      "Val Loss=369.6719\n",
      "-----\n",
      "2480\n",
      "Loss=352.6701\n",
      "Val Loss=369.6684\n",
      "-----\n",
      "2490\n",
      "Loss=352.7439\n",
      "Val Loss=369.6644\n",
      "-----\n",
      "2500\n",
      "Loss=352.8189\n",
      "Val Loss=369.6613\n",
      "-----\n",
      "2510\n",
      "Loss=352.8761\n",
      "Val Loss=369.6591\n",
      "-----\n",
      "2520\n",
      "Loss=352.9119\n",
      "Val Loss=369.6564\n",
      "-----\n",
      "2530\n",
      "Loss=352.9326\n",
      "Val Loss=369.6547\n",
      "-----\n",
      "2540\n",
      "Loss=352.9137\n",
      "Val Loss=369.6527\n",
      "-----\n",
      "2550\n",
      "Loss=352.8739\n",
      "Val Loss=369.6520\n",
      "-----\n",
      "2560\n",
      "Loss=352.8158\n",
      "Val Loss=369.6516\n",
      "-----\n",
      "2570\n",
      "Loss=352.7451\n",
      "Val Loss=369.6501\n",
      "-----\n",
      "2580\n",
      "Loss=352.6299\n",
      "Val Loss=369.6515\n",
      "-----\n",
      "2590\n",
      "Loss=352.5033\n",
      "Val Loss=369.6538\n",
      "-----\n",
      "2600\n",
      "Loss=352.3817\n",
      "Val Loss=369.6569\n",
      "-----\n",
      "2610\n",
      "Loss=352.2441\n",
      "Val Loss=369.6610\n",
      "-----\n",
      "2620\n",
      "Loss=352.0907\n",
      "Val Loss=369.6659\n",
      "-----\n",
      "2630\n",
      "Loss=351.9406\n",
      "Val Loss=369.6718\n",
      "-----\n",
      "2640\n",
      "Loss=351.7990\n",
      "Val Loss=369.6767\n",
      "-----\n",
      "2650\n",
      "Loss=351.6593\n",
      "Val Loss=369.6822\n",
      "-----\n",
      "2660\n",
      "Loss=351.5287\n",
      "Val Loss=369.6873\n",
      "-----\n",
      "2670\n",
      "Loss=351.4119\n",
      "Val Loss=369.6918\n",
      "-----\n",
      "2680\n",
      "Loss=351.3034\n",
      "Val Loss=369.6969\n",
      "-----\n",
      "2690\n",
      "Loss=351.2129\n",
      "Val Loss=369.6989\n",
      "-----\n",
      "2700\n",
      "Loss=351.1273\n",
      "Val Loss=369.7017\n",
      "-----\n",
      "2710\n",
      "Loss=351.0497\n",
      "Val Loss=369.7038\n",
      "-----\n",
      "2720\n",
      "Loss=350.9607\n",
      "Val Loss=369.7086\n",
      "-----\n",
      "2730\n",
      "Loss=350.8698\n",
      "Val Loss=369.7137\n",
      "-----\n",
      "2740\n",
      "Loss=350.7929\n",
      "Val Loss=369.7169\n",
      "-----\n",
      "2750\n",
      "Loss=350.7229\n",
      "Val Loss=369.7187\n",
      "-----\n",
      "2760\n",
      "Loss=350.6570\n",
      "Val Loss=369.7209\n",
      "-----\n",
      "2770\n",
      "Loss=350.5869\n",
      "Val Loss=369.7234\n",
      "-----\n",
      "2780\n",
      "Loss=350.5010\n",
      "Val Loss=369.7283\n",
      "-----\n",
      "2790\n",
      "Loss=350.4088\n",
      "Val Loss=369.7345\n",
      "-----\n",
      "2800\n",
      "Loss=350.3210\n",
      "Val Loss=369.7399\n",
      "-----\n",
      "2810\n",
      "Loss=350.2354\n",
      "Val Loss=369.7452\n",
      "-----\n",
      "2820\n",
      "Loss=350.1510\n",
      "Val Loss=369.7514\n",
      "-----\n",
      "2830\n",
      "Loss=350.0747\n",
      "Val Loss=369.7566\n",
      "-----\n",
      "2840\n",
      "Loss=350.0057\n",
      "Val Loss=369.7607\n",
      "-----\n",
      "2850\n",
      "Loss=349.9388\n",
      "Val Loss=369.7655\n",
      "-----\n",
      "2860\n",
      "Loss=349.8759\n",
      "Val Loss=369.7703\n",
      "-----\n",
      "2870\n",
      "Loss=349.8054\n",
      "Val Loss=369.7777\n",
      "-----\n",
      "2880\n",
      "Loss=349.7402\n",
      "Val Loss=369.7840\n",
      "-----\n",
      "2890\n",
      "Loss=349.6737\n",
      "Val Loss=369.7906\n",
      "-----\n",
      "2900\n",
      "Loss=349.6001\n",
      "Val Loss=369.7989\n",
      "-----\n",
      "2910\n",
      "Loss=349.5209\n",
      "Val Loss=369.8076\n",
      "-----\n",
      "2920\n",
      "Loss=349.4385\n",
      "Val Loss=369.8174\n",
      "-----\n",
      "2930\n",
      "Loss=349.3583\n",
      "Val Loss=369.8262\n",
      "-----\n",
      "2940\n",
      "Loss=349.2882\n",
      "Val Loss=369.8338\n",
      "-----\n",
      "2950\n",
      "Loss=349.2265\n",
      "Val Loss=369.8395\n",
      "-----\n",
      "2960\n",
      "Loss=349.1740\n",
      "Val Loss=369.8442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4f3b4f220523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# input x and predict based on x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# must be (1. nn output, 2. target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-4f3b4f220523>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# perform dropout on input vector embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 200)\n",
    "        self.lin3 = nn.Linear(200, 50)\n",
    "        self.lin4 = nn.Linear(50, 10)\n",
    "        self.lin5 = nn.Linear(10, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = self.lin5(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try temporal convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
