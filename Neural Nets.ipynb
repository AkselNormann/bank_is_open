{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = os.listdir(game_data_path)\n",
    "with open(game_data_path + files[0], 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 3, 504)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValSplit(X, y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "\n",
    "    p = np.random.permutation(len(X))\n",
    "    X = X[p]\n",
    "    y = y[p]\n",
    "\n",
    "    val = 0.2\n",
    "    val = round(len(X) * val)\n",
    "    X_val = X[:val]\n",
    "    y_val = y[:val]\n",
    "    X = X[val:]\n",
    "    y = y[val:]\n",
    "    \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = trainValSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[213.],\n",
      "        [210.],\n",
      "        [202.],\n",
      "        [191.],\n",
      "        [247.],\n",
      "        [207.],\n",
      "        [167.],\n",
      "        [229.],\n",
      "        [179.]])\n"
     ]
    }
   ],
   "source": [
    "# Split train/test:\n",
    "print(y[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1512, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (drop1): Dropout(p=0.5)\n",
      "  (drop2): Dropout(p=0.4)\n",
      "  (drop3): Dropout(p=0.25)\n",
      ")\n",
      "Loss=40131.9453\n",
      "Loss=536.7881\n",
      "Loss=495.8045\n",
      "Loss=489.6784\n",
      "Loss=483.7281\n",
      "Loss=477.7469\n",
      "Loss=471.0899\n",
      "Loss=465.7350\n",
      "Loss=460.8159\n",
      "Loss=456.0045\n",
      "Loss=451.1470\n",
      "Loss=446.2653\n",
      "Loss=441.5219\n",
      "Loss=436.9084\n",
      "Loss=432.3907\n",
      "Loss=427.9353\n",
      "Loss=423.5046\n",
      "Loss=419.0998\n",
      "Loss=414.7785\n",
      "Loss=410.5339\n",
      "Loss=406.3308\n",
      "Loss=402.1418\n",
      "Loss=397.9559\n",
      "Loss=393.7902\n",
      "Loss=389.6584\n",
      "Loss=385.5572\n",
      "Loss=381.4409\n",
      "Loss=377.2537\n",
      "Loss=372.9801\n",
      "Loss=368.7366\n",
      "Loss=364.6370\n",
      "Loss=360.6323\n",
      "Loss=356.6897\n",
      "Loss=352.7888\n",
      "Loss=348.9171\n",
      "Loss=345.0619\n",
      "Loss=341.2221\n",
      "Loss=337.3980\n",
      "Loss=333.5872\n",
      "Loss=329.7929\n",
      "Loss=326.0143\n",
      "Loss=322.2527\n",
      "Loss=318.5057\n",
      "Loss=314.7734\n",
      "Loss=311.0550\n",
      "Loss=307.3494\n",
      "Loss=303.6575\n",
      "Loss=299.9797\n",
      "Loss=296.3138\n",
      "Loss=292.6609\n",
      "Loss=289.0222\n",
      "Loss=285.3956\n",
      "Loss=281.7806\n",
      "Loss=278.1735\n",
      "Loss=274.5774\n",
      "Loss=270.9955\n",
      "Loss=267.4259\n",
      "Loss=263.8651\n",
      "Loss=260.3188\n",
      "Loss=256.7870\n",
      "Loss=253.2658\n",
      "Loss=249.7525\n",
      "Loss=246.2515\n",
      "Loss=242.7554\n",
      "Loss=239.2608\n",
      "Loss=235.7669\n",
      "Loss=232.2685\n",
      "Loss=228.7877\n",
      "Loss=225.3294\n",
      "Loss=221.8919\n",
      "Loss=218.4598\n",
      "Loss=215.0093\n",
      "Loss=211.5150\n",
      "Loss=208.0210\n",
      "Loss=204.5833\n",
      "Loss=201.2032\n",
      "Loss=197.8880\n",
      "Loss=194.6189\n",
      "Loss=191.3795\n",
      "Loss=188.1609\n",
      "Loss=184.9391\n",
      "Loss=181.7181\n",
      "Loss=178.5519\n",
      "Loss=175.4610\n",
      "Loss=172.4170\n",
      "Loss=169.4115\n",
      "Loss=166.4381\n",
      "Loss=163.4958\n",
      "Loss=160.5820\n",
      "Loss=157.6997\n",
      "Loss=154.8529\n",
      "Loss=152.0347\n",
      "Loss=149.2340\n",
      "Loss=146.4605\n",
      "Loss=143.7239\n",
      "Loss=141.0173\n",
      "Loss=138.3464\n",
      "Loss=135.7113\n",
      "Loss=133.1127\n",
      "Loss=130.5460\n",
      "Loss=128.0184\n",
      "Loss=125.5294\n",
      "Loss=123.0796\n",
      "Loss=120.6692\n",
      "Loss=118.2970\n",
      "Loss=115.9630\n",
      "Loss=113.6671\n",
      "Loss=111.4061\n",
      "Loss=109.1789\n",
      "Loss=106.9849\n",
      "Loss=104.8253\n",
      "Loss=102.6942\n",
      "Loss=100.5910\n",
      "Loss=98.5254\n",
      "Loss=96.5081\n",
      "Loss=94.5434\n",
      "Loss=92.6304\n",
      "Loss=90.7696\n",
      "Loss=88.9586\n",
      "Loss=87.1917\n",
      "Loss=85.4653\n",
      "Loss=83.7770\n",
      "Loss=82.1254\n",
      "Loss=80.5084\n",
      "Loss=78.9245\n",
      "Loss=77.3719\n",
      "Loss=75.8527\n",
      "Loss=74.3701\n",
      "Loss=72.9226\n",
      "Loss=71.5055\n",
      "Loss=70.1230\n",
      "Loss=68.7745\n",
      "Loss=67.4588\n",
      "Loss=66.1757\n",
      "Loss=64.9249\n",
      "Loss=63.7093\n",
      "Loss=62.5246\n",
      "Loss=61.3685\n",
      "Loss=60.2437\n",
      "Loss=59.1509\n",
      "Loss=58.0896\n",
      "Loss=57.0591\n",
      "Loss=56.0542\n",
      "Loss=55.0758\n",
      "Loss=54.1250\n",
      "Loss=53.1996\n",
      "Loss=52.2996\n",
      "Loss=51.4228\n",
      "Loss=50.5684\n",
      "Loss=49.7315\n",
      "Loss=48.9149\n",
      "Loss=48.1179\n",
      "Loss=47.3411\n",
      "Loss=46.5778\n",
      "Loss=45.8280\n",
      "Loss=45.0976\n",
      "Loss=44.3866\n",
      "Loss=43.6940\n",
      "Loss=43.0200\n",
      "Loss=42.3641\n",
      "Loss=41.7262\n",
      "Loss=41.1054\n",
      "Loss=40.4997\n",
      "Loss=39.9084\n",
      "Loss=39.3301\n",
      "Loss=38.7669\n",
      "Loss=38.2181\n",
      "Loss=37.6830\n",
      "Loss=37.1585\n",
      "Loss=36.6459\n",
      "Loss=36.1461\n",
      "Loss=35.6585\n",
      "Loss=35.1807\n",
      "Loss=34.7139\n",
      "Loss=34.2587\n",
      "Loss=33.8148\n",
      "Loss=33.3816\n",
      "Loss=32.9591\n",
      "Loss=32.5468\n",
      "Loss=32.1441\n",
      "Loss=31.7503\n",
      "Loss=31.3656\n",
      "Loss=30.9895\n",
      "Loss=30.6214\n",
      "Loss=30.2611\n",
      "Loss=29.9087\n",
      "Loss=29.5636\n",
      "Loss=29.2263\n",
      "Loss=28.8970\n",
      "Loss=28.5747\n",
      "Loss=28.2584\n",
      "Loss=27.9479\n",
      "Loss=27.6429\n",
      "Loss=27.3432\n",
      "Loss=27.0485\n",
      "Loss=26.7587\n",
      "Loss=26.4748\n",
      "Loss=26.1962\n",
      "Loss=25.9218\n",
      "Loss=25.6514\n",
      "Loss=25.3858\n",
      "Loss=25.1240\n",
      "Loss=24.8657\n",
      "Loss=24.6115\n",
      "Loss=24.3609\n",
      "Loss=24.1141\n",
      "Loss=23.8708\n",
      "Loss=23.6311\n",
      "Loss=23.3955\n",
      "Loss=23.1624\n",
      "Loss=22.9316\n",
      "Loss=22.7031\n",
      "Loss=22.4759\n",
      "Loss=22.2487\n",
      "Loss=22.0188\n",
      "Loss=21.7865\n",
      "Loss=21.5522\n",
      "Loss=21.3141\n",
      "Loss=21.0740\n",
      "Loss=20.8319\n",
      "Loss=20.5934\n",
      "Loss=20.3669\n",
      "Loss=20.1489\n",
      "Loss=19.9395\n",
      "Loss=19.7380\n",
      "Loss=19.5420\n",
      "Loss=19.3504\n",
      "Loss=19.1627\n",
      "Loss=18.9787\n",
      "Loss=18.7976\n",
      "Loss=18.6185\n",
      "Loss=18.4419\n",
      "Loss=18.2678\n",
      "Loss=18.0958\n",
      "Loss=17.9254\n",
      "Loss=17.7572\n",
      "Loss=17.5907\n",
      "Loss=17.4262\n",
      "Loss=17.2626\n",
      "Loss=17.1004\n",
      "Loss=16.9397\n",
      "Loss=16.7803\n",
      "Loss=16.6227\n",
      "Loss=16.4660\n",
      "Loss=16.3101\n",
      "Loss=16.1551\n",
      "Loss=16.0005\n",
      "Loss=15.8471\n",
      "Loss=15.6943\n",
      "Loss=15.5423\n",
      "Loss=15.3924\n",
      "Loss=15.2425\n",
      "Loss=15.0942\n",
      "Loss=14.9466\n",
      "Loss=14.8034\n",
      "Loss=14.6629\n",
      "Loss=14.5276\n",
      "Loss=14.3970\n",
      "Loss=14.2719\n",
      "Loss=14.1516\n",
      "Loss=14.0373\n",
      "Loss=13.9281\n",
      "Loss=13.8261\n",
      "Loss=13.7308\n",
      "Loss=13.6460\n",
      "Loss=13.5694\n",
      "Loss=13.5078\n",
      "Loss=13.4595\n",
      "Loss=13.4254\n",
      "Loss=13.4189\n",
      "Loss=13.4388\n",
      "Loss=13.4861\n",
      "Loss=13.5657\n",
      "Loss=13.6909\n",
      "Loss=13.8611\n",
      "Loss=14.1095\n",
      "Loss=14.4305\n",
      "Loss=14.8895\n",
      "Loss=15.4465\n",
      "Loss=16.2355\n",
      "Loss=17.1579\n",
      "Loss=18.4324\n",
      "Loss=19.8831\n",
      "Loss=21.8862\n",
      "Loss=24.0374\n",
      "Loss=27.0450\n",
      "Loss=29.9869\n",
      "Loss=34.1562\n",
      "Loss=37.8092\n",
      "Loss=42.9365\n",
      "Loss=46.5097\n",
      "Loss=51.6169\n",
      "Loss=53.8025\n",
      "Loss=57.4573\n",
      "Loss=56.9863\n",
      "Loss=57.7710\n",
      "Loss=54.2249\n",
      "Loss=52.0389\n",
      "Loss=46.5542\n",
      "Loss=42.7365\n",
      "Loss=37.2511\n",
      "Loss=33.4592\n",
      "Loss=29.1112\n",
      "Loss=26.1578\n",
      "Loss=23.1044\n",
      "Loss=21.0169\n",
      "Loss=18.9440\n",
      "Loss=17.4973\n",
      "Loss=16.0974\n",
      "Loss=15.1110\n",
      "Loss=14.1819\n",
      "Loss=13.5262\n",
      "Loss=12.9013\n",
      "Loss=12.4459\n",
      "Loss=11.9996\n",
      "Loss=11.6633\n",
      "Loss=11.3219\n",
      "Loss=11.0653\n",
      "Loss=10.8077\n",
      "Loss=10.6143\n",
      "Loss=10.4139\n",
      "Loss=10.2707\n",
      "Loss=10.1234\n",
      "Loss=10.0223\n",
      "Loss=9.9161\n",
      "Loss=9.8575\n",
      "Loss=9.7949\n",
      "Loss=9.7814\n",
      "Loss=9.7635\n",
      "Loss=9.8031\n",
      "Loss=9.8453\n",
      "Loss=9.9505\n",
      "Loss=10.0550\n",
      "Loss=10.2435\n",
      "Loss=10.4328\n",
      "Loss=10.7293\n",
      "Loss=11.0351\n",
      "Loss=11.4869\n",
      "Loss=11.9361\n",
      "Loss=12.6080\n",
      "Loss=13.2969\n",
      "Loss=14.2788\n",
      "Loss=15.2273\n",
      "Loss=16.5769\n",
      "Loss=17.8661\n",
      "Loss=19.7150\n",
      "Loss=21.3556\n",
      "Loss=23.7105\n",
      "Loss=25.6059\n",
      "Loss=28.4007\n",
      "Loss=30.3465\n",
      "Loss=33.3711\n",
      "Loss=34.9986\n",
      "Loss=37.7934\n",
      "Loss=38.5739\n",
      "Loss=40.5346\n",
      "Loss=40.0456\n",
      "Loss=40.7552\n",
      "Loss=38.9844\n",
      "Loss=38.4572\n",
      "Loss=35.8120\n",
      "Loss=34.4726\n",
      "Loss=31.5729\n",
      "Loss=29.8544\n",
      "Loss=27.0522\n",
      "Loss=25.3665\n",
      "Loss=23.0129\n",
      "Loss=21.5531\n",
      "Loss=19.6608\n",
      "Loss=18.4802\n",
      "Loss=17.0129\n",
      "Loss=16.1151\n",
      "Loss=15.0107\n",
      "Loss=14.3486\n",
      "Loss=13.5192\n",
      "Loss=13.0367\n",
      "Loss=12.4253\n",
      "Loss=12.0843\n",
      "Loss=11.6277\n",
      "Loss=11.3998\n",
      "Loss=11.0756\n",
      "Loss=10.9472\n",
      "Loss=10.7255\n",
      "Loss=10.6928\n",
      "Loss=10.5670\n",
      "Loss=10.6191\n",
      "Loss=10.5722\n",
      "Loss=10.7060\n",
      "Loss=10.7405\n",
      "Loss=10.9696\n",
      "Loss=11.0909\n",
      "Loss=11.4234\n",
      "Loss=11.6605\n",
      "Loss=12.1423\n",
      "Loss=12.4924\n",
      "Loss=13.1204\n",
      "Loss=13.5660\n",
      "Loss=14.3477\n",
      "Loss=14.9065\n",
      "Loss=15.8547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        \n",
    "        # layer 3 fully connected 1 unit (output)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "        # dropouts\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        # x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = self.drop2(F.relu(self.lin1(x)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        # x = self.drop3(F.relu(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1512, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.000001)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(500):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
