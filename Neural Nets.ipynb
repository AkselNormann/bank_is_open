{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-2015.pkl\n",
      "2015-2016.pkl\n",
      "(1263, 3, 504)\n",
      "(1269, 3, 504)\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "start = 6\n",
    "with open(game_data_path + files[start], 'rb') as f:\n",
    "    print(files[start])\n",
    "    X, y = pickle.load(f, encoding='latin1')\n",
    "for i in range(1):\n",
    "    with open(game_data_path + files[start + 1 + i], 'rb') as f:\n",
    "        print(files[start + 1 + i])\n",
    "        X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "        print(X.shape)\n",
    "        print(X_add.shape)\n",
    "        X = np.concatenate((X, X_add), axis = 0)\n",
    "        y = np.concatenate((y, y_add), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2476, 3, 504)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1241, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValSplit(X, y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "\n",
    "    p = np.random.permutation(len(X))\n",
    "    X = X[p]\n",
    "    y = y[p]\n",
    "\n",
    "    val = 0.2\n",
    "    val = round(len(X) * val)\n",
    "    X_val = X[:val]\n",
    "    y_val = y[:val]\n",
    "    X = X[val:]\n",
    "    y = y[val:]\n",
    "    \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = trainValSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-2016.pkl\n"
     ]
    }
   ],
   "source": [
    "# BySeason trainValSplit\n",
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "with open(game_data_path + files[7], 'rb') as f:\n",
    "    print(files[7])\n",
    "    X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "X = X[y > 0]\n",
    "y = y[y > 0]\n",
    "X_val = X_val[y_val > 0]\n",
    "y_val = y_val[y_val > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 208.],\n",
      "        [ 197.],\n",
      "        [ 188.],\n",
      "        [ 168.],\n",
      "        [ 218.],\n",
      "        [ 191.],\n",
      "        [ 181.],\n",
      "        [ 188.],\n",
      "        [ 189.],\n",
      "        [ 185.]])\n"
     ]
    }
   ],
   "source": [
    "# Split train/test:\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1512, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (drop1): Dropout(p=0.5)\n",
      "  (drop2): Dropout(p=0.4)\n",
      "  (drop3): Dropout(p=0.25)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=42639.1914\n",
      "Val Loss=9045.6992\n",
      "-----\n",
      "100\n",
      "Loss=889.0920\n",
      "Val Loss=1135.7629\n",
      "-----\n",
      "200\n",
      "Loss=670.3543\n",
      "Val Loss=793.4302\n",
      "-----\n",
      "300\n",
      "Loss=593.2773\n",
      "Val Loss=677.2971\n",
      "-----\n",
      "400\n",
      "Loss=538.7346\n",
      "Val Loss=596.7897\n",
      "-----\n",
      "500\n",
      "Loss=507.6362\n",
      "Val Loss=551.5566\n",
      "-----\n",
      "600\n",
      "Loss=482.6397\n",
      "Val Loss=516.6025\n",
      "-----\n",
      "700\n",
      "Loss=464.2978\n",
      "Val Loss=491.3290\n",
      "-----\n",
      "800\n",
      "Loss=448.8751\n",
      "Val Loss=470.2880\n",
      "-----\n",
      "900\n",
      "Loss=433.8632\n",
      "Val Loss=450.2365\n",
      "-----\n",
      "1000\n",
      "Loss=422.1149\n",
      "Val Loss=435.1807\n",
      "-----\n",
      "1100\n",
      "Loss=414.2003\n",
      "Val Loss=424.8576\n",
      "-----\n",
      "1200\n",
      "Loss=404.8118\n",
      "Val Loss=412.9027\n",
      "-----\n",
      "1300\n",
      "Loss=402.4381\n",
      "Val Loss=409.9925\n",
      "-----\n",
      "1400\n",
      "Loss=392.1365\n",
      "Val Loss=397.4779\n",
      "-----\n",
      "1500\n",
      "Loss=387.4836\n",
      "Val Loss=391.8299\n",
      "-----\n",
      "1600\n",
      "Loss=382.7631\n",
      "Val Loss=386.1480\n",
      "-----\n",
      "1700\n",
      "Loss=377.9149\n",
      "Val Loss=380.2994\n",
      "-----\n",
      "1800\n",
      "Loss=374.8104\n",
      "Val Loss=376.6573\n",
      "-----\n",
      "1900\n",
      "Loss=370.9333\n",
      "Val Loss=372.1880\n",
      "-----\n",
      "2000\n",
      "Loss=369.3873\n",
      "Val Loss=370.2810\n",
      "-----\n",
      "2100\n",
      "Loss=365.9337\n",
      "Val Loss=366.2833\n",
      "-----\n",
      "2200\n",
      "Loss=366.6689\n",
      "Val Loss=367.0104\n",
      "-----\n",
      "2300\n",
      "Loss=364.8392\n",
      "Val Loss=364.8495\n",
      "-----\n",
      "2400\n",
      "Loss=360.8645\n",
      "Val Loss=360.3035\n",
      "-----\n",
      "2500\n",
      "Loss=361.4574\n",
      "Val Loss=360.9555\n",
      "-----\n",
      "2600\n",
      "Loss=359.4532\n",
      "Val Loss=358.7073\n",
      "-----\n",
      "2700\n",
      "Loss=359.8502\n",
      "Val Loss=359.1324\n",
      "-----\n",
      "2800\n",
      "Loss=356.8325\n",
      "Val Loss=355.7172\n",
      "-----\n",
      "2900\n",
      "Loss=358.1307\n",
      "Val Loss=357.2046\n",
      "-----\n",
      "3000\n",
      "Loss=356.5916\n",
      "Val Loss=355.4311\n",
      "-----\n",
      "3100\n",
      "Loss=354.9779\n",
      "Val Loss=353.6173\n",
      "-----\n",
      "3200\n",
      "Loss=353.1539\n",
      "Val Loss=351.5744\n",
      "-----\n",
      "3300\n",
      "Loss=352.3003\n",
      "Val Loss=350.6792\n",
      "-----\n",
      "3400\n",
      "Loss=351.0471\n",
      "Val Loss=349.2984\n",
      "-----\n",
      "3500\n",
      "Loss=350.1522\n",
      "Val Loss=348.3418\n",
      "-----\n",
      "3600\n",
      "Loss=351.3797\n",
      "Val Loss=349.7902\n",
      "-----\n",
      "3700\n",
      "Loss=352.9344\n",
      "Val Loss=351.5326\n",
      "-----\n",
      "3800\n",
      "Loss=351.8825\n",
      "Val Loss=350.3592\n",
      "-----\n",
      "3900\n",
      "Loss=349.7042\n",
      "Val Loss=347.9326\n",
      "-----\n",
      "4000\n",
      "Loss=348.4247\n",
      "Val Loss=346.5319\n",
      "-----\n",
      "4100\n",
      "Loss=347.6537\n",
      "Val Loss=345.7076\n",
      "-----\n",
      "4200\n",
      "Loss=348.0457\n",
      "Val Loss=346.1729\n",
      "-----\n",
      "4300\n",
      "Loss=348.5132\n",
      "Val Loss=346.7318\n",
      "-----\n",
      "4400\n",
      "Loss=349.2248\n",
      "Val Loss=347.5233\n",
      "-----\n",
      "4500\n",
      "Loss=349.4389\n",
      "Val Loss=347.7744\n",
      "-----\n",
      "4600\n",
      "Loss=349.5278\n",
      "Val Loss=347.8915\n",
      "-----\n",
      "4700\n",
      "Loss=348.9956\n",
      "Val Loss=347.3134\n",
      "-----\n",
      "4800\n",
      "Loss=348.2706\n",
      "Val Loss=346.5184\n",
      "-----\n",
      "4900\n",
      "Loss=347.8843\n",
      "Val Loss=346.1331\n",
      "-----\n",
      "5000\n",
      "Loss=348.0951\n",
      "Val Loss=346.3904\n",
      "-----\n",
      "5100\n",
      "Loss=348.1942\n",
      "Val Loss=346.5114\n",
      "-----\n",
      "5200\n",
      "Loss=349.9934\n",
      "Val Loss=348.5853\n",
      "-----\n",
      "5300\n",
      "Loss=350.8507\n",
      "Val Loss=349.5391\n",
      "-----\n",
      "5400\n",
      "Loss=349.5467\n",
      "Val Loss=348.0971\n",
      "-----\n",
      "5500\n",
      "Loss=348.3846\n",
      "Val Loss=346.8204\n",
      "-----\n",
      "5600\n",
      "Loss=347.6631\n",
      "Val Loss=346.0528\n",
      "-----\n",
      "5700\n",
      "Loss=347.7489\n",
      "Val Loss=346.1661\n",
      "-----\n",
      "5800\n",
      "Loss=348.1443\n",
      "Val Loss=346.6267\n",
      "-----\n",
      "5900\n",
      "Loss=348.4531\n",
      "Val Loss=346.9890\n",
      "-----\n",
      "6000\n",
      "Loss=348.1266\n",
      "Val Loss=346.6542\n",
      "-----\n",
      "6100\n",
      "Loss=347.7426\n",
      "Val Loss=346.2592\n",
      "-----\n",
      "6200\n",
      "Loss=347.3387\n",
      "Val Loss=345.8185\n",
      "-----\n",
      "6300\n",
      "Loss=347.3824\n",
      "Val Loss=345.8895\n",
      "-----\n",
      "6400\n",
      "Loss=347.7571\n",
      "Val Loss=346.3405\n",
      "-----\n",
      "6500\n",
      "Loss=347.3360\n",
      "Val Loss=345.8777\n",
      "-----\n",
      "6600\n",
      "Loss=347.0665\n",
      "Val Loss=345.6018\n",
      "-----\n",
      "6700\n",
      "Loss=347.2772\n",
      "Val Loss=345.8781\n",
      "-----\n",
      "6800\n",
      "Loss=347.6578\n",
      "Val Loss=346.2975\n",
      "-----\n",
      "6900\n",
      "Loss=347.8240\n",
      "Val Loss=346.5162\n",
      "-----\n",
      "7000\n",
      "Loss=348.0381\n",
      "Val Loss=346.7607\n",
      "-----\n",
      "7100\n",
      "Loss=347.9644\n",
      "Val Loss=346.6995\n",
      "-----\n",
      "7200\n",
      "Loss=347.7301\n",
      "Val Loss=346.4511\n",
      "-----\n",
      "7300\n",
      "Loss=347.4658\n",
      "Val Loss=346.1801\n",
      "-----\n",
      "7400\n",
      "Loss=347.1749\n",
      "Val Loss=345.8683\n",
      "-----\n",
      "7500\n",
      "Loss=347.1547\n",
      "Val Loss=345.8725\n",
      "-----\n",
      "7600\n",
      "Loss=347.3034\n",
      "Val Loss=346.0597\n",
      "-----\n",
      "7700\n",
      "Loss=347.4276\n",
      "Val Loss=346.2244\n",
      "-----\n",
      "7800\n",
      "Loss=347.2933\n",
      "Val Loss=346.0886\n",
      "-----\n",
      "7900\n",
      "Loss=346.9502\n",
      "Val Loss=345.7329\n",
      "-----\n",
      "8000\n",
      "Loss=346.0056\n",
      "Val Loss=344.7083\n",
      "-----\n",
      "8100\n",
      "Loss=345.4325\n",
      "Val Loss=344.1029\n",
      "-----\n",
      "8200\n",
      "Loss=346.1068\n",
      "Val Loss=344.8919\n",
      "-----\n",
      "8300\n",
      "Loss=346.8240\n",
      "Val Loss=345.6973\n",
      "-----\n",
      "8400\n",
      "Loss=347.0307\n",
      "Val Loss=345.9441\n",
      "-----\n",
      "8500\n",
      "Loss=347.0394\n",
      "Val Loss=345.9858\n",
      "-----\n",
      "8600\n",
      "Loss=346.8712\n",
      "Val Loss=345.7993\n",
      "-----\n",
      "8700\n",
      "Loss=346.8581\n",
      "Val Loss=345.7979\n",
      "-----\n",
      "8800\n",
      "Loss=346.8830\n",
      "Val Loss=345.8518\n",
      "-----\n",
      "8900\n",
      "Loss=346.6172\n",
      "Val Loss=345.5849\n",
      "-----\n",
      "9000\n",
      "Loss=346.3357\n",
      "Val Loss=345.2941\n",
      "-----\n",
      "9100\n",
      "Loss=345.6945\n",
      "Val Loss=344.6202\n",
      "-----\n",
      "9200\n",
      "Loss=345.4646\n",
      "Val Loss=344.3774\n",
      "-----\n",
      "9300\n",
      "Loss=346.1385\n",
      "Val Loss=345.1420\n",
      "-----\n",
      "9400\n",
      "Loss=346.7934\n",
      "Val Loss=345.8828\n",
      "-----\n",
      "9500\n",
      "Loss=346.5353\n",
      "Val Loss=345.6066\n",
      "-----\n",
      "9600\n",
      "Loss=346.4939\n",
      "Val Loss=345.5755\n",
      "-----\n",
      "9700\n",
      "Loss=346.4118\n",
      "Val Loss=345.4991\n",
      "-----\n",
      "9800\n",
      "Loss=346.4510\n",
      "Val Loss=345.5607\n",
      "-----\n",
      "9900\n",
      "Loss=346.3215\n",
      "Val Loss=345.4312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        \n",
    "        # layer 3 fully connected 1 unit (output)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "        # self.lin4 = nn.Linear(50, n_output)\n",
    "        \n",
    "        # dropouts\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        # x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = self.drop2(F.relu(self.lin1(x)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        # x = self.drop3(F.relu(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1512, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.000001)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
