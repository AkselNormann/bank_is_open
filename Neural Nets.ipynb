{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# use surprise for collaborative filtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-2014.pkl\n",
      "2014-2015.pkl\n",
      "(1272, 3, 508)\n",
      "(1263, 3, 508)\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "start = 5\n",
    "with open(game_data_path + files[start], 'rb') as f:\n",
    "    print(files[start])\n",
    "    X, y = pickle.load(f, encoding='latin1')\n",
    "for i in range(1):\n",
    "    with open(game_data_path + files[start + 1 + i], 'rb') as f:\n",
    "        print(files[start + 1 + i])\n",
    "        X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "        print(X.shape)\n",
    "        print(X_add.shape)\n",
    "        X = np.concatenate((X, X_add), axis = 0)\n",
    "        y = np.concatenate((y, y_add), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2475, 1524])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2475, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValSplit(X, y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "\n",
    "    p = np.random.permutation(len(X))\n",
    "    X = X[p]\n",
    "    y = y[p]\n",
    "\n",
    "    val = 0.2\n",
    "    val = round(len(X) * val)\n",
    "    X_val = X[:val]\n",
    "    y_val = y[:val]\n",
    "    X = X[val:]\n",
    "    y = y[val:]\n",
    "    \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = trainValSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-2016.pkl\n"
     ]
    }
   ],
   "source": [
    "# BySeason trainValSplit\n",
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "with open(game_data_path + files[7], 'rb') as f:\n",
    "    print(files[7])\n",
    "    X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "X = X[y > 0]\n",
    "y = y[y > 0]\n",
    "X_val = X_val[y_val > 0]\n",
    "y_val = y_val[y_val > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 185.],\n",
      "        [ 255.],\n",
      "        [ 183.],\n",
      "        [ 200.],\n",
      "        [ 192.],\n",
      "        [ 227.],\n",
      "        [ 190.],\n",
      "        [ 202.],\n",
      "        [ 199.],\n",
      "        [ 217.]])\n"
     ]
    }
   ],
   "source": [
    "# Split train/test:\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (drop1): Dropout(p=0.5)\n",
      "  (drop2): Dropout(p=0.4)\n",
      "  (drop3): Dropout(p=0.25)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=36825.2031\n",
      "Val Loss=7107.9331\n",
      "-----\n",
      "100\n",
      "Loss=1170.4954\n",
      "Val Loss=10611.4912\n",
      "-----\n",
      "200\n",
      "Loss=807.7614\n",
      "Val Loss=10225.2627\n",
      "-----\n",
      "300\n",
      "Loss=677.3436\n",
      "Val Loss=10075.8594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5758bd32ac39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# input x and predict based on x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# must be (1. nn output, 2. target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-5758bd32ac39>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# perform dropout on input vector embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# x = self.drop1(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# x = self.drop2(F.relu(self.lin1(x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        \n",
    "        # layer 3 fully connected 1 unit (output)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "        # self.lin4 = nn.Linear(50, n_output)\n",
    "        \n",
    "        # dropouts\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        # x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = self.drop2(F.relu(self.lin1(x)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        # x = self.drop3(F.relu(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.000001, weight_decay = 1e-2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-2013.pkl\n",
      "2013-2014.pkl\n",
      "(1265, 3, 508)\n",
      "(1272, 3, 508)\n",
      "2014-2015.pkl\n",
      "(2537, 3, 508)\n",
      "(1263, 3, 508)\n",
      "2015-2016.pkl\n",
      "(3800, 3, 508)\n",
      "(1269, 3, 508)\n",
      "2016-2017.pkl\n",
      "2017-2018.pkl\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "\n",
    "files = [\"2012-2013.pkl\", \"2013-2014.pkl\", \"2014-2015.pkl\", \"2015-2016.pkl\",\"2016-2017.pkl\", \"2017-2018.pkl\"]\n",
    "\n",
    "X_train = np.zeros(5)\n",
    "\n",
    "for file in files[:-2]:\n",
    "    if \".pkl\" not in file: continue\n",
    "    \n",
    "    with open(game_data_path + file, 'rb') as f:\n",
    "        print(file)\n",
    "        if X_train.shape[0] == 5:\n",
    "            X_train, y_train = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "            print(X_train.shape)\n",
    "            print(X_add.shape)\n",
    "            X_train = np.concatenate((X_train, X_add), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_add), axis = 0)\n",
    "\n",
    "with open(game_data_path + files[-2], 'rb') as f:\n",
    "        print(files[-2])\n",
    "        X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "with open(game_data_path + files[-1], 'rb') as f:\n",
    "        print(files[-1])\n",
    "        X_test, y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X, y):\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X.reshape((X.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val.reshape((X_val.shape[0], -1))).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test.reshape((X_test.shape[0], -1))).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=33785.3164\n",
      "Val Loss=8918.6826\n",
      "-----\n",
      "100\n",
      "Loss=1563.3461\n",
      "Val Loss=2937.0347\n",
      "-----\n",
      "200\n",
      "Loss=985.1339\n",
      "Val Loss=1594.7726\n",
      "-----\n",
      "300\n",
      "Loss=765.8445\n",
      "Val Loss=1144.1522\n",
      "-----\n",
      "400\n",
      "Loss=654.3577\n",
      "Val Loss=932.2725\n",
      "-----\n",
      "500\n",
      "Loss=595.5217\n",
      "Val Loss=825.0071\n",
      "-----\n",
      "600\n",
      "Loss=556.5037\n",
      "Val Loss=755.7455\n",
      "-----\n",
      "700\n",
      "Loss=525.3077\n",
      "Val Loss=701.0831\n",
      "-----\n",
      "800\n",
      "Loss=502.7728\n",
      "Val Loss=662.5638\n",
      "-----\n",
      "900\n",
      "Loss=484.7969\n",
      "Val Loss=631.7883\n",
      "-----\n",
      "1000\n",
      "Loss=471.7117\n",
      "Val Loss=609.6647\n",
      "-----\n",
      "1100\n",
      "Loss=458.5612\n",
      "Val Loss=587.8519\n",
      "-----\n",
      "1200\n",
      "Loss=447.8412\n",
      "Val Loss=569.7866\n",
      "-----\n",
      "1300\n",
      "Loss=442.5939\n",
      "Val Loss=560.9316\n",
      "-----\n",
      "1400\n",
      "Loss=439.7016\n",
      "Val Loss=555.8060\n",
      "-----\n",
      "1500\n",
      "Loss=431.0389\n",
      "Val Loss=541.8175\n",
      "-----\n",
      "1600\n",
      "Loss=424.8046\n",
      "Val Loss=531.2537\n",
      "-----\n",
      "1700\n",
      "Loss=423.2522\n",
      "Val Loss=529.3687\n",
      "-----\n",
      "1800\n",
      "Loss=411.8873\n",
      "Val Loss=509.8192\n",
      "-----\n",
      "1900\n",
      "Loss=403.7342\n",
      "Val Loss=495.9784\n",
      "-----\n",
      "2000\n",
      "Loss=397.7825\n",
      "Val Loss=486.1068\n",
      "-----\n",
      "2100\n",
      "Loss=397.4094\n",
      "Val Loss=485.5478\n",
      "-----\n",
      "2200\n",
      "Loss=395.6142\n",
      "Val Loss=482.7234\n",
      "-----\n",
      "2300\n",
      "Loss=387.7166\n",
      "Val Loss=469.3416\n",
      "-----\n",
      "2400\n",
      "Loss=384.9765\n",
      "Val Loss=464.8377\n",
      "-----\n",
      "2500\n",
      "Loss=381.7712\n",
      "Val Loss=459.3698\n",
      "-----\n",
      "2600\n",
      "Loss=377.9476\n",
      "Val Loss=452.8523\n",
      "-----\n",
      "2700\n",
      "Loss=375.2342\n",
      "Val Loss=448.1406\n",
      "-----\n",
      "2800\n",
      "Loss=373.5734\n",
      "Val Loss=445.4286\n",
      "-----\n",
      "2900\n",
      "Loss=371.6293\n",
      "Val Loss=442.0831\n",
      "-----\n",
      "3000\n",
      "Loss=368.0094\n",
      "Val Loss=435.5896\n",
      "-----\n",
      "3100\n",
      "Loss=367.1343\n",
      "Val Loss=434.2079\n",
      "-----\n",
      "3200\n",
      "Loss=366.5140\n",
      "Val Loss=433.2523\n",
      "-----\n",
      "3300\n",
      "Loss=366.0182\n",
      "Val Loss=432.4929\n",
      "-----\n",
      "3400\n",
      "Loss=366.4672\n",
      "Val Loss=433.6159\n",
      "-----\n",
      "3500\n",
      "Loss=366.2339\n",
      "Val Loss=433.3291\n",
      "-----\n",
      "3600\n",
      "Loss=364.7100\n",
      "Val Loss=430.6276\n",
      "-----\n",
      "3700\n",
      "Loss=363.2518\n",
      "Val Loss=428.0550\n",
      "-----\n",
      "3800\n",
      "Loss=363.7597\n",
      "Val Loss=429.2303\n",
      "-----\n",
      "3900\n",
      "Loss=364.8251\n",
      "Val Loss=431.4604\n",
      "-----\n",
      "4000\n",
      "Loss=363.2684\n",
      "Val Loss=428.6893\n",
      "-----\n",
      "4100\n",
      "Loss=361.4476\n",
      "Val Loss=425.4147\n",
      "-----\n",
      "4200\n",
      "Loss=361.5667\n",
      "Val Loss=425.8189\n",
      "-----\n",
      "4300\n",
      "Loss=361.0224\n",
      "Val Loss=425.0169\n",
      "-----\n",
      "4400\n",
      "Loss=360.6242\n",
      "Val Loss=424.3706\n",
      "-----\n",
      "4500\n",
      "Loss=359.7036\n",
      "Val Loss=422.7572\n",
      "-----\n",
      "4600\n",
      "Loss=358.5341\n",
      "Val Loss=420.6340\n",
      "-----\n",
      "4700\n",
      "Loss=357.6155\n",
      "Val Loss=419.0080\n",
      "-----\n",
      "4800\n",
      "Loss=355.7886\n",
      "Val Loss=415.5068\n",
      "-----\n",
      "4900\n",
      "Loss=355.2842\n",
      "Val Loss=414.7288\n",
      "-----\n",
      "5000\n",
      "Loss=355.5103\n",
      "Val Loss=415.4456\n",
      "-----\n",
      "5100\n",
      "Loss=355.3832\n",
      "Val Loss=415.4041\n",
      "-----\n",
      "5200\n",
      "Loss=355.4253\n",
      "Val Loss=415.7044\n",
      "-----\n",
      "5300\n",
      "Loss=355.3907\n",
      "Val Loss=415.8365\n",
      "-----\n",
      "5400\n",
      "Loss=355.0202\n",
      "Val Loss=415.2882\n",
      "-----\n",
      "5500\n",
      "Loss=355.2706\n",
      "Val Loss=415.9778\n",
      "-----\n",
      "5600\n",
      "Loss=355.4169\n",
      "Val Loss=416.4919\n",
      "-----\n",
      "5700\n",
      "Loss=355.0399\n",
      "Val Loss=415.9427\n",
      "-----\n",
      "5800\n",
      "Loss=356.0314\n",
      "Val Loss=418.2155\n",
      "-----\n",
      "5900\n",
      "Loss=356.6409\n",
      "Val Loss=419.5242\n",
      "-----\n",
      "6000\n",
      "Loss=356.3663\n",
      "Val Loss=419.1824\n",
      "-----\n",
      "6100\n",
      "Loss=356.0518\n",
      "Val Loss=418.7368\n",
      "-----\n",
      "6200\n",
      "Loss=355.7024\n",
      "Val Loss=418.2462\n",
      "-----\n",
      "6300\n",
      "Loss=355.5407\n",
      "Val Loss=418.1396\n",
      "-----\n",
      "6400\n",
      "Loss=355.2729\n",
      "Val Loss=417.7886\n",
      "-----\n",
      "6500\n",
      "Loss=355.3227\n",
      "Val Loss=418.1324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-70c5842ab558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, 20)\n",
    "        self.lin3 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=42581.4375\n",
      "Val Loss=10719.4131\n",
      "-----\n",
      "100\n",
      "Loss=1306.5361\n",
      "Val Loss=2321.2966\n",
      "-----\n",
      "200\n",
      "Loss=850.5865\n",
      "Val Loss=1298.9583\n",
      "-----\n",
      "300\n",
      "Loss=689.8105\n",
      "Val Loss=990.5787\n",
      "-----\n",
      "400\n",
      "Loss=608.4941\n",
      "Val Loss=843.3696\n",
      "-----\n",
      "500\n",
      "Loss=557.8242\n",
      "Val Loss=754.6586\n",
      "-----\n",
      "600\n",
      "Loss=524.5273\n",
      "Val Loss=698.3322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9897ec0aae13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        self.lin3 = nn.Linear(100, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=37606.9297\n",
      "Val Loss=11558.6992\n",
      "-----\n",
      "100\n",
      "Loss=1368.6725\n",
      "Val Loss=2553.0046\n",
      "-----\n",
      "200\n",
      "Loss=955.1909\n",
      "Val Loss=1541.4231\n",
      "-----\n",
      "300\n",
      "Loss=795.8029\n",
      "Val Loss=1208.0969\n",
      "-----\n",
      "400\n",
      "Loss=709.5984\n",
      "Val Loss=1038.4926\n",
      "-----\n",
      "500\n",
      "Loss=646.3178\n",
      "Val Loss=915.2849\n",
      "-----\n",
      "600\n",
      "Loss=610.1110\n",
      "Val Loss=848.4869\n",
      "-----\n",
      "700\n",
      "Loss=574.7874\n",
      "Val Loss=783.7828\n",
      "-----\n",
      "800\n",
      "Loss=548.4568\n",
      "Val Loss=736.1489\n",
      "-----\n",
      "900\n",
      "Loss=527.2394\n",
      "Val Loss=699.5761\n",
      "-----\n",
      "1000\n",
      "Loss=510.4058\n",
      "Val Loss=671.8802\n",
      "-----\n",
      "1100\n",
      "Loss=492.3026\n",
      "Val Loss=641.2138\n",
      "-----\n",
      "1200\n",
      "Loss=477.6333\n",
      "Val Loss=615.8599\n",
      "-----\n",
      "1300\n",
      "Loss=468.2047\n",
      "Val Loss=600.1415\n",
      "-----\n",
      "1400\n",
      "Loss=455.9815\n",
      "Val Loss=580.1837\n",
      "-----\n",
      "1500\n",
      "Loss=448.0787\n",
      "Val Loss=567.5602\n",
      "-----\n",
      "1600\n",
      "Loss=443.2267\n",
      "Val Loss=560.1873\n",
      "-----\n",
      "1700\n",
      "Loss=435.6959\n",
      "Val Loss=547.8133\n",
      "-----\n",
      "1800\n",
      "Loss=430.2346\n",
      "Val Loss=539.0519\n",
      "-----\n",
      "1900\n",
      "Loss=426.4042\n",
      "Val Loss=532.8874\n",
      "-----\n",
      "2000\n",
      "Loss=427.5907\n",
      "Val Loss=535.1319\n",
      "-----\n",
      "2100\n",
      "Loss=422.4556\n",
      "Val Loss=526.8211\n",
      "-----\n",
      "2200\n",
      "Loss=417.0105\n",
      "Val Loss=518.0982\n",
      "-----\n",
      "2300\n",
      "Loss=411.3961\n",
      "Val Loss=508.8803\n",
      "-----\n",
      "2400\n",
      "Loss=409.5310\n",
      "Val Loss=506.0508\n",
      "-----\n",
      "2500\n",
      "Loss=403.2860\n",
      "Val Loss=495.9080\n",
      "-----\n",
      "2600\n",
      "Loss=399.6589\n",
      "Val Loss=490.1894\n",
      "-----\n",
      "2700\n",
      "Loss=399.5225\n",
      "Val Loss=490.3832\n",
      "-----\n",
      "2800\n",
      "Loss=393.0695\n",
      "Val Loss=479.7225\n",
      "-----\n",
      "2900\n",
      "Loss=388.8851\n",
      "Val Loss=472.8209\n",
      "-----\n",
      "3000\n",
      "Loss=389.8295\n",
      "Val Loss=474.7453\n",
      "-----\n",
      "3100\n",
      "Loss=385.7337\n",
      "Val Loss=468.1717\n",
      "-----\n",
      "3200\n",
      "Loss=381.8192\n",
      "Val Loss=461.5987\n",
      "-----\n",
      "3300\n",
      "Loss=382.3410\n",
      "Val Loss=462.9368\n",
      "-----\n",
      "3400\n",
      "Loss=379.6094\n",
      "Val Loss=458.3533\n",
      "-----\n",
      "3500\n",
      "Loss=375.9757\n",
      "Val Loss=452.1883\n",
      "-----\n",
      "3600\n",
      "Loss=372.9839\n",
      "Val Loss=447.1633\n",
      "-----\n",
      "3700\n",
      "Loss=373.3954\n",
      "Val Loss=448.1543\n",
      "-----\n",
      "3800\n",
      "Loss=376.3696\n",
      "Val Loss=453.6820\n",
      "-----\n",
      "3900\n",
      "Loss=373.6752\n",
      "Val Loss=449.1070\n",
      "-----\n",
      "4000\n",
      "Loss=373.0878\n",
      "Val Loss=448.2971\n",
      "-----\n",
      "4100\n",
      "Loss=382.9492\n",
      "Val Loss=465.8482\n",
      "-----\n",
      "4200\n",
      "Loss=386.0651\n",
      "Val Loss=471.4705\n",
      "-----\n",
      "4300\n",
      "Loss=380.1551\n",
      "Val Loss=461.4237\n",
      "-----\n",
      "4400\n",
      "Loss=381.3483\n",
      "Val Loss=463.7497\n",
      "-----\n",
      "4500\n",
      "Loss=379.3018\n",
      "Val Loss=460.4229\n",
      "-----\n",
      "4600\n",
      "Loss=374.3010\n",
      "Val Loss=451.9472\n",
      "-----\n",
      "4700\n",
      "Loss=370.4363\n",
      "Val Loss=445.3591\n",
      "-----\n",
      "4800\n",
      "Loss=369.5335\n",
      "Val Loss=443.8275\n",
      "-----\n",
      "4900\n",
      "Loss=369.8821\n",
      "Val Loss=444.6233\n",
      "-----\n",
      "5000\n",
      "Loss=371.1474\n",
      "Val Loss=447.0870\n",
      "-----\n",
      "5100\n",
      "Loss=371.4919\n",
      "Val Loss=447.8318\n",
      "-----\n",
      "5200\n",
      "Loss=369.5501\n",
      "Val Loss=444.5900\n",
      "-----\n",
      "5300\n",
      "Loss=367.2194\n",
      "Val Loss=440.5443\n",
      "-----\n",
      "5400\n",
      "Loss=365.4702\n",
      "Val Loss=437.5421\n",
      "-----\n",
      "5500\n",
      "Loss=365.0505\n",
      "Val Loss=437.0020\n",
      "-----\n",
      "5600\n",
      "Loss=364.9515\n",
      "Val Loss=436.9209\n",
      "-----\n",
      "5700\n",
      "Loss=364.5691\n",
      "Val Loss=436.4106\n",
      "-----\n",
      "5800\n",
      "Loss=364.1082\n",
      "Val Loss=435.6190\n",
      "-----\n",
      "5900\n",
      "Loss=362.6435\n",
      "Val Loss=433.0969\n",
      "-----\n",
      "6000\n",
      "Loss=361.4625\n",
      "Val Loss=431.0419\n",
      "-----\n",
      "6100\n",
      "Loss=361.0588\n",
      "Val Loss=430.4490\n",
      "-----\n",
      "6200\n",
      "Loss=361.2514\n",
      "Val Loss=430.9656\n",
      "-----\n",
      "6300\n",
      "Loss=361.5981\n",
      "Val Loss=431.7760\n",
      "-----\n",
      "6400\n",
      "Loss=361.5125\n",
      "Val Loss=431.7606\n",
      "-----\n",
      "6500\n",
      "Loss=361.0179\n",
      "Val Loss=430.9814\n",
      "-----\n",
      "6600\n",
      "Loss=360.3048\n",
      "Val Loss=429.7789\n",
      "-----\n",
      "6700\n",
      "Loss=359.7066\n",
      "Val Loss=428.8293\n",
      "-----\n",
      "6800\n",
      "Loss=359.5605\n",
      "Val Loss=428.7033\n",
      "-----\n",
      "6900\n",
      "Loss=359.5238\n",
      "Val Loss=428.7367\n",
      "-----\n",
      "7000\n",
      "Loss=359.4157\n",
      "Val Loss=428.6599\n",
      "-----\n",
      "7100\n",
      "Loss=359.5300\n",
      "Val Loss=429.0182\n",
      "-----\n",
      "7200\n",
      "Loss=359.7588\n",
      "Val Loss=429.6122\n",
      "-----\n",
      "7300\n",
      "Loss=359.9913\n",
      "Val Loss=430.1755\n",
      "-----\n",
      "7400\n",
      "Loss=360.0109\n",
      "Val Loss=430.3483\n",
      "-----\n",
      "7500\n",
      "Loss=359.7708\n",
      "Val Loss=430.0252\n",
      "-----\n",
      "7600\n",
      "Loss=359.8484\n",
      "Val Loss=430.3162\n",
      "-----\n",
      "7700\n",
      "Loss=359.9969\n",
      "Val Loss=430.7301\n",
      "-----\n",
      "7800\n",
      "Loss=359.9460\n",
      "Val Loss=430.7636\n",
      "-----\n",
      "7900\n",
      "Loss=360.0738\n",
      "Val Loss=431.1246\n",
      "-----\n",
      "8000\n",
      "Loss=360.2249\n",
      "Val Loss=431.5795\n",
      "-----\n",
      "8100\n",
      "Loss=360.4987\n",
      "Val Loss=432.2188\n",
      "-----\n",
      "8200\n",
      "Loss=360.4989\n",
      "Val Loss=432.3384\n",
      "-----\n",
      "8300\n",
      "Loss=360.2130\n",
      "Val Loss=431.9329\n",
      "-----\n",
      "8400\n",
      "Loss=360.0841\n",
      "Val Loss=431.8096\n",
      "-----\n",
      "8500\n",
      "Loss=360.0184\n",
      "Val Loss=431.8214\n",
      "-----\n",
      "8600\n",
      "Loss=360.1673\n",
      "Val Loss=432.2284\n",
      "-----\n",
      "8700\n",
      "Loss=360.2498\n",
      "Val Loss=432.5141\n",
      "-----\n",
      "8800\n",
      "Loss=360.2021\n",
      "Val Loss=432.5626\n",
      "-----\n",
      "8900\n",
      "Loss=360.1523\n",
      "Val Loss=432.5879\n",
      "-----\n",
      "9000\n",
      "Loss=360.1832\n",
      "Val Loss=432.7579\n",
      "-----\n",
      "9100\n",
      "Loss=360.2911\n",
      "Val Loss=433.0720\n",
      "-----\n",
      "9200\n",
      "Loss=360.2441\n",
      "Val Loss=433.1069\n",
      "-----\n",
      "9300\n",
      "Loss=360.1350\n",
      "Val Loss=433.0006\n",
      "-----\n",
      "9400\n",
      "Loss=360.1742\n",
      "Val Loss=433.1856\n",
      "-----\n",
      "9500\n",
      "Loss=360.2268\n",
      "Val Loss=433.3943\n",
      "-----\n",
      "9600\n",
      "Loss=360.3090\n",
      "Val Loss=433.6687\n",
      "-----\n",
      "9700\n",
      "Loss=360.5567\n",
      "Val Loss=434.2415\n",
      "-----\n",
      "9800\n",
      "Loss=360.8636\n",
      "Val Loss=434.9050\n",
      "-----\n",
      "9900\n",
      "Loss=360.9444\n",
      "Val Loss=435.1669\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, 20)\n",
    "        self.lin3 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 10)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score,min_score = y.max(),y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=6283.0562\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "10\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "20\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "30\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "40\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "50\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "60\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "70\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "80\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "90\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "100\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "110\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "120\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "130\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n",
      "-----\n",
      "140\n",
      "Loss=4894.1030\n",
      "Val Loss=6437.1416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f0f5211d0a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(100, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        # x = F.relu(self.lin2(x))\n",
    "        x = F.sigmoid(self.lin2(x)) * (max_score-min_score+1) + min_score-0.5\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (lin4): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=38260.6602\n",
      "Val Loss=32118.7559\n",
      "-----\n",
      "100\n",
      "Loss=1203.2887\n",
      "Val Loss=2061.3198\n",
      "-----\n",
      "200\n",
      "Loss=687.3368\n",
      "Val Loss=992.9130\n",
      "-----\n",
      "300\n",
      "Loss=542.2668\n",
      "Val Loss=727.0233\n",
      "-----\n",
      "400\n",
      "Loss=482.4226\n",
      "Val Loss=624.8755\n",
      "-----\n",
      "500\n",
      "Loss=448.2942\n",
      "Val Loss=568.5328\n",
      "-----\n",
      "600\n",
      "Loss=428.9934\n",
      "Val Loss=536.9005\n",
      "-----\n",
      "700\n",
      "Loss=417.5521\n",
      "Val Loss=518.4960\n",
      "-----\n",
      "800\n",
      "Loss=409.3563\n",
      "Val Loss=505.0832\n",
      "-----\n",
      "900\n",
      "Loss=398.0174\n",
      "Val Loss=486.2299\n",
      "-----\n",
      "1000\n",
      "Loss=391.3517\n",
      "Val Loss=475.3676\n",
      "-----\n",
      "1100\n",
      "Loss=381.5217\n",
      "Val Loss=458.7466\n",
      "-----\n",
      "1200\n",
      "Loss=377.2135\n",
      "Val Loss=451.6601\n",
      "-----\n",
      "1300\n",
      "Loss=374.6504\n",
      "Val Loss=447.6359\n",
      "-----\n",
      "1400\n",
      "Loss=370.7366\n",
      "Val Loss=441.0936\n",
      "-----\n",
      "1500\n",
      "Loss=367.8433\n",
      "Val Loss=436.3712\n",
      "-----\n",
      "1600\n",
      "Loss=364.0118\n",
      "Val Loss=429.7424\n",
      "-----\n",
      "1700\n",
      "Loss=359.5113\n",
      "Val Loss=421.5800\n",
      "-----\n",
      "1800\n",
      "Loss=355.9825\n",
      "Val Loss=415.0813\n",
      "-----\n",
      "1900\n",
      "Loss=354.5308\n",
      "Val Loss=412.6240\n",
      "-----\n",
      "2000\n",
      "Loss=353.1360\n",
      "Val Loss=410.2182\n",
      "-----\n",
      "2100\n",
      "Loss=351.8448\n",
      "Val Loss=407.9746\n",
      "-----\n",
      "2200\n",
      "Loss=351.0815\n",
      "Val Loss=406.8042\n",
      "-----\n",
      "2300\n",
      "Loss=351.1349\n",
      "Val Loss=407.4483\n",
      "-----\n",
      "2400\n",
      "Loss=351.3740\n",
      "Val Loss=408.4311\n",
      "-----\n",
      "2500\n",
      "Loss=350.8665\n",
      "Val Loss=407.7699\n",
      "-----\n",
      "2600\n",
      "Loss=350.2853\n",
      "Val Loss=406.9642\n",
      "-----\n",
      "2700\n",
      "Loss=350.0085\n",
      "Val Loss=406.8376\n",
      "-----\n",
      "2800\n",
      "Loss=349.9542\n",
      "Val Loss=407.1413\n",
      "-----\n",
      "2900\n",
      "Loss=349.8171\n",
      "Val Loss=407.2783\n",
      "-----\n",
      "3000\n",
      "Loss=349.7096\n",
      "Val Loss=407.4395\n",
      "-----\n",
      "3100\n",
      "Loss=349.4848\n",
      "Val Loss=407.4366\n",
      "-----\n",
      "3200\n",
      "Loss=349.4795\n",
      "Val Loss=407.8471\n",
      "-----\n",
      "3300\n",
      "Loss=349.4475\n",
      "Val Loss=408.2299\n",
      "-----\n",
      "3400\n",
      "Loss=349.0112\n",
      "Val Loss=407.6901\n",
      "-----\n",
      "3500\n",
      "Loss=348.5357\n",
      "Val Loss=407.0500\n",
      "-----\n",
      "3600\n",
      "Loss=347.9762\n",
      "Val Loss=406.2316\n",
      "-----\n",
      "3700\n",
      "Loss=347.5095\n",
      "Val Loss=405.5742\n",
      "-----\n",
      "3800\n",
      "Loss=347.8011\n",
      "Val Loss=406.6797\n",
      "-----\n",
      "3900\n",
      "Loss=348.1069\n",
      "Val Loss=407.7462\n",
      "-----\n",
      "4000\n",
      "Loss=348.1644\n",
      "Val Loss=408.2410\n",
      "-----\n",
      "4100\n",
      "Loss=347.8234\n",
      "Val Loss=407.8851\n",
      "-----\n",
      "4200\n",
      "Loss=347.7816\n",
      "Val Loss=408.1896\n",
      "-----\n",
      "4300\n",
      "Loss=347.8598\n",
      "Val Loss=408.7404\n",
      "-----\n",
      "4400\n",
      "Loss=348.1824\n",
      "Val Loss=409.8003\n",
      "-----\n",
      "4500\n",
      "Loss=348.1569\n",
      "Val Loss=410.0932\n",
      "-----\n",
      "4600\n",
      "Loss=347.5722\n",
      "Val Loss=409.1390\n",
      "-----\n",
      "4700\n",
      "Loss=346.9363\n",
      "Val Loss=408.1341\n",
      "-----\n",
      "4800\n",
      "Loss=346.9998\n",
      "Val Loss=408.6399\n",
      "-----\n",
      "4900\n",
      "Loss=347.2430\n",
      "Val Loss=409.5138\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        self.lin3 = nn.Linear(100, 20)\n",
    "        self.lin4 = nn.Linear(20, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = self.lin4(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "for t in range(5000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "        losses.append(loss.data.numpy())\n",
    "        val_losses.append(loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100*(np.arange(50) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-aab748f643c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10026b89b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(5000, losses, color=\"red\", label=\"Test Error\")\n",
    "plt.plot(5000, val_losses, color=\"blue\", label=\"Train Error\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE vs. Epochs (LSTM)\")\n",
    "# plt.savefig(\"lstm_train.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=1524, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (lin3): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (lin4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (lin5): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=41027.6523\n",
      "Val Loss=44475.3789\n",
      "-----\n",
      "10\n",
      "Loss=19628.6055\n",
      "Val Loss=14524.2520\n",
      "-----\n",
      "20\n",
      "Loss=5190.3882\n",
      "Val Loss=5981.8555\n",
      "-----\n",
      "30\n",
      "Loss=3884.6340\n",
      "Val Loss=4650.5967\n",
      "-----\n",
      "40\n",
      "Loss=2937.4805\n",
      "Val Loss=3663.1064\n",
      "-----\n",
      "50\n",
      "Loss=2283.9312\n",
      "Val Loss=2945.1121\n",
      "-----\n",
      "60\n",
      "Loss=1808.6974\n",
      "Val Loss=2380.6660\n",
      "-----\n",
      "70\n",
      "Loss=1423.9235\n",
      "Val Loss=1894.7966\n",
      "-----\n",
      "80\n",
      "Loss=1104.2139\n",
      "Val Loss=1475.8422\n",
      "-----\n",
      "90\n",
      "Loss=845.6357\n",
      "Val Loss=1127.8082\n",
      "-----\n",
      "100\n",
      "Loss=668.0111\n",
      "Val Loss=853.1544\n",
      "-----\n",
      "110\n",
      "Loss=2664.2532\n",
      "Val Loss=2561.8818\n",
      "-----\n",
      "120\n",
      "Loss=1752.3610\n",
      "Val Loss=1026.6965\n",
      "-----\n",
      "130\n",
      "Loss=1409.2616\n",
      "Val Loss=909.9985\n",
      "-----\n",
      "140\n",
      "Loss=1422.6881\n",
      "Val Loss=898.7137\n",
      "-----\n",
      "150\n",
      "Loss=1295.6401\n",
      "Val Loss=836.0547\n",
      "-----\n",
      "160\n",
      "Loss=1165.0991\n",
      "Val Loss=776.3022\n",
      "-----\n",
      "170\n",
      "Loss=1068.0223\n",
      "Val Loss=731.0525\n",
      "-----\n",
      "180\n",
      "Loss=994.8151\n",
      "Val Loss=696.2775\n",
      "-----\n",
      "190\n",
      "Loss=935.0551\n",
      "Val Loss=666.6011\n",
      "-----\n",
      "200\n",
      "Loss=881.2934\n",
      "Val Loss=639.4598\n",
      "-----\n",
      "210\n",
      "Loss=833.2753\n",
      "Val Loss=614.9340\n",
      "-----\n",
      "220\n",
      "Loss=789.4785\n",
      "Val Loss=591.8828\n",
      "-----\n",
      "230\n",
      "Loss=750.1949\n",
      "Val Loss=571.3456\n",
      "-----\n",
      "240\n",
      "Loss=716.8950\n",
      "Val Loss=553.7895\n",
      "-----\n",
      "250\n",
      "Loss=687.9847\n",
      "Val Loss=538.2284\n",
      "-----\n",
      "260\n",
      "Loss=661.8625\n",
      "Val Loss=524.0061\n",
      "-----\n",
      "270\n",
      "Loss=638.5239\n",
      "Val Loss=511.3352\n",
      "-----\n",
      "280\n",
      "Loss=618.5578\n",
      "Val Loss=500.3744\n",
      "-----\n",
      "290\n",
      "Loss=601.6212\n",
      "Val Loss=490.9507\n",
      "-----\n",
      "300\n",
      "Loss=586.3743\n",
      "Val Loss=482.3935\n",
      "-----\n",
      "310\n",
      "Loss=572.1767\n",
      "Val Loss=474.3342\n",
      "-----\n",
      "320\n",
      "Loss=558.7354\n",
      "Val Loss=466.9123\n",
      "-----\n",
      "330\n",
      "Loss=546.8245\n",
      "Val Loss=460.3641\n",
      "-----\n",
      "340\n",
      "Loss=536.3555\n",
      "Val Loss=454.5526\n",
      "-----\n",
      "350\n",
      "Loss=527.0900\n",
      "Val Loss=449.3639\n",
      "-----\n",
      "360\n",
      "Loss=518.2013\n",
      "Val Loss=444.2856\n",
      "-----\n",
      "370\n",
      "Loss=509.0738\n",
      "Val Loss=439.0767\n",
      "-----\n",
      "380\n",
      "Loss=500.2628\n",
      "Val Loss=434.2029\n",
      "-----\n",
      "390\n",
      "Loss=492.2884\n",
      "Val Loss=429.8888\n",
      "-----\n",
      "400\n",
      "Loss=485.2322\n",
      "Val Loss=426.1072\n",
      "-----\n",
      "410\n",
      "Loss=479.3778\n",
      "Val Loss=423.0375\n",
      "-----\n",
      "420\n",
      "Loss=474.7814\n",
      "Val Loss=420.6042\n",
      "-----\n",
      "430\n",
      "Loss=470.3506\n",
      "Val Loss=418.1507\n",
      "-----\n",
      "440\n",
      "Loss=465.5855\n",
      "Val Loss=415.5858\n",
      "-----\n",
      "450\n",
      "Loss=461.0707\n",
      "Val Loss=413.2323\n",
      "-----\n",
      "460\n",
      "Loss=457.2335\n",
      "Val Loss=411.1859\n",
      "-----\n",
      "470\n",
      "Loss=453.9427\n",
      "Val Loss=409.4848\n",
      "-----\n",
      "480\n",
      "Loss=450.7362\n",
      "Val Loss=407.7756\n",
      "-----\n",
      "490\n",
      "Loss=447.1195\n",
      "Val Loss=405.8636\n",
      "-----\n",
      "500\n",
      "Loss=443.3921\n",
      "Val Loss=403.9519\n",
      "-----\n",
      "510\n",
      "Loss=439.8271\n",
      "Val Loss=402.0949\n",
      "-----\n",
      "520\n",
      "Loss=436.3694\n",
      "Val Loss=400.2950\n",
      "-----\n",
      "530\n",
      "Loss=433.0855\n",
      "Val Loss=398.6417\n",
      "-----\n",
      "540\n",
      "Loss=430.2396\n",
      "Val Loss=397.2307\n",
      "-----\n",
      "550\n",
      "Loss=427.8740\n",
      "Val Loss=396.0756\n",
      "-----\n",
      "560\n",
      "Loss=425.6602\n",
      "Val Loss=394.9814\n",
      "-----\n",
      "570\n",
      "Loss=423.6073\n",
      "Val Loss=393.9629\n",
      "-----\n",
      "580\n",
      "Loss=421.5263\n",
      "Val Loss=392.9225\n",
      "-----\n",
      "590\n",
      "Loss=419.3449\n",
      "Val Loss=391.8362\n",
      "-----\n",
      "600\n",
      "Loss=417.1271\n",
      "Val Loss=390.7320\n",
      "-----\n",
      "610\n",
      "Loss=414.8470\n",
      "Val Loss=389.6778\n",
      "-----\n",
      "620\n",
      "Loss=412.9722\n",
      "Val Loss=388.8605\n",
      "-----\n",
      "630\n",
      "Loss=411.6259\n",
      "Val Loss=388.2813\n",
      "-----\n",
      "640\n",
      "Loss=410.6024\n",
      "Val Loss=387.8144\n",
      "-----\n",
      "650\n",
      "Loss=409.5107\n",
      "Val Loss=387.2898\n",
      "-----\n",
      "660\n",
      "Loss=408.4341\n",
      "Val Loss=386.7682\n",
      "-----\n",
      "670\n",
      "Loss=407.1808\n",
      "Val Loss=386.1346\n",
      "-----\n",
      "680\n",
      "Loss=405.5112\n",
      "Val Loss=385.3505\n",
      "-----\n",
      "690\n",
      "Loss=403.7843\n",
      "Val Loss=384.5679\n",
      "-----\n",
      "700\n",
      "Loss=401.8995\n",
      "Val Loss=383.7270\n",
      "-----\n",
      "710\n",
      "Loss=400.0931\n",
      "Val Loss=382.9499\n",
      "-----\n",
      "720\n",
      "Loss=398.5410\n",
      "Val Loss=382.2754\n",
      "-----\n",
      "730\n",
      "Loss=397.1604\n",
      "Val Loss=381.6872\n",
      "-----\n",
      "740\n",
      "Loss=396.0072\n",
      "Val Loss=381.2086\n",
      "-----\n",
      "750\n",
      "Loss=394.9499\n",
      "Val Loss=380.7758\n",
      "-----\n",
      "760\n",
      "Loss=393.8837\n",
      "Val Loss=380.3298\n",
      "-----\n",
      "770\n",
      "Loss=392.9988\n",
      "Val Loss=379.9931\n",
      "-----\n",
      "780\n",
      "Loss=392.3021\n",
      "Val Loss=379.7072\n",
      "-----\n",
      "790\n",
      "Loss=391.6337\n",
      "Val Loss=379.4331\n",
      "-----\n",
      "800\n",
      "Loss=391.0512\n",
      "Val Loss=379.2065\n",
      "-----\n",
      "810\n",
      "Loss=390.6094\n",
      "Val Loss=379.0331\n",
      "-----\n",
      "820\n",
      "Loss=390.1341\n",
      "Val Loss=378.8271\n",
      "-----\n",
      "830\n",
      "Loss=389.7141\n",
      "Val Loss=378.6496\n",
      "-----\n",
      "840\n",
      "Loss=389.3597\n",
      "Val Loss=378.5121\n",
      "-----\n",
      "850\n",
      "Loss=389.0148\n",
      "Val Loss=378.3809\n",
      "-----\n",
      "860\n",
      "Loss=388.7044\n",
      "Val Loss=378.2524\n",
      "-----\n",
      "870\n",
      "Loss=388.1448\n",
      "Val Loss=378.0399\n",
      "-----\n",
      "880\n",
      "Loss=387.5909\n",
      "Val Loss=377.8372\n",
      "-----\n",
      "890\n",
      "Loss=386.9473\n",
      "Val Loss=377.5761\n",
      "-----\n",
      "900\n",
      "Loss=386.0861\n",
      "Val Loss=377.2311\n",
      "-----\n",
      "910\n",
      "Loss=385.1528\n",
      "Val Loss=376.8804\n",
      "-----\n",
      "920\n",
      "Loss=384.3118\n",
      "Val Loss=376.5571\n",
      "-----\n",
      "930\n",
      "Loss=383.4470\n",
      "Val Loss=376.2294\n",
      "-----\n",
      "940\n",
      "Loss=382.4967\n",
      "Val Loss=375.8785\n",
      "-----\n",
      "950\n",
      "Loss=381.5636\n",
      "Val Loss=375.5423\n",
      "-----\n",
      "960\n",
      "Loss=380.6923\n",
      "Val Loss=375.2416\n",
      "-----\n",
      "970\n",
      "Loss=380.0219\n",
      "Val Loss=375.0190\n",
      "-----\n",
      "980\n",
      "Loss=379.5339\n",
      "Val Loss=374.8584\n",
      "-----\n",
      "990\n",
      "Loss=379.1027\n",
      "Val Loss=374.7234\n",
      "-----\n",
      "1000\n",
      "Loss=378.7722\n",
      "Val Loss=374.6161\n",
      "-----\n",
      "1010\n",
      "Loss=378.4194\n",
      "Val Loss=374.5001\n",
      "-----\n",
      "1020\n",
      "Loss=377.9910\n",
      "Val Loss=374.3719\n",
      "-----\n",
      "1030\n",
      "Loss=377.5899\n",
      "Val Loss=374.2489\n",
      "-----\n",
      "1040\n",
      "Loss=377.2805\n",
      "Val Loss=374.1543\n",
      "-----\n",
      "1050\n",
      "Loss=377.0218\n",
      "Val Loss=374.0720\n",
      "-----\n",
      "1060\n",
      "Loss=376.6953\n",
      "Val Loss=373.9669\n",
      "-----\n",
      "1070\n",
      "Loss=376.2697\n",
      "Val Loss=373.8282\n",
      "-----\n",
      "1080\n",
      "Loss=375.7050\n",
      "Val Loss=373.6445\n",
      "-----\n",
      "1090\n",
      "Loss=375.0555\n",
      "Val Loss=373.4389\n",
      "-----\n",
      "1100\n",
      "Loss=374.4380\n",
      "Val Loss=373.2507\n",
      "-----\n",
      "1110\n",
      "Loss=373.8974\n",
      "Val Loss=373.1005\n",
      "-----\n",
      "1120\n",
      "Loss=373.5026\n",
      "Val Loss=372.9972\n",
      "-----\n",
      "1130\n",
      "Loss=373.3052\n",
      "Val Loss=372.9559\n",
      "-----\n",
      "1140\n",
      "Loss=373.2034\n",
      "Val Loss=372.9389\n",
      "-----\n",
      "1150\n",
      "Loss=373.1723\n",
      "Val Loss=372.9482\n",
      "-----\n",
      "1160\n",
      "Loss=373.2139\n",
      "Val Loss=372.9756\n",
      "-----\n",
      "1170\n",
      "Loss=373.3182\n",
      "Val Loss=373.0246\n",
      "-----\n",
      "1180\n",
      "Loss=373.3821\n",
      "Val Loss=373.0540\n",
      "-----\n",
      "1190\n",
      "Loss=373.3787\n",
      "Val Loss=373.0588\n",
      "-----\n",
      "1200\n",
      "Loss=373.2795\n",
      "Val Loss=373.0331\n",
      "-----\n",
      "1210\n",
      "Loss=373.0934\n",
      "Val Loss=372.9766\n",
      "-----\n",
      "1220\n",
      "Loss=372.8297\n",
      "Val Loss=372.8972\n",
      "-----\n",
      "1230\n",
      "Loss=372.4591\n",
      "Val Loss=372.7862\n",
      "-----\n",
      "1240\n",
      "Loss=372.0675\n",
      "Val Loss=372.6772\n",
      "-----\n",
      "1250\n",
      "Loss=371.6400\n",
      "Val Loss=372.5562\n",
      "-----\n",
      "1260\n",
      "Loss=371.1778\n",
      "Val Loss=372.4292\n",
      "-----\n",
      "1270\n",
      "Loss=370.6488\n",
      "Val Loss=372.2791\n",
      "-----\n",
      "1280\n",
      "Loss=370.1014\n",
      "Val Loss=372.1280\n",
      "-----\n",
      "1290\n",
      "Loss=369.4736\n",
      "Val Loss=371.9530\n",
      "-----\n",
      "1300\n",
      "Loss=368.8829\n",
      "Val Loss=371.8000\n",
      "-----\n",
      "1310\n",
      "Loss=368.3287\n",
      "Val Loss=371.6645\n",
      "-----\n",
      "1320\n",
      "Loss=367.8703\n",
      "Val Loss=371.5573\n",
      "-----\n",
      "1330\n",
      "Loss=367.5144\n",
      "Val Loss=371.4761\n",
      "-----\n",
      "1340\n",
      "Loss=367.2080\n",
      "Val Loss=371.4060\n",
      "-----\n",
      "1350\n",
      "Loss=366.9601\n",
      "Val Loss=371.3581\n",
      "-----\n",
      "1360\n",
      "Loss=366.8322\n",
      "Val Loss=371.3354\n",
      "-----\n",
      "1370\n",
      "Loss=366.7543\n",
      "Val Loss=371.3168\n",
      "-----\n",
      "1380\n",
      "Loss=366.6423\n",
      "Val Loss=371.2927\n",
      "-----\n",
      "1390\n",
      "Loss=366.5254\n",
      "Val Loss=371.2636\n",
      "-----\n",
      "1400\n",
      "Loss=366.3693\n",
      "Val Loss=371.2285\n",
      "-----\n",
      "1410\n",
      "Loss=366.1578\n",
      "Val Loss=371.1813\n",
      "-----\n",
      "1420\n",
      "Loss=365.8910\n",
      "Val Loss=371.1200\n",
      "-----\n",
      "1430\n",
      "Loss=365.5678\n",
      "Val Loss=371.0498\n",
      "-----\n",
      "1440\n",
      "Loss=365.2571\n",
      "Val Loss=370.9867\n",
      "-----\n",
      "1450\n",
      "Loss=364.9903\n",
      "Val Loss=370.9331\n",
      "-----\n",
      "1460\n",
      "Loss=364.7146\n",
      "Val Loss=370.8779\n",
      "-----\n",
      "1470\n",
      "Loss=364.4487\n",
      "Val Loss=370.8279\n",
      "-----\n",
      "1480\n",
      "Loss=364.2323\n",
      "Val Loss=370.7925\n",
      "-----\n",
      "1490\n",
      "Loss=364.0885\n",
      "Val Loss=370.7658\n",
      "-----\n",
      "1500\n",
      "Loss=363.8935\n",
      "Val Loss=370.7243\n",
      "-----\n",
      "1510\n",
      "Loss=363.5966\n",
      "Val Loss=370.6669\n",
      "-----\n",
      "1520\n",
      "Loss=363.3068\n",
      "Val Loss=370.6105\n",
      "-----\n",
      "1530\n",
      "Loss=363.0395\n",
      "Val Loss=370.5592\n",
      "-----\n",
      "1540\n",
      "Loss=362.7838\n",
      "Val Loss=370.5112\n",
      "-----\n",
      "1550\n",
      "Loss=362.5477\n",
      "Val Loss=370.4659\n",
      "-----\n",
      "1560\n",
      "Loss=362.3467\n",
      "Val Loss=370.4283\n",
      "-----\n",
      "1570\n",
      "Loss=362.1102\n",
      "Val Loss=370.3868\n",
      "-----\n",
      "1580\n",
      "Loss=361.8510\n",
      "Val Loss=370.3448\n",
      "-----\n",
      "1590\n",
      "Loss=361.5745\n",
      "Val Loss=370.3002\n",
      "-----\n",
      "1600\n",
      "Loss=361.2672\n",
      "Val Loss=370.2517\n",
      "-----\n",
      "1610\n",
      "Loss=360.9253\n",
      "Val Loss=370.1985\n",
      "-----\n",
      "1620\n",
      "Loss=360.5858\n",
      "Val Loss=370.1493\n",
      "-----\n",
      "1630\n",
      "Loss=360.2661\n",
      "Val Loss=370.1048\n",
      "-----\n",
      "1640\n",
      "Loss=359.9453\n",
      "Val Loss=370.0619\n",
      "-----\n",
      "1650\n",
      "Loss=359.6086\n",
      "Val Loss=370.0179\n",
      "-----\n",
      "1660\n",
      "Loss=359.2537\n",
      "Val Loss=369.9745\n",
      "-----\n",
      "1670\n",
      "Loss=358.9088\n",
      "Val Loss=369.9346\n",
      "-----\n",
      "1680\n",
      "Loss=358.5381\n",
      "Val Loss=369.8937\n",
      "-----\n",
      "1690\n",
      "Loss=358.1247\n",
      "Val Loss=369.8526\n",
      "-----\n",
      "1700\n",
      "Loss=357.7084\n",
      "Val Loss=369.8173\n",
      "-----\n",
      "1710\n",
      "Loss=357.3122\n",
      "Val Loss=369.7886\n",
      "-----\n",
      "1720\n",
      "Loss=356.9247\n",
      "Val Loss=369.7648\n",
      "-----\n",
      "1730\n",
      "Loss=356.5823\n",
      "Val Loss=369.7484\n",
      "-----\n",
      "1740\n",
      "Loss=356.2874\n",
      "Val Loss=369.7357\n",
      "-----\n",
      "1750\n",
      "Loss=356.0069\n",
      "Val Loss=369.7288\n",
      "-----\n",
      "1760\n",
      "Loss=355.7816\n",
      "Val Loss=369.7259\n",
      "-----\n",
      "1770\n",
      "Loss=355.5822\n",
      "Val Loss=369.7233\n",
      "-----\n",
      "1780\n",
      "Loss=355.4173\n",
      "Val Loss=369.7234\n",
      "-----\n",
      "1790\n",
      "Loss=355.3406\n",
      "Val Loss=369.7240\n",
      "-----\n",
      "1800\n",
      "Loss=355.2633\n",
      "Val Loss=369.7255\n",
      "-----\n",
      "1810\n",
      "Loss=355.2086\n",
      "Val Loss=369.7289\n",
      "-----\n",
      "1820\n",
      "Loss=355.1728\n",
      "Val Loss=369.7310\n",
      "-----\n",
      "1830\n",
      "Loss=355.1822\n",
      "Val Loss=369.7332\n",
      "-----\n",
      "1840\n",
      "Loss=355.1989\n",
      "Val Loss=369.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "1850\n",
      "Loss=355.2239\n",
      "Val Loss=369.7361\n",
      "-----\n",
      "1860\n",
      "Loss=355.2445\n",
      "Val Loss=369.7366\n",
      "-----\n",
      "1870\n",
      "Loss=355.2372\n",
      "Val Loss=369.7373\n",
      "-----\n",
      "1880\n",
      "Loss=355.2397\n",
      "Val Loss=369.7376\n",
      "-----\n",
      "1890\n",
      "Loss=355.2574\n",
      "Val Loss=369.7370\n",
      "-----\n",
      "1900\n",
      "Loss=355.2547\n",
      "Val Loss=369.7366\n",
      "-----\n",
      "1910\n",
      "Loss=355.2372\n",
      "Val Loss=369.7352\n",
      "-----\n",
      "1920\n",
      "Loss=355.2008\n",
      "Val Loss=369.7334\n",
      "-----\n",
      "1930\n",
      "Loss=355.1436\n",
      "Val Loss=369.7316\n",
      "-----\n",
      "1940\n",
      "Loss=355.0762\n",
      "Val Loss=369.7285\n",
      "-----\n",
      "1950\n",
      "Loss=354.9994\n",
      "Val Loss=369.7243\n",
      "-----\n",
      "1960\n",
      "Loss=354.9267\n",
      "Val Loss=369.7209\n",
      "-----\n",
      "1970\n",
      "Loss=354.8502\n",
      "Val Loss=369.7162\n",
      "-----\n",
      "1980\n",
      "Loss=354.7498\n",
      "Val Loss=369.7127\n",
      "-----\n",
      "1990\n",
      "Loss=354.6483\n",
      "Val Loss=369.7100\n",
      "-----\n",
      "2000\n",
      "Loss=354.5378\n",
      "Val Loss=369.7078\n",
      "-----\n",
      "2010\n",
      "Loss=354.4214\n",
      "Val Loss=369.7071\n",
      "-----\n",
      "2020\n",
      "Loss=354.2861\n",
      "Val Loss=369.7054\n",
      "-----\n",
      "2030\n",
      "Loss=354.1297\n",
      "Val Loss=369.7048\n",
      "-----\n",
      "2040\n",
      "Loss=353.9742\n",
      "Val Loss=369.7064\n",
      "-----\n",
      "2050\n",
      "Loss=353.8115\n",
      "Val Loss=369.7094\n",
      "-----\n",
      "2060\n",
      "Loss=353.6440\n",
      "Val Loss=369.7136\n",
      "-----\n",
      "2070\n",
      "Loss=353.4841\n",
      "Val Loss=369.7190\n",
      "-----\n",
      "2080\n",
      "Loss=353.3255\n",
      "Val Loss=369.7248\n",
      "-----\n",
      "2090\n",
      "Loss=353.1472\n",
      "Val Loss=369.7316\n",
      "-----\n",
      "2100\n",
      "Loss=352.9515\n",
      "Val Loss=369.7414\n",
      "-----\n",
      "2110\n",
      "Loss=352.7497\n",
      "Val Loss=369.7513\n",
      "-----\n",
      "2120\n",
      "Loss=352.5502\n",
      "Val Loss=369.7636\n",
      "-----\n",
      "2130\n",
      "Loss=352.3574\n",
      "Val Loss=369.7767\n",
      "-----\n",
      "2140\n",
      "Loss=352.1668\n",
      "Val Loss=369.7909\n",
      "-----\n",
      "2150\n",
      "Loss=351.9791\n",
      "Val Loss=369.8053\n",
      "-----\n",
      "2160\n",
      "Loss=351.8053\n",
      "Val Loss=369.8212\n",
      "-----\n",
      "2170\n",
      "Loss=351.6364\n",
      "Val Loss=369.8368\n",
      "-----\n",
      "2180\n",
      "Loss=351.4880\n",
      "Val Loss=369.8510\n",
      "-----\n",
      "2190\n",
      "Loss=351.3626\n",
      "Val Loss=369.8637\n",
      "-----\n",
      "2200\n",
      "Loss=351.2527\n",
      "Val Loss=369.8757\n",
      "-----\n",
      "2210\n",
      "Loss=351.1515\n",
      "Val Loss=369.8864\n",
      "-----\n",
      "2220\n",
      "Loss=351.0623\n",
      "Val Loss=369.8948\n",
      "-----\n",
      "2230\n",
      "Loss=350.9798\n",
      "Val Loss=369.9031\n",
      "-----\n",
      "2240\n",
      "Loss=350.8924\n",
      "Val Loss=369.9121\n",
      "-----\n",
      "2250\n",
      "Loss=350.8288\n",
      "Val Loss=369.9178\n",
      "-----\n",
      "2260\n",
      "Loss=350.7675\n",
      "Val Loss=369.9233\n",
      "-----\n",
      "2270\n",
      "Loss=350.6992\n",
      "Val Loss=369.9289\n",
      "-----\n",
      "2280\n",
      "Loss=350.6548\n",
      "Val Loss=369.9310\n",
      "-----\n",
      "2290\n",
      "Loss=350.6333\n",
      "Val Loss=369.9298\n",
      "-----\n",
      "2300\n",
      "Loss=350.6281\n",
      "Val Loss=369.9260\n",
      "-----\n",
      "2310\n",
      "Loss=350.6544\n",
      "Val Loss=369.9178\n",
      "-----\n",
      "2320\n",
      "Loss=350.7238\n",
      "Val Loss=369.9016\n",
      "-----\n",
      "2330\n",
      "Loss=350.8074\n",
      "Val Loss=369.8849\n",
      "-----\n",
      "2340\n",
      "Loss=350.9171\n",
      "Val Loss=369.8635\n",
      "-----\n",
      "2350\n",
      "Loss=351.0452\n",
      "Val Loss=369.8406\n",
      "-----\n",
      "2360\n",
      "Loss=351.1676\n",
      "Val Loss=369.8215\n",
      "-----\n",
      "2370\n",
      "Loss=351.2864\n",
      "Val Loss=369.8040\n",
      "-----\n",
      "2380\n",
      "Loss=351.4134\n",
      "Val Loss=369.7852\n",
      "-----\n",
      "2390\n",
      "Loss=351.5594\n",
      "Val Loss=369.7655\n",
      "-----\n",
      "2400\n",
      "Loss=351.7133\n",
      "Val Loss=369.7462\n",
      "-----\n",
      "2410\n",
      "Loss=351.8643\n",
      "Val Loss=369.7282\n",
      "-----\n",
      "2420\n",
      "Loss=351.9819\n",
      "Val Loss=369.7143\n",
      "-----\n",
      "2430\n",
      "Loss=352.1021\n",
      "Val Loss=369.7021\n",
      "-----\n",
      "2440\n",
      "Loss=352.2165\n",
      "Val Loss=369.6919\n",
      "-----\n",
      "2450\n",
      "Loss=352.3587\n",
      "Val Loss=369.6822\n",
      "-----\n",
      "2460\n",
      "Loss=352.4847\n",
      "Val Loss=369.6761\n",
      "-----\n",
      "2470\n",
      "Loss=352.5846\n",
      "Val Loss=369.6719\n",
      "-----\n",
      "2480\n",
      "Loss=352.6701\n",
      "Val Loss=369.6684\n",
      "-----\n",
      "2490\n",
      "Loss=352.7439\n",
      "Val Loss=369.6644\n",
      "-----\n",
      "2500\n",
      "Loss=352.8189\n",
      "Val Loss=369.6613\n",
      "-----\n",
      "2510\n",
      "Loss=352.8761\n",
      "Val Loss=369.6591\n",
      "-----\n",
      "2520\n",
      "Loss=352.9119\n",
      "Val Loss=369.6564\n",
      "-----\n",
      "2530\n",
      "Loss=352.9326\n",
      "Val Loss=369.6547\n",
      "-----\n",
      "2540\n",
      "Loss=352.9137\n",
      "Val Loss=369.6527\n",
      "-----\n",
      "2550\n",
      "Loss=352.8739\n",
      "Val Loss=369.6520\n",
      "-----\n",
      "2560\n",
      "Loss=352.8158\n",
      "Val Loss=369.6516\n",
      "-----\n",
      "2570\n",
      "Loss=352.7451\n",
      "Val Loss=369.6501\n",
      "-----\n",
      "2580\n",
      "Loss=352.6299\n",
      "Val Loss=369.6515\n",
      "-----\n",
      "2590\n",
      "Loss=352.5033\n",
      "Val Loss=369.6538\n",
      "-----\n",
      "2600\n",
      "Loss=352.3817\n",
      "Val Loss=369.6569\n",
      "-----\n",
      "2610\n",
      "Loss=352.2441\n",
      "Val Loss=369.6610\n",
      "-----\n",
      "2620\n",
      "Loss=352.0907\n",
      "Val Loss=369.6659\n",
      "-----\n",
      "2630\n",
      "Loss=351.9406\n",
      "Val Loss=369.6718\n",
      "-----\n",
      "2640\n",
      "Loss=351.7990\n",
      "Val Loss=369.6767\n",
      "-----\n",
      "2650\n",
      "Loss=351.6593\n",
      "Val Loss=369.6822\n",
      "-----\n",
      "2660\n",
      "Loss=351.5287\n",
      "Val Loss=369.6873\n",
      "-----\n",
      "2670\n",
      "Loss=351.4119\n",
      "Val Loss=369.6918\n",
      "-----\n",
      "2680\n",
      "Loss=351.3034\n",
      "Val Loss=369.6969\n",
      "-----\n",
      "2690\n",
      "Loss=351.2129\n",
      "Val Loss=369.6989\n",
      "-----\n",
      "2700\n",
      "Loss=351.1273\n",
      "Val Loss=369.7017\n",
      "-----\n",
      "2710\n",
      "Loss=351.0497\n",
      "Val Loss=369.7038\n",
      "-----\n",
      "2720\n",
      "Loss=350.9607\n",
      "Val Loss=369.7086\n",
      "-----\n",
      "2730\n",
      "Loss=350.8698\n",
      "Val Loss=369.7137\n",
      "-----\n",
      "2740\n",
      "Loss=350.7929\n",
      "Val Loss=369.7169\n",
      "-----\n",
      "2750\n",
      "Loss=350.7229\n",
      "Val Loss=369.7187\n",
      "-----\n",
      "2760\n",
      "Loss=350.6570\n",
      "Val Loss=369.7209\n",
      "-----\n",
      "2770\n",
      "Loss=350.5869\n",
      "Val Loss=369.7234\n",
      "-----\n",
      "2780\n",
      "Loss=350.5010\n",
      "Val Loss=369.7283\n",
      "-----\n",
      "2790\n",
      "Loss=350.4088\n",
      "Val Loss=369.7345\n",
      "-----\n",
      "2800\n",
      "Loss=350.3210\n",
      "Val Loss=369.7399\n",
      "-----\n",
      "2810\n",
      "Loss=350.2354\n",
      "Val Loss=369.7452\n",
      "-----\n",
      "2820\n",
      "Loss=350.1510\n",
      "Val Loss=369.7514\n",
      "-----\n",
      "2830\n",
      "Loss=350.0747\n",
      "Val Loss=369.7566\n",
      "-----\n",
      "2840\n",
      "Loss=350.0057\n",
      "Val Loss=369.7607\n",
      "-----\n",
      "2850\n",
      "Loss=349.9388\n",
      "Val Loss=369.7655\n",
      "-----\n",
      "2860\n",
      "Loss=349.8759\n",
      "Val Loss=369.7703\n",
      "-----\n",
      "2870\n",
      "Loss=349.8054\n",
      "Val Loss=369.7777\n",
      "-----\n",
      "2880\n",
      "Loss=349.7402\n",
      "Val Loss=369.7840\n",
      "-----\n",
      "2890\n",
      "Loss=349.6737\n",
      "Val Loss=369.7906\n",
      "-----\n",
      "2900\n",
      "Loss=349.6001\n",
      "Val Loss=369.7989\n",
      "-----\n",
      "2910\n",
      "Loss=349.5209\n",
      "Val Loss=369.8076\n",
      "-----\n",
      "2920\n",
      "Loss=349.4385\n",
      "Val Loss=369.8174\n",
      "-----\n",
      "2930\n",
      "Loss=349.3583\n",
      "Val Loss=369.8262\n",
      "-----\n",
      "2940\n",
      "Loss=349.2882\n",
      "Val Loss=369.8338\n",
      "-----\n",
      "2950\n",
      "Loss=349.2265\n",
      "Val Loss=369.8395\n",
      "-----\n",
      "2960\n",
      "Loss=349.1740\n",
      "Val Loss=369.8442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4f3b4f220523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# input x and predict based on x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# must be (1. nn output, 2. target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-4f3b4f220523>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# perform dropout on input vector embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        self.lin1 = nn.Linear(n_feature, 500)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        self.lin2 = nn.Linear(500, 200)\n",
    "        self.lin3 = nn.Linear(200, 50)\n",
    "        self.lin4 = nn.Linear(50, 10)\n",
    "        self.lin5 = nn.Linear(10, n_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = self.lin5(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try temporal convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-2013.pkl\n",
      "2013-2014.pkl\n",
      "(1265, 3, 508)\n",
      "(1272, 3, 508)\n",
      "2014-2015.pkl\n",
      "(2537, 3, 508)\n",
      "(1263, 3, 508)\n",
      "2015-2016.pkl\n",
      "(3800, 3, 508)\n",
      "(1269, 3, 508)\n",
      "2016-2017.pkl\n",
      "2017-2018.pkl\n"
     ]
    }
   ],
   "source": [
    "game_data_path = \"data/neural_net_data/\"\n",
    "files = sorted(os.listdir(game_data_path))\n",
    "\n",
    "files = [\"2012-2013.pkl\", \"2013-2014.pkl\", \"2014-2015.pkl\", \"2015-2016.pkl\",\"2016-2017.pkl\", \"2017-2018.pkl\"]\n",
    "\n",
    "X_train = np.zeros(5)\n",
    "\n",
    "for file in files[:-2]:\n",
    "    if \".pkl\" not in file: continue\n",
    "    \n",
    "    with open(game_data_path + file, 'rb') as f:\n",
    "        print(file)\n",
    "        if X_train.shape[0] == 5:\n",
    "            X_train, y_train = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            X_add, y_add = pickle.load(f, encoding='latin1')\n",
    "            print(X_train.shape)\n",
    "            print(X_add.shape)\n",
    "            X_train = np.concatenate((X_train, X_add), axis = 0)\n",
    "            y_train = np.concatenate((y_train, y_add), axis = 0)\n",
    "\n",
    "with open(game_data_path + files[-2], 'rb') as f:\n",
    "        print(files[-2])\n",
    "        X_val, y_val = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "with open(game_data_path + files[-1], 'rb') as f:\n",
    "        print(files[-1])\n",
    "        X_test, y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X, y):\n",
    "    X = X[y > 0]\n",
    "    y = y[y > 0]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_data(X_train, y_train)\n",
    "X_val, y_val = clean_data(X_val, y_val)\n",
    "X_test, y_test = clean_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(y[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y_val = torch.from_numpy(y_val[:,np.newaxis]).type(torch.FloatTensor)\n",
    "X_val = torch.from_numpy(X_val).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4961, 3, 508])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(3, 150, kernel_size=(508,), stride=(1,))\n",
      "  (lin1): Linear(in_features=150, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n",
      "-----\n",
      "0\n",
      "Loss=37214.9180\n",
      "Val Loss=11678.8203\n",
      "-----\n",
      "10\n",
      "Loss=4439.6040\n",
      "Val Loss=8365.9746\n",
      "-----\n",
      "20\n",
      "Loss=3227.7144\n",
      "Val Loss=6480.4014\n",
      "-----\n",
      "30\n",
      "Loss=2631.0728\n",
      "Val Loss=5450.3979\n",
      "-----\n",
      "40\n",
      "Loss=2298.3240\n",
      "Val Loss=4819.7783\n",
      "-----\n",
      "50\n",
      "Loss=2058.1562\n",
      "Val Loss=4295.7886\n",
      "-----\n",
      "60\n",
      "Loss=1877.6304\n",
      "Val Loss=3861.4360\n",
      "-----\n",
      "70\n",
      "Loss=1733.9609\n",
      "Val Loss=3501.6238\n",
      "-----\n",
      "80\n",
      "Loss=1609.5328\n",
      "Val Loss=3177.9939\n",
      "-----\n",
      "90\n",
      "Loss=1510.3425\n",
      "Val Loss=2921.5898\n",
      "-----\n",
      "100\n",
      "Loss=1421.0330\n",
      "Val Loss=2689.3794\n",
      "-----\n",
      "110\n",
      "Loss=1347.0741\n",
      "Val Loss=2499.1250\n",
      "-----\n",
      "120\n",
      "Loss=1283.6223\n",
      "Val Loss=2336.9709\n",
      "-----\n",
      "130\n",
      "Loss=1229.1184\n",
      "Val Loss=2201.0745\n",
      "-----\n",
      "140\n",
      "Loss=1179.6904\n",
      "Val Loss=2078.8281\n",
      "-----\n",
      "150\n",
      "Loss=1134.8365\n",
      "Val Loss=1969.3644\n",
      "-----\n",
      "160\n",
      "Loss=1095.9230\n",
      "Val Loss=1876.0896\n",
      "-----\n",
      "170\n",
      "Loss=1062.7239\n",
      "Val Loss=1796.6823\n",
      "-----\n",
      "180\n",
      "Loss=1030.9600\n",
      "Val Loss=1723.9149\n",
      "-----\n",
      "190\n",
      "Loss=1003.6530\n",
      "Val Loss=1660.4553\n",
      "-----\n",
      "200\n",
      "Loss=976.9434\n",
      "Val Loss=1600.5480\n",
      "-----\n",
      "210\n",
      "Loss=949.2987\n",
      "Val Loss=1539.8958\n",
      "-----\n",
      "220\n",
      "Loss=926.7083\n",
      "Val Loss=1489.9401\n",
      "-----\n",
      "230\n",
      "Loss=903.9307\n",
      "Val Loss=1440.7554\n",
      "-----\n",
      "240\n",
      "Loss=884.9888\n",
      "Val Loss=1400.6012\n",
      "-----\n",
      "250\n",
      "Loss=865.4747\n",
      "Val Loss=1360.0295\n",
      "-----\n",
      "260\n",
      "Loss=849.1942\n",
      "Val Loss=1324.6930\n",
      "-----\n",
      "270\n",
      "Loss=833.7883\n",
      "Val Loss=1292.4664\n",
      "-----\n",
      "280\n",
      "Loss=819.9721\n",
      "Val Loss=1263.7911\n",
      "-----\n",
      "290\n",
      "Loss=806.4125\n",
      "Val Loss=1235.8018\n",
      "-----\n",
      "300\n",
      "Loss=796.3281\n",
      "Val Loss=1214.8419\n",
      "-----\n",
      "310\n",
      "Loss=784.5526\n",
      "Val Loss=1191.0704\n",
      "-----\n",
      "320\n",
      "Loss=772.0239\n",
      "Val Loss=1165.8058\n",
      "-----\n",
      "330\n",
      "Loss=759.2771\n",
      "Val Loss=1141.5737\n",
      "-----\n",
      "340\n",
      "Loss=748.7048\n",
      "Val Loss=1120.8876\n",
      "-----\n",
      "350\n",
      "Loss=738.8671\n",
      "Val Loss=1101.1484\n",
      "-----\n",
      "360\n",
      "Loss=729.0328\n",
      "Val Loss=1081.7258\n",
      "-----\n",
      "370\n",
      "Loss=720.9176\n",
      "Val Loss=1065.9092\n",
      "-----\n",
      "380\n",
      "Loss=712.2313\n",
      "Val Loss=1048.7228\n",
      "-----\n",
      "390\n",
      "Loss=702.5466\n",
      "Val Loss=1029.8981\n",
      "-----\n",
      "400\n",
      "Loss=693.3375\n",
      "Val Loss=1012.6205\n",
      "-----\n",
      "410\n",
      "Loss=687.8408\n",
      "Val Loss=1002.4088\n",
      "-----\n",
      "420\n",
      "Loss=682.3202\n",
      "Val Loss=991.8527\n",
      "-----\n",
      "430\n",
      "Loss=676.5042\n",
      "Val Loss=980.8326\n",
      "-----\n",
      "440\n",
      "Loss=669.9916\n",
      "Val Loss=968.4158\n",
      "-----\n",
      "450\n",
      "Loss=663.2697\n",
      "Val Loss=955.3401\n",
      "-----\n",
      "460\n",
      "Loss=655.1808\n",
      "Val Loss=940.3152\n",
      "-----\n",
      "470\n",
      "Loss=649.0825\n",
      "Val Loss=928.9310\n",
      "-----\n",
      "480\n",
      "Loss=643.6735\n",
      "Val Loss=918.6403\n",
      "-----\n",
      "490\n",
      "Loss=639.3490\n",
      "Val Loss=910.5750\n",
      "-----\n",
      "500\n",
      "Loss=635.5902\n",
      "Val Loss=903.5812\n",
      "-----\n",
      "510\n",
      "Loss=630.6599\n",
      "Val Loss=894.4114\n",
      "-----\n",
      "520\n",
      "Loss=624.5912\n",
      "Val Loss=883.1260\n",
      "-----\n",
      "530\n",
      "Loss=619.6668\n",
      "Val Loss=874.0967\n",
      "-----\n",
      "540\n",
      "Loss=615.8449\n",
      "Val Loss=867.1162\n",
      "-----\n",
      "550\n",
      "Loss=610.8146\n",
      "Val Loss=857.8779\n",
      "-----\n",
      "560\n",
      "Loss=606.7305\n",
      "Val Loss=850.4077\n",
      "-----\n",
      "570\n",
      "Loss=602.4583\n",
      "Val Loss=842.4393\n",
      "-----\n",
      "580\n",
      "Loss=598.1868\n",
      "Val Loss=834.2847\n",
      "-----\n",
      "590\n",
      "Loss=594.0889\n",
      "Val Loss=826.4297\n",
      "-----\n",
      "600\n",
      "Loss=590.0428\n",
      "Val Loss=819.2185\n",
      "-----\n",
      "610\n",
      "Loss=586.8308\n",
      "Val Loss=813.3838\n",
      "-----\n",
      "620\n",
      "Loss=582.8332\n",
      "Val Loss=806.2222\n",
      "-----\n",
      "630\n",
      "Loss=579.2637\n",
      "Val Loss=800.2919\n",
      "-----\n",
      "640\n",
      "Loss=577.4189\n",
      "Val Loss=797.1850\n",
      "-----\n",
      "650\n",
      "Loss=574.1945\n",
      "Val Loss=791.0481\n",
      "-----\n",
      "660\n",
      "Loss=571.3399\n",
      "Val Loss=786.1403\n",
      "-----\n",
      "670\n",
      "Loss=568.4484\n",
      "Val Loss=780.9476\n",
      "-----\n",
      "680\n",
      "Loss=564.4854\n",
      "Val Loss=773.6875\n",
      "-----\n",
      "690\n",
      "Loss=560.7700\n",
      "Val Loss=767.1655\n",
      "-----\n",
      "700\n",
      "Loss=558.3972\n",
      "Val Loss=762.7532\n",
      "-----\n",
      "710\n",
      "Loss=557.4654\n",
      "Val Loss=761.2307\n",
      "-----\n",
      "720\n",
      "Loss=554.7477\n",
      "Val Loss=756.1486\n",
      "-----\n",
      "730\n",
      "Loss=551.3052\n",
      "Val Loss=750.0432\n",
      "-----\n",
      "740\n",
      "Loss=548.5012\n",
      "Val Loss=745.1281\n",
      "-----\n",
      "750\n",
      "Loss=545.3489\n",
      "Val Loss=739.2817\n",
      "-----\n",
      "760\n",
      "Loss=542.5070\n",
      "Val Loss=734.1625\n",
      "-----\n",
      "770\n",
      "Loss=539.5599\n",
      "Val Loss=728.9493\n",
      "-----\n",
      "780\n",
      "Loss=537.1351\n",
      "Val Loss=724.5828\n",
      "-----\n",
      "790\n",
      "Loss=534.9224\n",
      "Val Loss=720.5742\n",
      "-----\n",
      "800\n",
      "Loss=532.9999\n",
      "Val Loss=717.2476\n",
      "-----\n",
      "810\n",
      "Loss=531.5414\n",
      "Val Loss=714.5275\n",
      "-----\n",
      "820\n",
      "Loss=528.9634\n",
      "Val Loss=709.6443\n",
      "-----\n",
      "830\n",
      "Loss=525.8100\n",
      "Val Loss=704.0742\n",
      "-----\n",
      "840\n",
      "Loss=523.3276\n",
      "Val Loss=699.9332\n",
      "-----\n",
      "850\n",
      "Loss=521.4334\n",
      "Val Loss=696.4456\n",
      "-----\n",
      "860\n",
      "Loss=519.6865\n",
      "Val Loss=693.2798\n",
      "-----\n",
      "870\n",
      "Loss=519.5785\n",
      "Val Loss=693.2112\n",
      "-----\n",
      "880\n",
      "Loss=518.6450\n",
      "Val Loss=691.4494\n",
      "-----\n",
      "890\n",
      "Loss=516.6452\n",
      "Val Loss=687.8769\n",
      "-----\n",
      "900\n",
      "Loss=514.9730\n",
      "Val Loss=684.9008\n",
      "-----\n",
      "910\n",
      "Loss=513.4128\n",
      "Val Loss=682.0064\n",
      "-----\n",
      "920\n",
      "Loss=511.8421\n",
      "Val Loss=678.9887\n",
      "-----\n",
      "930\n",
      "Loss=510.0024\n",
      "Val Loss=675.6658\n",
      "-----\n",
      "940\n",
      "Loss=508.5426\n",
      "Val Loss=673.1903\n",
      "-----\n",
      "950\n",
      "Loss=506.3380\n",
      "Val Loss=669.3002\n",
      "-----\n",
      "960\n",
      "Loss=503.9799\n",
      "Val Loss=665.4020\n",
      "-----\n",
      "970\n",
      "Loss=502.4694\n",
      "Val Loss=662.9880\n",
      "-----\n",
      "980\n",
      "Loss=500.3759\n",
      "Val Loss=659.3022\n",
      "-----\n",
      "990\n",
      "Loss=498.0872\n",
      "Val Loss=655.3798\n",
      "-----\n",
      "1000\n",
      "Loss=496.0952\n",
      "Val Loss=652.0023\n",
      "-----\n",
      "1010\n",
      "Loss=494.8443\n",
      "Val Loss=649.8277\n",
      "-----\n",
      "1020\n",
      "Loss=493.3911\n",
      "Val Loss=647.2343\n",
      "-----\n",
      "1030\n",
      "Loss=491.3959\n",
      "Val Loss=643.7149\n",
      "-----\n",
      "1040\n",
      "Loss=490.4979\n",
      "Val Loss=642.3256\n",
      "-----\n",
      "1050\n",
      "Loss=490.8204\n",
      "Val Loss=643.1743\n",
      "-----\n",
      "1060\n",
      "Loss=490.5989\n",
      "Val Loss=642.6835\n",
      "-----\n",
      "1070\n",
      "Loss=489.2195\n",
      "Val Loss=640.3359\n",
      "-----\n",
      "1080\n",
      "Loss=487.4916\n",
      "Val Loss=637.3664\n",
      "-----\n",
      "1090\n",
      "Loss=486.4473\n",
      "Val Loss=635.6787\n",
      "-----\n",
      "1100\n",
      "Loss=486.3494\n",
      "Val Loss=635.5840\n",
      "-----\n",
      "1110\n",
      "Loss=486.6310\n",
      "Val Loss=636.1688\n",
      "-----\n",
      "1120\n",
      "Loss=486.5414\n",
      "Val Loss=635.8024\n",
      "-----\n",
      "1130\n",
      "Loss=485.2492\n",
      "Val Loss=633.4619\n",
      "-----\n",
      "1140\n",
      "Loss=483.1486\n",
      "Val Loss=629.9727\n",
      "-----\n",
      "1150\n",
      "Loss=480.9319\n",
      "Val Loss=626.1780\n",
      "-----\n",
      "1160\n",
      "Loss=478.7054\n",
      "Val Loss=622.4360\n",
      "-----\n",
      "1170\n",
      "Loss=477.1334\n",
      "Val Loss=619.8310\n",
      "-----\n",
      "1180\n",
      "Loss=475.3848\n",
      "Val Loss=616.7738\n",
      "-----\n",
      "1190\n",
      "Loss=474.3359\n",
      "Val Loss=615.0040\n",
      "-----\n",
      "1200\n",
      "Loss=473.3479\n",
      "Val Loss=613.3459\n",
      "-----\n",
      "1210\n",
      "Loss=473.0070\n",
      "Val Loss=612.8408\n",
      "-----\n",
      "1220\n",
      "Loss=472.3557\n",
      "Val Loss=611.7412\n",
      "-----\n",
      "1230\n",
      "Loss=470.7969\n",
      "Val Loss=608.9873\n",
      "-----\n",
      "1240\n",
      "Loss=468.8378\n",
      "Val Loss=605.5997\n",
      "-----\n",
      "1250\n",
      "Loss=466.8442\n",
      "Val Loss=602.1934\n",
      "-----\n",
      "1260\n",
      "Loss=465.1377\n",
      "Val Loss=599.3165\n",
      "-----\n",
      "1270\n",
      "Loss=464.0964\n",
      "Val Loss=597.6100\n",
      "-----\n",
      "1280\n",
      "Loss=463.5728\n",
      "Val Loss=597.0247\n",
      "-----\n",
      "1290\n",
      "Loss=462.9433\n",
      "Val Loss=595.9099\n",
      "-----\n",
      "1300\n",
      "Loss=461.4348\n",
      "Val Loss=593.1906\n",
      "-----\n",
      "1310\n",
      "Loss=459.3829\n",
      "Val Loss=589.5906\n",
      "-----\n",
      "1320\n",
      "Loss=457.8806\n",
      "Val Loss=587.2663\n",
      "-----\n",
      "1330\n",
      "Loss=458.0350\n",
      "Val Loss=587.7161\n",
      "-----\n",
      "1340\n",
      "Loss=458.6534\n",
      "Val Loss=588.6537\n",
      "-----\n",
      "1350\n",
      "Loss=458.2142\n",
      "Val Loss=587.6523\n",
      "-----\n",
      "1360\n",
      "Loss=457.2198\n",
      "Val Loss=585.8610\n",
      "-----\n",
      "1370\n",
      "Loss=455.6659\n",
      "Val Loss=583.1801\n",
      "-----\n",
      "1380\n",
      "Loss=454.4526\n",
      "Val Loss=581.2060\n",
      "-----\n",
      "1390\n",
      "Loss=453.1277\n",
      "Val Loss=578.9072\n",
      "-----\n",
      "1400\n",
      "Loss=451.4760\n",
      "Val Loss=576.0002\n",
      "-----\n",
      "1410\n",
      "Loss=449.3766\n",
      "Val Loss=572.4593\n",
      "-----\n",
      "1420\n",
      "Loss=447.5128\n",
      "Val Loss=569.2881\n",
      "-----\n",
      "1430\n",
      "Loss=446.1631\n",
      "Val Loss=567.1069\n",
      "-----\n",
      "1440\n",
      "Loss=445.7801\n",
      "Val Loss=566.5993\n",
      "-----\n",
      "1450\n",
      "Loss=445.5822\n",
      "Val Loss=566.3079\n",
      "-----\n",
      "1460\n",
      "Loss=445.6886\n",
      "Val Loss=566.5185\n",
      "-----\n",
      "1470\n",
      "Loss=445.8267\n",
      "Val Loss=566.7137\n",
      "-----\n",
      "1480\n",
      "Loss=445.5616\n",
      "Val Loss=566.3458\n",
      "-----\n",
      "1490\n",
      "Loss=445.5392\n",
      "Val Loss=566.1801\n",
      "-----\n",
      "1500\n",
      "Loss=444.9024\n",
      "Val Loss=565.0018\n",
      "-----\n",
      "1510\n",
      "Loss=443.8602\n",
      "Val Loss=563.1945\n",
      "-----\n",
      "1520\n",
      "Loss=442.5397\n",
      "Val Loss=561.0814\n",
      "-----\n",
      "1530\n",
      "Loss=441.2430\n",
      "Val Loss=558.9087\n",
      "-----\n",
      "1540\n",
      "Loss=440.2883\n",
      "Val Loss=557.3822\n",
      "-----\n",
      "1550\n",
      "Loss=439.9480\n",
      "Val Loss=557.0184\n",
      "-----\n",
      "1560\n",
      "Loss=440.0870\n",
      "Val Loss=557.3373\n",
      "-----\n",
      "1570\n",
      "Loss=440.4903\n",
      "Val Loss=558.0865\n",
      "-----\n",
      "1580\n",
      "Loss=440.5261\n",
      "Val Loss=558.1151\n",
      "-----\n",
      "1590\n",
      "Loss=439.7771\n",
      "Val Loss=556.6996\n",
      "-----\n",
      "1600\n",
      "Loss=438.8680\n",
      "Val Loss=555.2144\n",
      "-----\n",
      "1610\n",
      "Loss=437.5971\n",
      "Val Loss=553.0362\n",
      "-----\n",
      "1620\n",
      "Loss=436.2617\n",
      "Val Loss=550.8239\n",
      "-----\n",
      "1630\n",
      "Loss=434.6646\n",
      "Val Loss=548.1206\n",
      "-----\n",
      "1640\n",
      "Loss=433.1168\n",
      "Val Loss=545.5990\n",
      "-----\n",
      "1650\n",
      "Loss=432.3154\n",
      "Val Loss=544.4453\n",
      "-----\n",
      "1660\n",
      "Loss=432.2586\n",
      "Val Loss=544.5120\n",
      "-----\n",
      "1670\n",
      "Loss=432.5399\n",
      "Val Loss=544.9977\n",
      "-----\n",
      "1680\n",
      "Loss=432.8149\n",
      "Val Loss=545.4158\n",
      "-----\n",
      "1690\n",
      "Loss=433.1165\n",
      "Val Loss=545.9902\n",
      "-----\n",
      "1700\n",
      "Loss=433.2820\n",
      "Val Loss=546.2822\n",
      "-----\n",
      "1710\n",
      "Loss=432.8697\n",
      "Val Loss=545.6176\n",
      "-----\n",
      "1720\n",
      "Loss=431.8980\n",
      "Val Loss=544.0166\n",
      "-----\n",
      "1730\n",
      "Loss=430.9070\n",
      "Val Loss=542.3564\n",
      "-----\n",
      "1740\n",
      "Loss=430.3392\n",
      "Val Loss=541.6487\n",
      "-----\n",
      "1750\n",
      "Loss=430.5728\n",
      "Val Loss=542.1805\n",
      "-----\n",
      "1760\n",
      "Loss=430.7856\n",
      "Val Loss=542.5525\n",
      "-----\n",
      "1770\n",
      "Loss=430.3496\n",
      "Val Loss=541.7507\n",
      "-----\n",
      "1780\n",
      "Loss=429.5275\n",
      "Val Loss=540.3782\n",
      "-----\n",
      "1790\n",
      "Loss=428.8819\n",
      "Val Loss=539.3908\n",
      "-----\n",
      "1800\n",
      "Loss=428.1837\n",
      "Val Loss=538.1790\n",
      "-----\n",
      "1810\n",
      "Loss=427.3606\n",
      "Val Loss=536.8232\n",
      "-----\n",
      "1820\n",
      "Loss=426.8128\n",
      "Val Loss=535.9921\n",
      "-----\n",
      "1830\n",
      "Loss=426.4762\n",
      "Val Loss=535.5234\n",
      "-----\n",
      "1840\n",
      "Loss=425.9612\n",
      "Val Loss=534.7208\n",
      "-----\n",
      "1850\n",
      "Loss=425.3188\n",
      "Val Loss=533.6682\n",
      "-----\n",
      "1860\n",
      "Loss=424.6636\n",
      "Val Loss=532.6533\n",
      "-----\n",
      "1870\n",
      "Loss=424.2159\n",
      "Val Loss=531.9048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "1880\n",
      "Loss=423.5368\n",
      "Val Loss=530.7604\n",
      "-----\n",
      "1890\n",
      "Loss=422.7183\n",
      "Val Loss=529.4564\n",
      "-----\n",
      "1900\n",
      "Loss=421.9995\n",
      "Val Loss=528.2945\n",
      "-----\n",
      "1910\n",
      "Loss=421.6255\n",
      "Val Loss=527.7038\n",
      "-----\n",
      "1920\n",
      "Loss=421.3592\n",
      "Val Loss=527.2621\n",
      "-----\n",
      "1930\n",
      "Loss=421.1794\n",
      "Val Loss=527.0029\n",
      "-----\n",
      "1940\n",
      "Loss=420.9837\n",
      "Val Loss=526.6664\n",
      "-----\n",
      "1950\n",
      "Loss=420.9526\n",
      "Val Loss=526.5855\n",
      "-----\n",
      "1960\n",
      "Loss=421.1181\n",
      "Val Loss=526.9565\n",
      "-----\n",
      "1970\n",
      "Loss=421.7203\n",
      "Val Loss=528.0994\n",
      "-----\n",
      "1980\n",
      "Loss=422.0638\n",
      "Val Loss=528.6963\n",
      "-----\n",
      "1990\n",
      "Loss=421.9872\n",
      "Val Loss=528.6566\n",
      "-----\n",
      "2000\n",
      "Loss=421.7113\n",
      "Val Loss=528.2611\n",
      "-----\n",
      "2010\n",
      "Loss=420.9028\n",
      "Val Loss=526.9022\n",
      "-----\n",
      "2020\n",
      "Loss=419.5657\n",
      "Val Loss=524.6724\n",
      "-----\n",
      "2030\n",
      "Loss=418.3279\n",
      "Val Loss=522.6796\n",
      "-----\n",
      "2040\n",
      "Loss=417.2904\n",
      "Val Loss=521.0428\n",
      "-----\n",
      "2050\n",
      "Loss=416.1252\n",
      "Val Loss=519.0920\n",
      "-----\n",
      "2060\n",
      "Loss=414.7583\n",
      "Val Loss=516.7968\n",
      "-----\n",
      "2070\n",
      "Loss=413.6328\n",
      "Val Loss=514.9460\n",
      "-----\n",
      "2080\n",
      "Loss=412.7554\n",
      "Val Loss=513.4824\n",
      "-----\n",
      "2090\n",
      "Loss=412.1720\n",
      "Val Loss=512.4734\n",
      "-----\n",
      "2100\n",
      "Loss=411.8834\n",
      "Val Loss=512.0331\n",
      "-----\n",
      "2110\n",
      "Loss=411.7042\n",
      "Val Loss=511.7303\n",
      "-----\n",
      "2120\n",
      "Loss=411.7183\n",
      "Val Loss=511.7881\n",
      "-----\n",
      "2130\n",
      "Loss=411.7517\n",
      "Val Loss=511.7859\n",
      "-----\n",
      "2140\n",
      "Loss=411.8954\n",
      "Val Loss=511.9582\n",
      "-----\n",
      "2150\n",
      "Loss=412.2157\n",
      "Val Loss=512.5435\n",
      "-----\n",
      "2160\n",
      "Loss=412.4521\n",
      "Val Loss=512.9307\n",
      "-----\n",
      "2170\n",
      "Loss=412.5332\n",
      "Val Loss=513.0494\n",
      "-----\n",
      "2180\n",
      "Loss=412.4768\n",
      "Val Loss=512.9545\n",
      "-----\n",
      "2190\n",
      "Loss=412.3800\n",
      "Val Loss=512.8038\n",
      "-----\n",
      "2200\n",
      "Loss=412.1772\n",
      "Val Loss=512.4786\n",
      "-----\n",
      "2210\n",
      "Loss=411.5039\n",
      "Val Loss=511.3269\n",
      "-----\n",
      "2220\n",
      "Loss=410.6765\n",
      "Val Loss=509.9778\n",
      "-----\n",
      "2230\n",
      "Loss=409.6400\n",
      "Val Loss=508.2819\n",
      "-----\n",
      "2240\n",
      "Loss=408.4178\n",
      "Val Loss=506.2510\n",
      "-----\n",
      "2250\n",
      "Loss=407.3234\n",
      "Val Loss=504.4915\n",
      "-----\n",
      "2260\n",
      "Loss=406.2301\n",
      "Val Loss=502.6830\n",
      "-----\n",
      "2270\n",
      "Loss=405.2150\n",
      "Val Loss=501.0291\n",
      "-----\n",
      "2280\n",
      "Loss=404.2905\n",
      "Val Loss=499.5347\n",
      "-----\n",
      "2290\n",
      "Loss=403.4015\n",
      "Val Loss=498.0389\n",
      "-----\n",
      "2300\n",
      "Loss=402.6039\n",
      "Val Loss=496.7083\n",
      "-----\n",
      "2310\n",
      "Loss=401.8642\n",
      "Val Loss=495.4727\n",
      "-----\n",
      "2320\n",
      "Loss=401.1515\n",
      "Val Loss=494.3229\n",
      "-----\n",
      "2330\n",
      "Loss=400.3952\n",
      "Val Loss=493.0730\n",
      "-----\n",
      "2340\n",
      "Loss=399.6363\n",
      "Val Loss=491.8365\n",
      "-----\n",
      "2350\n",
      "Loss=398.9259\n",
      "Val Loss=490.7068\n",
      "-----\n",
      "2360\n",
      "Loss=398.2191\n",
      "Val Loss=489.5654\n",
      "-----\n",
      "2370\n",
      "Loss=397.4770\n",
      "Val Loss=488.3611\n",
      "-----\n",
      "2380\n",
      "Loss=396.7362\n",
      "Val Loss=487.1815\n",
      "-----\n",
      "2390\n",
      "Loss=396.0753\n",
      "Val Loss=486.0621\n",
      "-----\n",
      "2400\n",
      "Loss=395.4567\n",
      "Val Loss=485.0318\n",
      "-----\n",
      "2410\n",
      "Loss=394.8358\n",
      "Val Loss=483.9826\n",
      "-----\n",
      "2420\n",
      "Loss=394.1626\n",
      "Val Loss=482.8492\n",
      "-----\n",
      "2430\n",
      "Loss=393.4920\n",
      "Val Loss=481.7615\n",
      "-----\n",
      "2440\n",
      "Loss=392.9494\n",
      "Val Loss=480.8817\n",
      "-----\n",
      "2450\n",
      "Loss=392.5121\n",
      "Val Loss=480.2273\n",
      "-----\n",
      "2460\n",
      "Loss=392.0840\n",
      "Val Loss=479.5814\n",
      "-----\n",
      "2470\n",
      "Loss=391.6048\n",
      "Val Loss=478.8152\n",
      "-----\n",
      "2480\n",
      "Loss=391.0485\n",
      "Val Loss=477.9082\n",
      "-----\n",
      "2490\n",
      "Loss=390.4200\n",
      "Val Loss=476.8399\n",
      "-----\n",
      "2500\n",
      "Loss=389.7794\n",
      "Val Loss=475.7565\n",
      "-----\n",
      "2510\n",
      "Loss=389.1989\n",
      "Val Loss=474.7793\n",
      "-----\n",
      "2520\n",
      "Loss=388.6698\n",
      "Val Loss=473.9128\n",
      "-----\n",
      "2530\n",
      "Loss=388.0719\n",
      "Val Loss=472.8977\n",
      "-----\n",
      "2540\n",
      "Loss=387.4251\n",
      "Val Loss=471.8171\n",
      "-----\n",
      "2550\n",
      "Loss=386.8506\n",
      "Val Loss=470.8794\n",
      "-----\n",
      "2560\n",
      "Loss=386.3326\n",
      "Val Loss=470.0152\n",
      "-----\n",
      "2570\n",
      "Loss=385.7897\n",
      "Val Loss=469.0986\n",
      "-----\n",
      "2580\n",
      "Loss=385.2387\n",
      "Val Loss=468.1490\n",
      "-----\n",
      "2590\n",
      "Loss=384.6182\n",
      "Val Loss=467.0898\n",
      "-----\n",
      "2600\n",
      "Loss=383.9674\n",
      "Val Loss=465.9653\n",
      "-----\n",
      "2610\n",
      "Loss=383.3196\n",
      "Val Loss=464.8711\n",
      "-----\n",
      "2620\n",
      "Loss=382.7236\n",
      "Val Loss=463.8689\n",
      "-----\n",
      "2630\n",
      "Loss=382.2343\n",
      "Val Loss=463.0510\n",
      "-----\n",
      "2640\n",
      "Loss=381.7827\n",
      "Val Loss=462.3114\n",
      "-----\n",
      "2650\n",
      "Loss=381.4132\n",
      "Val Loss=461.7165\n",
      "-----\n",
      "2660\n",
      "Loss=381.0347\n",
      "Val Loss=461.1218\n",
      "-----\n",
      "2670\n",
      "Loss=380.7351\n",
      "Val Loss=460.6622\n",
      "-----\n",
      "2680\n",
      "Loss=380.3858\n",
      "Val Loss=460.0966\n",
      "-----\n",
      "2690\n",
      "Loss=379.9684\n",
      "Val Loss=459.3980\n",
      "-----\n",
      "2700\n",
      "Loss=379.5839\n",
      "Val Loss=458.7466\n",
      "-----\n",
      "2710\n",
      "Loss=379.1872\n",
      "Val Loss=458.0734\n",
      "-----\n",
      "2720\n",
      "Loss=378.7314\n",
      "Val Loss=457.2837\n",
      "-----\n",
      "2730\n",
      "Loss=378.2335\n",
      "Val Loss=456.4321\n",
      "-----\n",
      "2740\n",
      "Loss=377.7077\n",
      "Val Loss=455.5385\n",
      "-----\n",
      "2750\n",
      "Loss=377.1746\n",
      "Val Loss=454.6312\n",
      "-----\n",
      "2760\n",
      "Loss=376.6491\n",
      "Val Loss=453.7407\n",
      "-----\n",
      "2770\n",
      "Loss=376.1206\n",
      "Val Loss=452.8414\n",
      "-----\n",
      "2780\n",
      "Loss=375.6094\n",
      "Val Loss=451.9796\n",
      "-----\n",
      "2790\n",
      "Loss=375.1010\n",
      "Val Loss=451.1334\n",
      "-----\n",
      "2800\n",
      "Loss=374.6398\n",
      "Val Loss=450.3590\n",
      "-----\n",
      "2810\n",
      "Loss=374.2476\n",
      "Val Loss=449.6793\n",
      "-----\n",
      "2820\n",
      "Loss=373.8883\n",
      "Val Loss=449.0719\n",
      "-----\n",
      "2830\n",
      "Loss=373.5438\n",
      "Val Loss=448.4863\n",
      "-----\n",
      "2840\n",
      "Loss=373.2562\n",
      "Val Loss=448.0237\n",
      "-----\n",
      "2850\n",
      "Loss=372.9877\n",
      "Val Loss=447.5831\n",
      "-----\n",
      "2860\n",
      "Loss=372.6935\n",
      "Val Loss=447.0891\n",
      "-----\n",
      "2870\n",
      "Loss=372.4571\n",
      "Val Loss=446.7191\n",
      "-----\n",
      "2880\n",
      "Loss=372.3000\n",
      "Val Loss=446.4703\n",
      "-----\n",
      "2890\n",
      "Loss=372.1478\n",
      "Val Loss=446.2129\n",
      "-----\n",
      "2900\n",
      "Loss=371.9858\n",
      "Val Loss=445.9254\n",
      "-----\n",
      "2910\n",
      "Loss=371.8068\n",
      "Val Loss=445.6270\n",
      "-----\n",
      "2920\n",
      "Loss=371.6312\n",
      "Val Loss=445.3380\n",
      "-----\n",
      "2930\n",
      "Loss=371.4481\n",
      "Val Loss=445.0208\n",
      "-----\n",
      "2940\n",
      "Loss=371.2965\n",
      "Val Loss=444.7503\n",
      "-----\n",
      "2950\n",
      "Loss=371.1208\n",
      "Val Loss=444.4558\n",
      "-----\n",
      "2960\n",
      "Loss=370.9569\n",
      "Val Loss=444.1920\n",
      "-----\n",
      "2970\n",
      "Loss=370.7778\n",
      "Val Loss=443.9050\n",
      "-----\n",
      "2980\n",
      "Loss=370.6402\n",
      "Val Loss=443.6988\n",
      "-----\n",
      "2990\n",
      "Loss=370.5450\n",
      "Val Loss=443.5735\n",
      "-----\n",
      "3000\n",
      "Loss=370.4738\n",
      "Val Loss=443.4659\n",
      "-----\n",
      "3010\n",
      "Loss=370.4022\n",
      "Val Loss=443.3531\n",
      "-----\n",
      "3020\n",
      "Loss=370.3101\n",
      "Val Loss=443.2145\n",
      "-----\n",
      "3030\n",
      "Loss=370.2350\n",
      "Val Loss=443.1109\n",
      "-----\n",
      "3040\n",
      "Loss=370.1310\n",
      "Val Loss=442.9508\n",
      "-----\n",
      "3050\n",
      "Loss=369.9893\n",
      "Val Loss=442.7268\n",
      "-----\n",
      "3060\n",
      "Loss=369.7989\n",
      "Val Loss=442.4062\n",
      "-----\n",
      "3070\n",
      "Loss=369.5905\n",
      "Val Loss=442.0588\n",
      "-----\n",
      "3080\n",
      "Loss=369.3492\n",
      "Val Loss=441.6346\n",
      "-----\n",
      "3090\n",
      "Loss=369.1071\n",
      "Val Loss=441.2312\n",
      "-----\n",
      "3100\n",
      "Loss=368.8833\n",
      "Val Loss=440.8647\n",
      "-----\n",
      "3110\n",
      "Loss=368.6806\n",
      "Val Loss=440.5358\n",
      "-----\n",
      "3120\n",
      "Loss=368.5003\n",
      "Val Loss=440.2456\n",
      "-----\n",
      "3130\n",
      "Loss=368.3196\n",
      "Val Loss=439.9617\n",
      "-----\n",
      "3140\n",
      "Loss=368.1366\n",
      "Val Loss=439.6492\n",
      "-----\n",
      "3150\n",
      "Loss=367.9574\n",
      "Val Loss=439.3608\n",
      "-----\n",
      "3160\n",
      "Loss=367.7727\n",
      "Val Loss=439.0367\n",
      "-----\n",
      "3170\n",
      "Loss=367.5998\n",
      "Val Loss=438.7441\n",
      "-----\n",
      "3180\n",
      "Loss=367.4471\n",
      "Val Loss=438.4905\n",
      "-----\n",
      "3190\n",
      "Loss=367.2984\n",
      "Val Loss=438.2375\n",
      "-----\n",
      "3200\n",
      "Loss=367.1713\n",
      "Val Loss=438.0421\n",
      "-----\n",
      "3210\n",
      "Loss=367.0806\n",
      "Val Loss=437.9187\n",
      "-----\n",
      "3220\n",
      "Loss=367.0170\n",
      "Val Loss=437.8446\n",
      "-----\n",
      "3230\n",
      "Loss=366.9625\n",
      "Val Loss=437.7737\n",
      "-----\n",
      "3240\n",
      "Loss=366.8889\n",
      "Val Loss=437.6697\n",
      "-----\n",
      "3250\n",
      "Loss=366.8145\n",
      "Val Loss=437.5578\n",
      "-----\n",
      "3260\n",
      "Loss=366.7557\n",
      "Val Loss=437.4912\n",
      "-----\n",
      "3270\n",
      "Loss=366.6899\n",
      "Val Loss=437.3941\n",
      "-----\n",
      "3280\n",
      "Loss=366.6134\n",
      "Val Loss=437.3005\n",
      "-----\n",
      "3290\n",
      "Loss=366.5485\n",
      "Val Loss=437.2175\n",
      "-----\n",
      "3300\n",
      "Loss=366.4777\n",
      "Val Loss=437.1271\n",
      "-----\n",
      "3310\n",
      "Loss=366.3831\n",
      "Val Loss=437.0015\n",
      "-----\n",
      "3320\n",
      "Loss=366.2597\n",
      "Val Loss=436.7917\n",
      "-----\n",
      "3330\n",
      "Loss=366.1134\n",
      "Val Loss=436.5383\n",
      "-----\n",
      "3340\n",
      "Loss=365.9776\n",
      "Val Loss=436.3007\n",
      "-----\n",
      "3350\n",
      "Loss=365.8715\n",
      "Val Loss=436.1320\n",
      "-----\n",
      "3360\n",
      "Loss=365.7813\n",
      "Val Loss=436.0004\n",
      "-----\n",
      "3370\n",
      "Loss=365.6782\n",
      "Val Loss=435.8455\n",
      "-----\n",
      "3380\n",
      "Loss=365.5607\n",
      "Val Loss=435.6611\n",
      "-----\n",
      "3390\n",
      "Loss=365.4229\n",
      "Val Loss=435.4433\n",
      "-----\n",
      "3400\n",
      "Loss=365.2621\n",
      "Val Loss=435.1865\n",
      "-----\n",
      "3410\n",
      "Loss=365.1036\n",
      "Val Loss=434.9151\n",
      "-----\n",
      "3420\n",
      "Loss=364.9325\n",
      "Val Loss=434.6210\n",
      "-----\n",
      "3430\n",
      "Loss=364.7603\n",
      "Val Loss=434.3433\n",
      "-----\n",
      "3440\n",
      "Loss=364.5862\n",
      "Val Loss=434.0524\n",
      "-----\n",
      "3450\n",
      "Loss=364.3746\n",
      "Val Loss=433.6758\n",
      "-----\n",
      "3460\n",
      "Loss=364.1480\n",
      "Val Loss=433.2997\n",
      "-----\n",
      "3470\n",
      "Loss=363.9193\n",
      "Val Loss=432.9208\n",
      "-----\n",
      "3480\n",
      "Loss=363.7041\n",
      "Val Loss=432.5415\n",
      "-----\n",
      "3490\n",
      "Loss=363.4769\n",
      "Val Loss=432.1703\n",
      "-----\n",
      "3500\n",
      "Loss=363.2468\n",
      "Val Loss=431.7607\n",
      "-----\n",
      "3510\n",
      "Loss=363.0149\n",
      "Val Loss=431.3757\n",
      "-----\n",
      "3520\n",
      "Loss=362.7985\n",
      "Val Loss=431.0071\n",
      "-----\n",
      "3530\n",
      "Loss=362.5837\n",
      "Val Loss=430.6601\n",
      "-----\n",
      "3540\n",
      "Loss=362.3810\n",
      "Val Loss=430.3067\n",
      "-----\n",
      "3550\n",
      "Loss=362.1786\n",
      "Val Loss=429.9810\n",
      "-----\n",
      "3560\n",
      "Loss=361.9995\n",
      "Val Loss=429.6620\n",
      "-----\n",
      "3570\n",
      "Loss=361.8163\n",
      "Val Loss=429.3500\n",
      "-----\n",
      "3580\n",
      "Loss=361.6455\n",
      "Val Loss=429.0543\n",
      "-----\n",
      "3590\n",
      "Loss=361.4902\n",
      "Val Loss=428.7816\n",
      "-----\n",
      "3600\n",
      "Loss=361.3491\n",
      "Val Loss=428.5491\n",
      "-----\n",
      "3610\n",
      "Loss=361.2345\n",
      "Val Loss=428.3629\n",
      "-----\n",
      "3620\n",
      "Loss=361.1103\n",
      "Val Loss=428.1638\n",
      "-----\n",
      "3630\n",
      "Loss=360.9843\n",
      "Val Loss=427.9656\n",
      "-----\n",
      "3640\n",
      "Loss=360.8555\n",
      "Val Loss=427.7532\n",
      "-----\n",
      "3650\n",
      "Loss=360.7381\n",
      "Val Loss=427.5578\n",
      "-----\n",
      "3660\n",
      "Loss=360.6203\n",
      "Val Loss=427.3824\n",
      "-----\n",
      "3670\n",
      "Loss=360.5094\n",
      "Val Loss=427.2072\n",
      "-----\n",
      "3680\n",
      "Loss=360.4144\n",
      "Val Loss=427.0735\n",
      "-----\n",
      "3690\n",
      "Loss=360.3187\n",
      "Val Loss=426.9360\n",
      "-----\n",
      "3700\n",
      "Loss=360.2485\n",
      "Val Loss=426.8513\n",
      "-----\n",
      "3710\n",
      "Loss=360.2104\n",
      "Val Loss=426.8147\n",
      "-----\n",
      "3720\n",
      "Loss=360.1745\n",
      "Val Loss=426.7682\n",
      "-----\n",
      "3730\n",
      "Loss=360.1285\n",
      "Val Loss=426.7029\n",
      "-----\n",
      "3740\n",
      "Loss=360.0874\n",
      "Val Loss=426.6559\n",
      "-----\n",
      "3750\n",
      "Loss=360.0504\n",
      "Val Loss=426.6103\n",
      "-----\n",
      "3760\n",
      "Loss=360.0274\n",
      "Val Loss=426.5974\n",
      "-----\n",
      "3770\n",
      "Loss=360.0034\n",
      "Val Loss=426.5778\n",
      "-----\n",
      "3780\n",
      "Loss=359.9680\n",
      "Val Loss=426.5285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "3790\n",
      "Loss=359.9350\n",
      "Val Loss=426.4884\n",
      "-----\n",
      "3800\n",
      "Loss=359.9317\n",
      "Val Loss=426.5097\n",
      "-----\n",
      "3810\n",
      "Loss=359.9241\n",
      "Val Loss=426.5287\n",
      "-----\n",
      "3820\n",
      "Loss=359.9145\n",
      "Val Loss=426.5495\n",
      "-----\n",
      "3830\n",
      "Loss=359.9087\n",
      "Val Loss=426.5782\n",
      "-----\n",
      "3840\n",
      "Loss=359.9026\n",
      "Val Loss=426.6108\n",
      "-----\n",
      "3850\n",
      "Loss=359.8949\n",
      "Val Loss=426.6203\n",
      "-----\n",
      "3860\n",
      "Loss=359.8802\n",
      "Val Loss=426.6178\n",
      "-----\n",
      "3870\n",
      "Loss=359.8420\n",
      "Val Loss=426.5749\n",
      "-----\n",
      "3880\n",
      "Loss=359.8029\n",
      "Val Loss=426.5277\n",
      "-----\n",
      "3890\n",
      "Loss=359.7657\n",
      "Val Loss=426.4815\n",
      "-----\n",
      "3900\n",
      "Loss=359.7304\n",
      "Val Loss=426.4452\n",
      "-----\n",
      "3910\n",
      "Loss=359.6898\n",
      "Val Loss=426.4007\n",
      "-----\n",
      "3920\n",
      "Loss=359.6517\n",
      "Val Loss=426.3651\n",
      "-----\n",
      "3930\n",
      "Loss=359.6024\n",
      "Val Loss=426.2966\n",
      "-----\n",
      "3940\n",
      "Loss=359.5480\n",
      "Val Loss=426.2260\n",
      "-----\n",
      "3950\n",
      "Loss=359.4986\n",
      "Val Loss=426.1575\n",
      "-----\n",
      "3960\n",
      "Loss=359.4635\n",
      "Val Loss=426.1201\n",
      "-----\n",
      "3970\n",
      "Loss=359.4412\n",
      "Val Loss=426.0986\n",
      "-----\n",
      "3980\n",
      "Loss=359.4257\n",
      "Val Loss=426.0995\n",
      "-----\n",
      "3990\n",
      "Loss=359.4072\n",
      "Val Loss=426.0861\n",
      "-----\n",
      "4000\n",
      "Loss=359.3848\n",
      "Val Loss=426.0854\n",
      "-----\n",
      "4010\n",
      "Loss=359.3643\n",
      "Val Loss=426.0787\n",
      "-----\n",
      "4020\n",
      "Loss=359.3462\n",
      "Val Loss=426.0745\n",
      "-----\n",
      "4030\n",
      "Loss=359.3345\n",
      "Val Loss=426.0728\n",
      "-----\n",
      "4040\n",
      "Loss=359.3382\n",
      "Val Loss=426.0982\n",
      "-----\n",
      "4050\n",
      "Loss=359.3408\n",
      "Val Loss=426.1346\n",
      "-----\n",
      "4060\n",
      "Loss=359.3431\n",
      "Val Loss=426.1656\n",
      "-----\n",
      "4070\n",
      "Loss=359.3437\n",
      "Val Loss=426.2009\n",
      "-----\n",
      "4080\n",
      "Loss=359.3441\n",
      "Val Loss=426.2202\n",
      "-----\n",
      "4090\n",
      "Loss=359.3368\n",
      "Val Loss=426.2329\n",
      "-----\n",
      "4100\n",
      "Loss=359.2980\n",
      "Val Loss=426.1934\n",
      "-----\n",
      "4110\n",
      "Loss=359.2591\n",
      "Val Loss=426.1435\n",
      "-----\n",
      "4120\n",
      "Loss=359.2057\n",
      "Val Loss=426.0400\n",
      "-----\n",
      "4130\n",
      "Loss=359.1461\n",
      "Val Loss=425.9437\n",
      "-----\n",
      "4140\n",
      "Loss=359.0693\n",
      "Val Loss=425.8318\n",
      "-----\n",
      "4150\n",
      "Loss=359.0203\n",
      "Val Loss=425.7574\n",
      "-----\n",
      "4160\n",
      "Loss=358.9745\n",
      "Val Loss=425.7018\n",
      "-----\n",
      "4170\n",
      "Loss=358.9307\n",
      "Val Loss=425.6657\n",
      "-----\n",
      "4180\n",
      "Loss=358.8911\n",
      "Val Loss=425.6243\n",
      "-----\n",
      "4190\n",
      "Loss=358.8574\n",
      "Val Loss=425.5966\n",
      "-----\n",
      "4200\n",
      "Loss=358.8143\n",
      "Val Loss=425.5544\n",
      "-----\n",
      "4210\n",
      "Loss=358.7722\n",
      "Val Loss=425.4993\n",
      "-----\n",
      "4220\n",
      "Loss=358.7209\n",
      "Val Loss=425.4274\n",
      "-----\n",
      "4230\n",
      "Loss=358.6514\n",
      "Val Loss=425.3091\n",
      "-----\n",
      "4240\n",
      "Loss=358.5615\n",
      "Val Loss=425.1683\n",
      "-----\n",
      "4250\n",
      "Loss=358.4926\n",
      "Val Loss=425.0717\n",
      "-----\n",
      "4260\n",
      "Loss=358.4362\n",
      "Val Loss=425.0090\n",
      "-----\n",
      "4270\n",
      "Loss=358.3728\n",
      "Val Loss=424.9269\n",
      "-----\n",
      "4280\n",
      "Loss=358.2998\n",
      "Val Loss=424.8236\n",
      "-----\n",
      "4290\n",
      "Loss=358.2370\n",
      "Val Loss=424.7390\n",
      "-----\n",
      "4300\n",
      "Loss=358.1761\n",
      "Val Loss=424.6547\n",
      "-----\n",
      "4310\n",
      "Loss=358.1165\n",
      "Val Loss=424.5628\n",
      "-----\n",
      "4320\n",
      "Loss=358.0733\n",
      "Val Loss=424.5039\n",
      "-----\n",
      "4330\n",
      "Loss=358.0198\n",
      "Val Loss=424.4204\n",
      "-----\n",
      "4340\n",
      "Loss=357.9583\n",
      "Val Loss=424.3275\n",
      "-----\n",
      "4350\n",
      "Loss=357.9035\n",
      "Val Loss=424.2537\n",
      "-----\n",
      "4360\n",
      "Loss=357.8543\n",
      "Val Loss=424.1791\n",
      "-----\n",
      "4370\n",
      "Loss=357.8088\n",
      "Val Loss=424.1262\n",
      "-----\n",
      "4380\n",
      "Loss=357.7506\n",
      "Val Loss=424.0406\n",
      "-----\n",
      "4390\n",
      "Loss=357.7010\n",
      "Val Loss=423.9767\n",
      "-----\n",
      "4400\n",
      "Loss=357.6453\n",
      "Val Loss=423.8950\n",
      "-----\n",
      "4410\n",
      "Loss=357.5705\n",
      "Val Loss=423.7862\n",
      "-----\n",
      "4420\n",
      "Loss=357.5046\n",
      "Val Loss=423.6886\n",
      "-----\n",
      "4430\n",
      "Loss=357.4431\n",
      "Val Loss=423.5931\n",
      "-----\n",
      "4440\n",
      "Loss=357.3852\n",
      "Val Loss=423.5070\n",
      "-----\n",
      "4450\n",
      "Loss=357.3259\n",
      "Val Loss=423.4226\n",
      "-----\n",
      "4460\n",
      "Loss=357.2744\n",
      "Val Loss=423.3373\n",
      "-----\n",
      "4470\n",
      "Loss=357.2209\n",
      "Val Loss=423.2643\n",
      "-----\n",
      "4480\n",
      "Loss=357.1698\n",
      "Val Loss=423.1968\n",
      "-----\n",
      "4490\n",
      "Loss=357.0995\n",
      "Val Loss=423.0999\n",
      "-----\n",
      "4500\n",
      "Loss=357.0407\n",
      "Val Loss=423.0210\n",
      "-----\n",
      "4510\n",
      "Loss=356.9869\n",
      "Val Loss=422.9468\n",
      "-----\n",
      "4520\n",
      "Loss=356.9524\n",
      "Val Loss=422.8896\n",
      "-----\n",
      "4530\n",
      "Loss=356.9098\n",
      "Val Loss=422.8441\n",
      "-----\n",
      "4540\n",
      "Loss=356.8663\n",
      "Val Loss=422.7848\n",
      "-----\n",
      "4550\n",
      "Loss=356.8219\n",
      "Val Loss=422.7285\n",
      "-----\n",
      "4560\n",
      "Loss=356.7783\n",
      "Val Loss=422.6743\n",
      "-----\n",
      "4570\n",
      "Loss=356.7593\n",
      "Val Loss=422.6766\n",
      "-----\n",
      "4580\n",
      "Loss=356.7431\n",
      "Val Loss=422.6818\n",
      "-----\n",
      "4590\n",
      "Loss=356.7426\n",
      "Val Loss=422.7112\n",
      "-----\n",
      "4600\n",
      "Loss=356.7585\n",
      "Val Loss=422.7708\n",
      "-----\n",
      "4610\n",
      "Loss=356.7808\n",
      "Val Loss=422.8379\n",
      "-----\n",
      "4620\n",
      "Loss=356.8004\n",
      "Val Loss=422.8920\n",
      "-----\n",
      "4630\n",
      "Loss=356.8300\n",
      "Val Loss=422.9696\n",
      "-----\n",
      "4640\n",
      "Loss=356.8642\n",
      "Val Loss=423.0725\n",
      "-----\n",
      "4650\n",
      "Loss=356.9193\n",
      "Val Loss=423.2145\n",
      "-----\n",
      "4660\n",
      "Loss=356.9742\n",
      "Val Loss=423.3646\n",
      "-----\n",
      "4670\n",
      "Loss=357.0437\n",
      "Val Loss=423.5290\n",
      "-----\n",
      "4680\n",
      "Loss=357.1187\n",
      "Val Loss=423.6988\n",
      "-----\n",
      "4690\n",
      "Loss=357.1751\n",
      "Val Loss=423.8283\n",
      "-----\n",
      "4700\n",
      "Loss=357.2240\n",
      "Val Loss=423.9376\n",
      "-----\n",
      "4710\n",
      "Loss=357.2920\n",
      "Val Loss=424.1085\n",
      "-----\n",
      "4720\n",
      "Loss=357.3667\n",
      "Val Loss=424.2674\n",
      "-----\n",
      "4730\n",
      "Loss=357.4309\n",
      "Val Loss=424.4199\n",
      "-----\n",
      "4740\n",
      "Loss=357.4992\n",
      "Val Loss=424.5729\n",
      "-----\n",
      "4750\n",
      "Loss=357.5710\n",
      "Val Loss=424.7399\n",
      "-----\n",
      "4760\n",
      "Loss=357.6436\n",
      "Val Loss=424.9090\n",
      "-----\n",
      "4770\n",
      "Loss=357.7132\n",
      "Val Loss=425.0732\n",
      "-----\n",
      "4780\n",
      "Loss=357.7946\n",
      "Val Loss=425.2644\n",
      "-----\n",
      "4790\n",
      "Loss=357.8728\n",
      "Val Loss=425.4418\n",
      "-----\n",
      "4800\n",
      "Loss=357.9464\n",
      "Val Loss=425.6118\n",
      "-----\n",
      "4810\n",
      "Loss=358.0219\n",
      "Val Loss=425.7823\n",
      "-----\n",
      "4820\n",
      "Loss=358.1069\n",
      "Val Loss=425.9604\n",
      "-----\n",
      "4830\n",
      "Loss=358.1911\n",
      "Val Loss=426.1593\n",
      "-----\n",
      "4840\n",
      "Loss=358.2720\n",
      "Val Loss=426.3578\n",
      "-----\n",
      "4850\n",
      "Loss=358.3736\n",
      "Val Loss=426.5836\n",
      "-----\n",
      "4860\n",
      "Loss=358.4822\n",
      "Val Loss=426.8331\n",
      "-----\n",
      "4870\n",
      "Loss=358.5858\n",
      "Val Loss=427.0471\n",
      "-----\n",
      "4880\n",
      "Loss=358.6714\n",
      "Val Loss=427.2486\n",
      "-----\n",
      "4890\n",
      "Loss=358.7404\n",
      "Val Loss=427.4087\n",
      "-----\n",
      "4900\n",
      "Loss=358.7934\n",
      "Val Loss=427.5206\n",
      "-----\n",
      "4910\n",
      "Loss=358.8258\n",
      "Val Loss=427.6322\n",
      "-----\n",
      "4920\n",
      "Loss=358.8189\n",
      "Val Loss=427.6460\n",
      "-----\n",
      "4930\n",
      "Loss=358.8009\n",
      "Val Loss=427.6200\n",
      "-----\n",
      "4940\n",
      "Loss=358.7667\n",
      "Val Loss=427.5743\n",
      "-----\n",
      "4950\n",
      "Loss=358.7249\n",
      "Val Loss=427.5205\n",
      "-----\n",
      "4960\n",
      "Loss=358.6896\n",
      "Val Loss=427.4833\n",
      "-----\n",
      "4970\n",
      "Loss=358.6401\n",
      "Val Loss=427.4054\n",
      "-----\n",
      "4980\n",
      "Loss=358.5806\n",
      "Val Loss=427.3241\n",
      "-----\n",
      "4990\n",
      "Loss=358.5078\n",
      "Val Loss=427.2135\n",
      "-----\n",
      "5000\n",
      "Loss=358.4353\n",
      "Val Loss=427.0938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-af6eb31b839a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# input x and predict based on x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# must be (1. nn output, 2. target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-af6eb31b839a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# perform dropout on input vector embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 fully connected 150 units\n",
    "        # self.lin1 = nn.Linear(n_feature, 100)\n",
    "        \n",
    "        # layer 2 fully connected 50 units\n",
    "        # self.lin2 = nn.Linear(100, 20)\n",
    "        # self.lin3 = nn.Linear(20, n_output)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(3,150,kernel_size = 508)\n",
    "        self.lin1 = nn.Linear(150, 50)\n",
    "        self.lin2 = nn.Linear(50, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform dropout on input vector embeddings\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x  \n",
    "\n",
    "net = Net(n_feature=1524, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-6, weight_decay = 1)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(10000):\n",
    "    prediction = net(X)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    # Do validation loss\n",
    "    with torch.no_grad():\n",
    "        pred_val = net(X_val)\n",
    "        loss_val = loss_func(pred_val, y_val)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # plot and show learning process\n",
    "        '''\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "        '''\n",
    "        print(\"-----\")\n",
    "        print(t)\n",
    "        print('Loss=%.4f' % loss.data.numpy())\n",
    "        print('Val Loss=%.4f' % loss_val.data.numpy())\n",
    "\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
